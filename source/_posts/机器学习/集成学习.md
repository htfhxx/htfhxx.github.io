---
title: 集成学习
date: 2019-12-07
author: 长腿咚咚咚
toc: true
mathjax: true
categories: machine-learning
tags:
	- 集成学习
---



[TOC]

## 1 集成学习

目前的集成学习有两大类：

1. 个体学习器之间不存在强依赖关系，可以同时生成、并行计算
2. 个体学习器之间存在强依赖关系，必须串行生成

1的代表：Bagging、随机森林

2的代表：Boosting、AdaBoost

## 2 Bagging

- 自助采样法  Bootstrap sample: 
  - 对原始数据进行有放回的随机采样，m个样本的数据集中，有放回的采样m个样本；
  - 等价于给每个样本点赋予不同权重
- 思想：
  1. 基于自助采样法（bootstrap sample）构造T个样本集
  2. 基于每个采样集训练T个基学习器，然后将这些基学习器结合
  3. 对分类任务采用投票法，对回归任务采用平均法
- Bagging可以降低模型方差，但不改变模型偏差

## 3 随机森林（Ensemble Learning）

随机森林以决策树为基学习器。是 Bagging 的优化版本。

* 其包含的思想在于
  * 随机选择一部分特征
  * 随机选择一部分样本

* 构建过程：
  1. 基于自助采样法（bootstrap sample）构造多个样本集
  2. 随机选取特征集合，特征数远小于原特征集合；
  3. 每棵树都尽最大程度的生长，并且没有剪枝过程；
  4. 进行投票决策

## 4 Boosting

Boosting是将弱学习器提升为强学习器的算法

* Boosting的工作机制
  1. 从初始训练集中学习一个基学习器
  3. 学习下一个基学习器，使其能帮助第一个基学习器
  4. 重复进行，最终组合所有的弱学习器



## 5 AdaBoost

* AdaBoost的思想
  * 在弱学习器失败的样本上学习第二个弱学习器
  * 学习第二个弱学习器的方式是：**样本重加权**
    – 分对的样本，其权重减小
    – 分错的样本，其权重增大

* **AdaBoost的学习过程**
  有训练数据集   $\mathrm{T}=\left\{\left(\mathrm{x}_{1},\mathrm{y}_{1}\right),\left(\mathrm{x}_{2},  \mathrm{y}_{2}\right), \ldots,\left(\mathrm{x}_{\mathrm{N}}, \mathrm{y}_{\mathrm{N}}\right)\right\}$

  1. 初始化训练数据的权值分布
     $$
     D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N
     $$

  2. 根据当前训练数据的权值分布**学习到一个新的基学习器$G_{m}$**
  $$
     G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}
  $$
  
  3. **更新训练数据的权值分布**
  
  4. 重复步骤2、3，得到多个基学习器
  
  5. 构建基本分类器的线性组合，并得到最终分类器
     $$
     G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)  \\ 
     f(x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)
     $$



* 第3步中**为基学习器$G_{m+1}$更新权值分布**的步骤

  2. 计算基学习器在训练集上的分类误差率$e_{m}$
     $$
     e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)
     $$

  3. 根据分类误差率得到系数$\alpha_{m}$，这个系数权重也是最终分类器组合时的权重：
     $$
     \alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}
     $$

  4. 更新训练数据集的权值分布

$$
\begin{array}{c}
{D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)} \\
{w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N}
\end{array}
$$

* 伪代码描述

<img src="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/1578318702125.png" alt="1578318702125" style="zoom: 50%;" />



## GBDT





## XGBoost











## reference：

```
《统计学习方法》
UCAS《模式识别与机器学习》课件
《机器学习》——西瓜书
https://github.com/NLP-LOVE/ML-NLP
https://github.com/htfhxx/NLPer-Interview
```

