<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>BERT</title>
      <link href="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/"/>
      <url>/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-bert">1 BERT</span></h2><p>BERT论文：</p><p>《BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding》</p><h2><span id="2-主要结构">2 主要结构</span></h2><h3><span id="21-输入">2.1 输入</span></h3><p>​        Encoder端每个大模块接收的输入是不一样的，第一个大模块(最底下的那个)接收的输入是输入序列的embedding，其余大模块接收的是其前一个大模块的输出，最后一个模块的输出作为整个Encoder端的输出。  </p><h3><span id="22-transformer-encoder">2.2 Transformer Encoder</span></h3><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/1574779976637.png" alt="1574779976637" style="zoom: 80%;"></p><p>​        Encoder部分有两个sub-layer，<strong>Multi-Head Attention</strong>和<strong>Feed Forward</strong>。上图的Add代表残差网络，由图可知在每个sub-layer都加入了残差项。其中的Norm是Layer Normalization。</p><p>​        Multi-Head Attention是Transformer Encoder的核心，主要是self Attention；Feed Forward是一个全连接前馈神经网络。</p><h4><span id="221-multi-head-attention">2.21 Multi-Head Attention</span></h4><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/1574775307731.png" alt="1574775307731" style="zoom: 33%;"></p><p>​        </p><p>​        对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。单个Multi-Head Attention层的输入进行处理得到QKV，通过线性变换输入到Scaled Dot-Product Attention，得到多组结果进行concat并加权后，作为Encoding后的结果。</p><p>​        整个过程的公式：</p><script type="math/tex; mode=display">\begin{aligned} \text { MultiHead }(Q, K, V) &\left.=\text { Concat (head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><h4><span id="222-self-attention">2.22 Self-Attention</span></h4><p>Q和K计算相似度，此处使用点积的方法；防止结果过大，除以${\sqrt{d_{k}}}$，其中${\sqrt{d_{k}}}$是K的维度；进行softmax归一化得到Q和K的Attention；Attention与V相乘，得到self-Attention的结果。</p><h4><span id="223-scaled-dot-product-attention">2.23 Scaled Dot-Product Attention</span></h4><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/1574777556699.png" alt="1574777556699" style="zoom:33%;"></p><p>​        Scaled Dot-Product Attention是Transformer较为重要的核心部分，整个过程：</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ol><li>Q和K计算相似度，此处使用点积的方法；</li><li>防止结果过大，除以${\sqrt{d_{k}}}$，其中${d_{k}}$是K的维度；（内积太大的话softmax后就非0即1了,不够“soft”了）（为什么除以${\sqrt{d_{k}}}$: 让qk的方差与dk无关，从${d_{k}}$变为1，参考：<a href="https://blog.csdn.net/qq_37430422/article/details/105042303）" target="_blank" rel="noopener">https://blog.csdn.net/qq_37430422/article/details/105042303）</a></li><li>经过一个Mask操作； Q，K长度是不定时，进行补齐操作，将补齐的数据设置为负无穷</li><li>进行softmax归一化得到Q和K的Attention；</li><li>Attention与V相乘，得到self-Attention的结果。</li></ol><p>​        这里的QKV论文里没有详细展开，现有的博客文章很少提到，本文引用的第二篇博客中有段话写的很好。Attention机制中，将Source中看做是由一系列的(Key,Value)对构成，此时给定某元素Query，计算Query和各个Key的相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</p><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/att7.png" alt="image" style="zoom:33%;"></p><p>​        而对于本文的self-Attention来说，key、value和query都是其本身，也就是上一层的输出，作为下一层的输入。</p><h4><span id="24-残差">2.4 残差</span></h4><p><strong>残差是啥？</strong></p><p>一般表示：</p><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/image-20210708212857375.png" alt="image-20210708212857375"></p><p>通用表示：</p><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/image-20210708213229895.png" alt="image-20210708213229895"></p><p>h一般是直接映射，f一般是relu这种激活函数。</p><p><strong>残差有啥好处？</strong></p><ol><li>不会出现梯度消失</li><li>第L层的梯度可以传到任何一个浅层。</li><li>意义：能够很好的消除 层数加深所带来的信息损失问题，信息可以非常畅通的在高层和低层之间相互传导</li></ol><p>第L层和第l层的关系就是，L层可以表示为任意一个比它浅的l层和他们之间的残差部分之和。</p><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/image-20210708214126667.png" alt="image-20210708214126667"></p><p><img src="/2021/07/04/shen-du-xue-xi-yu-nlp-ji-chu/bert/image-20210708214313740.png" alt="image-20210708214313740"></p><p>ps: 如果两层的shape不一样，例如feature_map数量不一样，就需要使用诸如1×1卷积来升维或降维。</p><h3><span id="23-position-wise-feed-forward-networks">2.3 Position-wise Feed-Forward Networks</span></h3><p>​        除了Attention子层之外，其他子层都包含了一个全连接的前馈网络。包括两个线性变换，中间有一个ReLU：</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><h3><span id> </span></h3><h2><span id="3-forward过程整理">3 Forward过程整理</span></h2><p>BERT-Forward</p><ul><li>输入：[batch_size, seq_len]</li><li>Embedding层：[batch_size, seq_len] -&gt; [batch_size, seq_len, hidden_size]</li><li>Multi-head Attention: [batch_size, seq_len, hidden_size] -&gt;  [batch_size, seq_len, hidden_size]<ul><li>计算QKV: [batch_size, seq_len, hidden_size] -&gt; [batch_size, seq_len, hidden_size/num_head] </li><li>计算A并缩放+softmax:  [batch_size, seq_len, hidden_size/num_head] -&gt; [batch_size, seq_length, seq_length]</li><li>计算单头：[batch_size, seq_len, hidden_size/num_head] </li><li>合并多头：[batch_size, seq_len, hidden_size] </li><li>再过一个非线性层：[batch_size, seq_len, hidden_size] </li></ul></li><li>Feed Forward: [batch_size, seq_len, hidden_size] -&gt;  [batch_size, seq_len, hidden_size]</li></ul><p>NSP：  [batch_size, 1, hidden_size] -&gt;  [batch_size, hidden_size] -&gt;  [batch_size, 2]</p><p>MLM: </p><ul><li>全连接：[batch_size, seq_len, hidden_size] -&gt; [batch_size, seq_len, hidden_size] </li><li>embedding矩阵转置的映射：[batch_size, seq_len, hidden_size] -&gt; [batch_size, seq_len, vocab_size]</li><li>计算概率： [batch_size, seq_len, vocab_size] -&gt; [batch_size, seq_len, vocab_size]</li></ul><h2><span id="4-参数量计算">4 参数量计算</span></h2><p>base模型：</p><p>（30522+512 + 2）<em> 768 +  12 </em> （768 <em>（768 / 12） </em> 3 <em> 12   + 768 </em>  768 + 768 <em> 3072 </em> 2 ） =  23,835,648 + 84,934,656 =  108,770,304</p><p>large模型：</p><p>（30522+512 + 2）<em> 1024 +  24 </em> （1024 <em>（1024 / 16） </em> 3 <em> 16   + 1024 </em>  1024  + 1024 <em> 3072 </em> 2 ） = 31,780,864 + 24 * （3,145,728 + 1,048,576 + 6,291,456）= 31,780,864 +  251,658,240 = 283,439,104  ？？？</p><h3><span id="41-embedding参数">4.1 Embedding参数</span></h3><p>[seq_len] -&gt; [seq_len, hidden_size]</p><p>词向量包括三个部分的编码：vocab向量参数，position_id向量参数，segment_id参数</p><p>Bert采用：</p><ul><li>vocab_size=30522</li><li>hidden_size=768</li><li>max_position_embeddings=512</li><li>token_type_embeddings=2</li></ul><p>embedding参数 = （30522+512 + 2）* 768 = 23,835,648</p><h3><span id="42-encoder参数">4.2 Encoder参数</span></h3><p>[seq_len, hidden_size] -&gt; [seq_len, hidden_size]</p><ol><li>Multi-heads1: 计算QKV:  768 <em> （768 / 12）</em> 3 <em> 12  </em> 12</li><li>Multi-heads2: concat后还有个非线性层:  768 <em>  768 </em> 12</li><li>FeedForward: 两层relu连接的前馈： 768 <em> 3072 </em> 2  * 12</li></ol><p>12层一共有：（768 <em>（768 / 12） </em> 3 <em> 12  + 768 </em>  768 + 768 <em> 3072 </em> 2 ） <em> 12 = （1,769,472 +  589,824 + 4,718,592）</em>12 = 7,077,888 * 12 = 84,934,656</p><h2><span id="5-常见问题总结">5 常见问题总结</span></h2><h2><span id="reference">Reference</span></h2><p><a href="https://blog.csdn.net/weixin_43922901/article/details/102602557" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43922901/article/details/102602557</a></p><p><a href="http://www.sniper97.cn/index.php/note/deep-learning/note-deep-learning/3810/" target="_blank" rel="noopener">http://www.sniper97.cn/index.php/note/deep-learning/note-deep-learning/3810/</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BERT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集成学习</title>
      <link href="/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/"/>
      <url>/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-集成学习">1 集成学习</span></h2><p>集成学习，通过<strong>构建</strong>并<strong>结合</strong>多个机器学习器来完成学习任务</p><p>常见的集成学习框架有2种：</p><ol><li>Bagging： 基学习器之间无强依赖关系，可并行生成</li><li>Boosting：有强依赖关系，必须串行生成</li></ol><h3><span id="bagging">Bagging</span></h3><ul><li>有放回的随机采样</li><li>思想：<ol><li>基于自助采样法（bootstrap sample）构造T个样本集</li><li>基于每个采样集训练T个基学习器，然后将这些基学习器结合</li><li>对分类任务采用投票法，对回归任务采用平均法</li></ol></li><li>Bagging可以降低模型方差，但不改变模型偏差</li></ul><p><img src="/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/1042406-20161204200000787-1988863729.png" alt="img" style="zoom: 33%;"></p><h3><span id="boosting">Boosting</span></h3><ul><li>每个基学习器都会在前一个的基础上进行学习，最终结合所有基学习器来预测</li><li><p>Boosting的工作机制：将弱学习器提升为强学习器的算法</p><ol><li>从初始训练集中学习一个基学习器</li><li>基于前一个基学习器，学习下一个基学习器</li><li>重复进行，最终得到所有的弱学习器，并组合成强学习器</li></ol></li><li><p>组合方法涉及两种：</p><ol><li>加法模型</li><li>前向分布算法</li></ol></li></ul><h3><span id="方差amp偏差">方差&amp;偏差</span></h3><p>偏差（Bias）描述的是预测值和真实值之差；</p><p>方差（Variance）描述的是预测值作为随机变量的离散程度。</p><p>基分类器的错误，是偏差和方差之和</p><ul><li>boosting通过逐步聚焦分类器分错的样本，减少集成分类器的<strong>偏差</strong></li><li>Bagging采用分而治之的策略，通过对样本多次采样，分别训练多个模型，减少<strong>方差</strong></li></ul><p>为什么决策树是常用的基分类器？</p><ul><li>决策树受样本和特征集合扰动的影响比较大，随机性大，不稳定的模型适合当基学习器</li></ul><h2><span id="2-随机森林">2 随机森林</span></h2><p>随机森林以决策树为基学习器。是 Bagging 的优化版本。</p><ul><li><p>构建过程：</p><ol><li>随机选择一部分样本：基于自助采样法（bootstrap sample）构造多个样本集</li><li>随机选择一部分特征：随机选取特征集合，特征数远小于原特征集合；</li><li>每棵树都尽最大程度的生长，并且没有剪枝过程；</li><li>进行投票决策</li></ol></li><li><p>优点：</p><ol><li>防止单一决策树过拟合，消除了决策树容易过拟合的缺点</li><li>减少预测的方差（偏差不一定减小</li></ol></li></ul><h2><span id="3-adaboost">3 AdaBoost</span></h2><p><img src="/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/1042406-20161204194331365-2142863547.png" alt="img" style="zoom: 67%;"></p><ul><li><p>AdaBoost （Adaptive Boosting）的思想</p><ul><li>在前一个学习器分错的样本基础上学习第二个弱学习器</li><li>学习第二个弱学习器的方式是：<strong>样本重加权</strong><br>– 分对的样本，其权重减小<br>– 分错的样本，其权重增大</li><li>最终的集成，根据错误率对基分类器的<strong>权重系数</strong>做加权</li></ul></li><li><p><strong>AdaBoost的学习过程</strong><br>有训练数据集   $\mathrm{T}=\left\{\left(\mathrm{x}_{1},\mathrm{y}_{1}\right),\left(\mathrm{x}_{2},  \mathrm{y}_{2}\right), \ldots,\left(\mathrm{x}_{\mathrm{N}}, \mathrm{y}_{\mathrm{N}}\right)\right\}$</p><ol><li><p>初始化训练数据的权值分布</p><script type="math/tex; mode=display">D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N</script></li><li><p>训练一个新的基学习器、计算学习器系数、更新样本权重</p><ol><li><p>根据当前训练数据的权值分布<strong>学习到一个新的基学习器$G_{m}$</strong></p><script type="math/tex; mode=display">G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}</script></li><li><p>计算基学习器在训练集上的分类误差率$e_{m}$</p><script type="math/tex; mode=display">e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)</script></li><li><p>根据分类误差率得到系数$\alpha_{m}$，这个权重系数也是最终分类器组合时的权重：</p><script type="math/tex; mode=display">\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}</script></li><li><p>更新训练数据集的权值分布</p><script type="math/tex; mode=display">\begin{array}{c}{D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)} \\{w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N}\end{array}</script></li></ol></li><li><p>重复步骤2，得到多个基学习器，构建基本分类器的线性组合，并得到最终分类器</p></li></ol><script type="math/tex; mode=display">G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)</script></li></ul><ul><li>一些问题<ul><li>为什么分类误差率使用这个公式：对loss重写使loss公式包含$\alpha_{m}$，对其求偏导后，极值点对应的偏导为0时loss为em，得到这个系数权重</li></ul></li></ul><h2><span id="4-gbdt">4 GBDT</span></h2><h3><span id="bdt">BDT</span></h3><p>以CART树为基学习器的集成学习模型。</p><p><img src="/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/image-20210710190409892.png" alt="image-20210710190409892" style="zoom:50%;"></p><h3><span id="gbdt">GBDT</span></h3><p>GBDT（梯度提升决策树 Gradient Boosting Decision Tree）GBDT 的核心在于累加所有树的结果作为最终结果。</p><p>核心思想是<strong>梯度迭代（Gradient Boosting）</strong>：根据前一个基学习器预测结果与训练样本真实值的残差，来训练下一个基学习器（训练f使得Fk逼近真实值）</p><p><img src="/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/image-20210706212436417.png" alt="image-20210706212436417"></p><p>举个例子：比如说 A 用户年龄 20 岁，第一棵树预测 12 岁，那么残差就是 8，第二棵树用 8 来学习，假设其预测为 5，那么其残差即为 3，如此继续学习即可。</p><p><img src="/2021/07/01/ji-qi-xue-xi/ji-cheng-xue-xi/v2-fedb38d98fdc20eeaea35d966f085836_720w.jpg" alt="img"></p><p>缺点：依赖强、并行难、效率低</p><h2><span id="5-xgboost">5. XGBoost</span></h2><p>基于GBDT，但是进行了若干优化</p><h2><span id="6-lightgbm">6. LightGBM</span></h2><h2><span id="reference">reference：</span></h2><pre><code>《统计学习方法》《机器学习》https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interviewhttps://zhuanlan.zhihu.com/p/87885678https://zhuanlan.zhihu.com/p/162001079https://zhuanlan.zhihu.com/p/148050748https://zhuanlan.zhihu.com/p/86263786https://www.cnblogs.com/pinard/p/6131423.html</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度文本匹配前沿进展survey2021</title>
      <link href="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/"/>
      <url>/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-深度文本匹配">1 深度文本匹配</span></h2><h3><span id="11-任务概述">1.1 任务概述</span></h3><p>文本匹配是一项非常基础且重要的自然语言处理任务。</p><p>文本匹配大多是判断句对是否具有语义相似的关系，例如paraphrase检测，语义文本相似度、问答系统中的问题对匹配、信息检索（query和doc）。</p><p>另外，一些其他句子关系建模的任务与文本匹配比较相关，最主要的关联是任务形式相近或者可以用相似的方法解决，比如文本蕴含、问题答案对匹配等句子对分类任务。</p><p>传统的文本匹配技术例如有TF-IDF、 BM25、Jaccord、SimHash等算法，如BM25算法通过候选句子的字段对qurey字段的覆盖程度来计算两者间的匹配得分，得分越高的候选项与query的匹配度更好。主要解决词汇层面的匹配问题，或者说词汇层面的相似度问题。</p><p>深度文本匹配在过去若干年内，经历了若干阶段的变化，具体的演变这里就不瞎说了，本篇着重讨论匹配任务的前沿进展。</p><h3><span id="1-2-匹配数据集">1. 2 匹配数据集</span></h3><p>其实对句子关系建模的数据集都可以和文本匹配沾边：判断句子匹配标签的、计算语义相似度的、判断句子是否有蕴含关系的、QA数据等等</p><ul><li><p>中文数据集</p><ul><li>文本匹配强相关<ul><li>LCQMC - 通用句对匹配 26w pair（倾向意图匹配而不是paraphrase） <a href="http://icrc.hitsz.edu.cn/info/1037/1146.htm" target="_blank" rel="noopener">http://icrc.hitsz.edu.cn/info/1037/1146.htm</a></li><li>AFQMC - 蚂蚁金服金融 4w pair: <a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li><li>BQ Corpus  - 银行领域  12w pair - <a href="http://icrc.hitsz.edu.cn/Article/show/175.html" target="_blank" rel="noopener">http://icrc.hitsz.edu.cn/Article/show/175.html</a></li></ul></li><li>文本蕴含<ul><li>OCNLI: <a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li><li>CMNLI: <a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li><li>XNLI： <a href="https://github.com/facebookresearch/XNLI" target="_blank" rel="noopener">https://github.com/facebookresearch/XNLI</a> (多语言)</li></ul></li></ul></li></ul><ul><li><p>英文数据集</p><ul><li>文本匹配强相关<ul><li>Quora （QQP）（GLUE）</li><li>The Microsoft Research Paraphrase dataset (MRPC) （GLUE）</li><li>STS-B （语义文本相似度）（GLUE）</li><li>SemEval STS 系列（无监督的测试数据，STS任务常用）</li></ul></li><li>文本蕴含<ul><li>SciTail ： <a href="https://allenai.org/data/scitail" target="_blank" rel="noopener">https://allenai.org/data/scitail</a></li><li>SNLI：<a href="https://nlp.stanford.edu/projects/snli/" target="_blank" rel="noopener">https://nlp.stanford.edu/projects/snli/</a></li><li>XNLI： <a href="https://github.com/facebookresearch/XNLI" target="_blank" rel="noopener">https://github.com/facebookresearch/XNLI</a> (多语言)</li><li>MNLI（GLUE）</li><li>QNLI（GLUE）</li><li>RTE（GLUE）</li></ul></li><li><p>Answer Selection </p><ul><li>TrecQ 5.6w问答对</li><li>The SemEval CQA dataset </li><li>WikiQA   1w问答对</li></ul></li></ul></li></ul><h2><span id="2-学术前沿论文分类">2 学术前沿论文分类！</span></h2><ol><li>专注新颖的角度<br>提出有趣的观点，提出novel的方法论（讲更好听的故事）</li><li>权衡性能与效率<br>对交互式模型的性能、表示层模型的效率 做一个平衡</li><li>专注模型结构与性能<br>从模型设计方面提高匹配任务的性能（搭更好看或者说更复杂的积木）<ol><li>交互型深度匹配模型 - 有监督<ol><li>传统交互型深度匹配模型（少） </li><li>致力于语言模型的预训练（多） :  预训练得到预训练语言模型，将其作为交互式的模型进行finetune</li></ol></li><li>双塔型深度匹配模型 - 无监督&amp;有监督<ol><li>传统表示型模型（少） </li><li>致力于句子表示模型（多） : 预训练得到预训练句子表示模型，将其作为单个句子的表示模型，得到句子表示用来相似度计算</li></ol></li></ol></li><li>特定应用场景或特性<br>基于特定的场景或任务的特性，对匹配任务进行优化</li></ol><p><strong>传统的深度匹配模型分类：</strong></p><ol><li><strong>representation-based method 表示型方法/双塔式方法</strong></li></ol><ul><li>将待匹配的两个对象通过深度学习模型进行表示</li><li>计算这两个表示之间的相似度，便可输出两个对象的匹配度</li></ul><p>侧重表示层的构建，匹配度函数一般固定参数的相似度度量函数 or  可学习的匹配度打分模型 。</p><ol><li><strong>interaction-based method 交互型方法</strong> </li></ol><ul><li>首先基于表示层采用与词位置对应的词向量 </li><li>然后对两个句子按词对应交互，由此构建两段文本之间的 matching pattern，这里面包括了更细致更局部的文本交互信息</li><li>基于该匹配矩阵，可以进一步使用DNN等来提取更高层次的匹配特征，最后计算得到最终匹配得分。</li></ul><p>该方式更强调待匹配的两个句子得到更充分的交互，以及交互后的匹配。</p><p>对比：</p><ul><li>表示型的方式侧重于表示层的构建，由于可以离线计算，效率更高；</li><li>交互型方式建模更加细致、充分，一般来说效果更好，但计算成本增加，更加适合一些效果精度要求高但对计算性能要求不高的场景。</li></ul><h2><span id="3-专注新颖的角度">3 专注新颖的角度</span></h2><p><strong>[1] Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling     ACL 2019 </strong> </p><ul><li><p>动机：相关性匹配和语义匹配的gap，两者信息融合提升效果</p><ul><li>IR中的相关性匹配注重关键词的匹配，看重两者的相关性</li><li>语义匹配通过词汇信息和语句的组成结构，强调整体“含义”的相似关系</li><li>以往的工作证明，NLP中文本相似性建模对于IR任务可能会产生较差的结果，反之亦然。</li></ul></li><li>模型结构：<ul><li>混合的encoder模块，包括deep、wide和contextual<ul><li>Deep Encoder：堆叠多层的卷积层，表示向量经过多层的卷积来得到高层的表示</li><li>Wide Encoder：并行的将表示向量通过拥有不同卷积核的卷积层，分别得到每一层对应的表示向量</li><li>Contextual Encoder：使用双向的LSTM来获取长程依赖特征</li></ul></li><li>相关性模块，两者的表示进行矩阵相乘（内积），得到一个相似性得分矩阵，引入idf权重，做池化</li><li>语义匹配：使用了Co-Attention来计算查询向量和文本向量之间的注意力，将其编码后过LSTM作为语义匹配得分</li></ul></li><li>贡献点：<ul><li>讨论了相关性匹配和语义匹配的区别（模型之间是否适配，相关性和语义匹配的标签关系）</li><li>提出一个新的交互式的模型（一个RM一个SM，和一个结合的）</li><li>3个NLP任务和2个IR数据集上进行实验；关联性和语义匹配信号在许多问题上互补</li></ul></li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617145028723.png" alt="image-20210617145028723"></p><ul><li><strong>[2] Neural Graph Matching Networks for Chinese Short Text Matching ACL 2020</strong> </li></ul><p>中文短文本匹配问题</p><p>动机：基于词的匹配模型，可能会因为中文分词的错误、模糊或不一致，损害最终的匹配性能。</p><p>方法：提出了一种用于中文短文本匹配的神经图匹配方法(GMN)。保留所有可能的分词结果，形成一个lattice图神经图匹配网络，处理多粒度输入信息的匹配框架。</p><p>模型结构：上下文节点嵌入模块(通过BERT得到节点表示)、图匹配模块和关系分类器。</p><p>新颖点：在匹配任务上，引入GNN来解决分词带来的误差，提升匹配任务的性能。</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617154009877.png" alt="image-20210617154009877" style="zoom:50%;"></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20201026154234577.png" alt="image-20201026154234577" style="zoom: 67%;"></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210628130028739.png" alt="image-20210628130028739"></p><ul><li><strong>[3] tBERT: Topic Models and BERT Joining Forcesf or Semantic Similarity Detection ACL 2020</strong> </li></ul><p>解决的任务：语义相似性检测</p><p>核心思想：融合了主题模型和BERT，基于BERT的上下文与主题信息结合</p><p>模型：用基础的BERT对两个句子编码，得到句子对的表示。根据主题模型得到两个句子的文档主题。<br>通过主题模型给句子中每一个单词分配一个主题然后取平均得到单词主题矩阵W。最后一起送到FC</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210618004924080.png" alt="image-20210618004924080"></p><h2><span id="4-权衡性能与效率">4 权衡性能与效率</span></h2><ul><li><strong>[4] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.  2019 EMNLP</strong></li></ul><p>动机：BERT和RoBERTa用于匹配任务需要将比较的两个句子都传入模型中计算，导致了巨大的计算开销（大规模相似度检索中，求得一个问题的检索答案需要：N(N-1)/2）；聚类任务和语义搜索中，BERT映射成的向量效果一般。</p><p>核心思想：训练时通过有监督训练上层的分类函数，推断时通过SBERT模型获取到的句子embedding，可以直接通过cos相似度计算两个句子的相似度，大大减少了计算量</p><p>贡献：提出sentence-BERT，性能不低，效率++。</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617171854526.png" alt="image-20210617171854526"></p><p><strong>[5] Poly-encoders: architectures and pre-trainingstrategies for fast and accurate multi-sentence scoring ICLR 2020 </strong></p><p>动机：Cross-encoders方法相比于Bi-encoders效果好但是慢；</p><p>贡献：提出Poly-encoders，详细比较三种方法的性能和速度，做了一个折中。</p><p>模型核心思想：用多个attention模块获取 query 中一词多义或切词带来的不同语义信息，使得双塔表示进行充分的交互</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617172034578.png" alt="image-20210617172034578"></p><p><strong>[6] DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding. SIGIR2020</strong></p><ul><li>动机：开放域 QA，reRank的基本操作是将问题和检索到的每个文档进行拼接作为 BERT 的输入，输出相关性 score。每一个问题都需要与 retrieve 模块检索出的每一个文档进行拼接，这需要对大量检索文档进行重编码，非常耗时。</li><li>贡献：为了解决效率问题，DC-BERT 提出具有双重 BERT 模型的解耦上下文编码框架：在线的 BERT 只对问题进行一次编码，而离线的 BERT 对所有文档进行预编码并缓存它们的编码。DC-BERT 在文档检索上实现了 10 倍的加速，同时与最先进的开放域 QA 方法相比，保留了大部分(约98%)的 QA 问答性能。</li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617215307134.png" alt="image-20210617215307134"></p><p><strong>[17] ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT  SIGIR 2020</strong> </p><ul><li>动机：bi-encoder的效率问题</li><li>贡献：ColBERT 提出了一种新颖的后期交互范式，用于估计查询 query 和文档 doc 之间的相关性。query 和doc 分别通过各自的 encoder 编码，得到两组 token level 的 embedding 集合；然后，评估 query 和 doc 中的每个 item 的关联，得到快速排序的目的。</li><li>模型结构：ColBERT 的模型结构整体还是类似于 Siamese 结构，分为 Query 端和 Doc 端，最后在进行交互计算文本分相似度。模型主体上分为 Query Encoder  、Document Encoder   、以及之后的 Late Interaction 部分。</li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617220427552.png" alt="image-20210617220427552"></p><p><strong>[7] Simple and Effective Text Matching with Richer Alignment Features. ACL 2019</strong></p><p>动机：少参数量，更快的推理速度，但是效果也并没有很低的模型</p><p>贡献点：他的主要设计理念就是一切从简，embedding只有word embedding, 主干网络采用了CNN，作者尽可能的减少了参数和运算量。一个encoder。三个任务中的四个数据集达到与SOTA相当的水平；最少的参数数量和最快的推理速度；消融实验和替代方案对比</p><p>模型：embedding(1)与上一模块的输出(2)一起扔进Encoder，(1)与(2)与Encoder(3)的输出扔进Attention，(1)与(2)与(3)与Attention(4)的结果一起fusion，然后池化后扔进分类器。</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20201027103336691.png" alt="image-20201027103336691" style="zoom: 67%;"></p><h2><span id="5-专注模型结构与性能">5 专注模型结构与性能</span></h2><h3><span id="51-交互型深度匹配模型">5.1 交互型深度匹配模型</span></h3><h4><span id="511-传统交互型深度匹配模型">5.1.1 传统交互型深度匹配模型</span></h4><p><strong>[1] Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling     ACL 2019 </strong> </p><p>动机：相关性匹配和语义匹配的gap，两者信息融合提升效果</p><ul><li>IR中的相关性匹配注重关键词的匹配，看重两者的相关性</li><li>语义匹配通过词汇信息和语句的组成结构，强调整体“含义”的相似关系</li><li>以往的工作证明，NLP中文本相似性建模对于IR任务可能会产生较差的结果，反之亦然。</li></ul><p>模型结构：</p><ul><li>混合的encoder模块，包括deep、wide和contextual<ul><li>Deep Encoder：堆叠多层的卷积层，表示向量经过多层的卷积来得到高层的表示</li><li>Wide Encoder：并行的将表示向量通过拥有不同卷积核的卷积层，分别得到每一层对应的表示向量</li><li>Contextual Encoder：使用双向的LSTM来获取长程依赖特征</li></ul></li><li>相关性模块，两者的表示进行矩阵相乘（内积），得到一个相似性得分矩阵，引入idf权重，做池化</li><li>语义匹配：使用了Co-Attention来计算查询向量和文本向量之间的注意力，将其编码后过LSTM作为语义匹配得分</li></ul><p>贡献点：</p><ul><li>讨论了相关性匹配和语义匹配的区别（模型之间是否适配，相关性和语义匹配的标签关系）</li><li>提出一个新的交互式的模型（一个RM一个SM，和一个结合的）</li><li>3个NLP任务和2个IR数据集上进行实验；关联性和语义匹配信号在许多问题上互补</li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617145028723.png" alt="image-20210617145028723"></p><h4><span id="512-基于pretrainfinetune的句子表示建模">5.1.2 基于pretrain+finetune的句子表示建模</span></h4><ul><li><strong>[8] Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention ACL 2020</strong></li></ul><p>动机：中文预训练模型中以字符为基本单位而忽略了词的语义，以往工作将词信息融入到文本表示显示有效果</p><p>关键问题：如何将分词信息集成到预训练模型中；分词工具带来的cascading(级联)噪声。</p><p>贡献：提出一种词对齐注意机制，显式利用词的信息。进行了包括6个数据集的5个任务。</p><p>模型：实质上就是在实验看有什么办法去各种拐着弯儿向 character-level 的表示模型融入词信息，并减轻外部分词器引入的分词噪声。1. 通过字符序列和分词序列的attention，得到按词划分的（word-based）字级别（character-level）的 attention 权重组合。然后pooling后和原向量乘到一块。2. 集成了多种分词工具的结果，以减少分词噪声</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617222813476.png" alt="image-20210617222813476"></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210617222959971.png" alt="image-20210617222959971"></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20201027163729424.png" alt="image-20201027163729424" style="zoom:50%;"></p><p><strong>[9] Cross-Thought for Sentence Encoder Pre-training. EMNLP-2020</strong> </p><p><a href="https://mp.weixin.qq.com/s/S03iGgtsXj2cy37F8yjCfg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/S03iGgtsXj2cy37F8yjCfg</a></p><p>动机：针对CLS的定向建模。目前主流的语言模型预训练任务，都采用序列的第一个特殊符号（如：[CLS]）作为句子的表示，但是预训练任务的目标都没有损失直接作用在这个特殊符号上，这样就导致在这个特殊符号上学出来的句子表示很难包含充分的信息。</p><p>主流的预训练都作用在非常长的序列上（比如512），使得模型在恢复被mask的token时，可以直接获取到周边句子的信息。这对于学习有效的上下文token表示很有用，但对于学习句子表示则效率较低，因为信息不会主动聚集到[CLS]符号上。</p><p>贡献：本文提出了Cross-Thought，它将输入文本切分成许多短句，使得恢复某个短句中被mask掉token的信息较难出现在同一个短句中，而不得不依赖于其上下文其它短句的信息。</p><p>模型：<br>预训练：长句子切分为M个短句子，每个短句子前添加N个特殊的token，编码后特殊token处的向量有M*N个，N列的特殊token向量做attention得到加权表示，M*N与M个句子的编码结果分别扔进cross-transformer来预测mask token。<br>finetune：M*N个特殊token表示的attention分数得到ranking loss来排序，或者是M*N个特殊token表示扔到分类器</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616215336802.png" alt="image-20210616215336802"></p><h3><span id="52-双塔型深度匹配模型">5.2 双塔型深度匹配模型</span></h3><h4><span id="521-传统双塔型匹配模型">5.2.1 传统双塔型匹配模型</span></h4><p><strong>[10] Siamese Recurrent Architectures for Learning Sentence Similarity 2016 AAAI</strong></p><p>用共享权重的LSTM编码，取最后一步的输出作为表示，使用了Manhattan距离计算损失</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616232033673.png" alt="image-20210616232033673"></p><h4><span id="522-基于pretrainfinetune的句子表示建模">5.2.2 基于pretrain+finetune的句子表示建模</span></h4><p>偏向句子表示，无监督的匹配</p><ul><li><strong>[11] BERT-flow: On the Sentence Embeddings from Pre-trained Language Models  EMNLP-2020</strong></li></ul><p>动机：没有经过微调，直接由BERT得到的句子表示在语义文本相似性方面明显薄弱，甚至会弱于GloVe得到的表示。</p><p>模型：探究发现，词频影响向量分布：1. 高频词更接近原点；2. 低频词更稀疏。总之：BERT总是将句子映射为一个非光滑的各向异性语义空间中的向量。于是将BERT embedding映射到一个标准高斯隐空间（因为一些此分布听起来高大上的特性：平滑的、各向同性的）</p><p>贡献：一种BERT的句子表示增强方法。</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616230902963.png" alt="image-20210616230902963"></p><ul><li><p><strong>[12] BERT-whitening: Whitening Sentence Representations for Better Semantics and Faster Retrieval   arxiv-2021</strong></p><p>BERT-whitening可看成是<a href="https://arxiv.org/abs/2011.05864" target="_blank" rel="noopener">BERT-flow</a>的最简单实现，这样简单的实现足以媲美一般的BERT-flow模型</p><ul><li>BERT-whitening的思路很简单，就是在得到每个句子的句向量后，对这些矩阵进行一个白化（也就是PCA），使得每个维度的均值为0、协方差矩阵为单位阵，然后保留k个主成分</li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616232236120.png" alt="image-20210616232236120"></p></li><li><p><strong>[13] SimCSE: Simple Contrastive Learning of Sentence Embeddings   arxiv-2021</strong></p><ul><li>动机：更好的句子表示，将对比学习的思想引入了SBERT</li><li>方法：对比学习的本质就是构建正负样本，SimCSE最大的贡献就是提出一种构造正样本的方法。本质上来说就是&lt;样本A的一种表示, 样本A的另一种表示&gt;作为正例、&lt;样本A的一种表示, 样本B的一种表示&gt;作为负例来训练对比学习模型。怎么得到自己的两种表示：两次经过带Dropout的Encoder，两次随机Dropout，得到两次表示</li></ul></li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616215756099.png" alt="image-20210616215756099"></p><p><strong>[14] ConSERT: A Contrastive Framework for Self-Supervised Sentence Representation Transfer  ACL-2021</strong></p><ul><li><p>动机：给定一个类似BERT的预训练语言模型M，以及从目标领域数据分布中收集的无标签文本语料库D，我们希望通过构建自监督任务在D上对M进行Fine-tune，使得Fine-tune后的模型能够在目标任务（用于文本语义匹配的句子表示）上表现最好。</p></li><li><p>提出了一种基于对比学习的句子表示迁移框架ConSERT。一个Batch内的N条样本，通过数据增强，得到2N条，同一个句子得到的两个样本为正样本对，batch内其他的句子作为负样本对。</p></li><li><p>探究了若干数据增强方法。显性的：回译（翻译两遍）、CBERT（mask后补全）、意译（paraphrase模型）；隐性的：对抗攻击（有监督场景）、shuffle（Position Ids进行Shuffle）、裁剪（embedding或token置为0）、dropout（同SimCSE）</p></li></ul><ul><li><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616232733013.png" alt="image-20210616232733013" style="zoom:50%;"></li></ul><p><strong>Meta-Embeddings: Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity. ACL-2020</strong></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616215850772.png" alt="image-20210616215850772"></p><h2><span id="6-特定场景或特性">6 特定场景或特性</span></h2><p><strong>[15] Match²: A Matching over Matching Model for Similar Question Identification SIGIR 2020</strong></p><p>动机：CQA问题，提出了一种相似问题的二次匹配模型，将待匹配问句对应的回答作为连接二者的桥梁，辅助判定待匹配问句是否与 用户问句相似</p><p>模型：第一模块是 Matching Pattern Layer，该模块分别计算两个问题与答案直接的相似性表示。第二模块是 Pattern Similarity Layer，该模块计算以上两种匹配模式之间的相似性  作为两个问题的相似性表示。第三模块是 Compression Layer，将 Pattern Similarity和QQ相似度一起做分类</p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210618002305256.png" alt="image-20210618002305256"></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210618002724185.png" alt="image-20210618002724185"></p><p>匹配模型的少样本迁移 EMNLP-2020</p><p>[16] 《MultiCQA: Zero-Shot Transfer of Self-Supervised Text Matching Models on a Massive Scale》</p><p>非对称域文本匹配 EMNLP-2020</p><p>[18] Wasserstein Distance Regularized Sequence Representation for Text Matching in Asymmetrical Domains</p><ul><li>动机：随着训练的进行，从不同域投影的特征向量往往难以区分。</li></ul><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210618003324915.png" alt="image-20210618003324915"></p><p><img src="/2021/06/09/nlp-jin-jie/shen-du-wen-ben-pi-pei-qian-yan-jin-zhan-survey2021/image-20210616220954575.png" alt="image-20210616220954575"></p><h2><span id="reference">Reference</span></h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI0NTg1MTI1NQ==&amp;amp;mid=2247484166&amp;amp;idx=1&amp;amp;sn=99e78d78013092e6b712d5a4e5efd487&amp;amp;chksm=e949761ede3eff080f381dfb5b618282d76a2349c1b3d664d05a1d39878e5d1f744219e6cd7e&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=1022VqCPD7HGFwpBSOtBqvWe&amp;amp;sharer_sharetime=1603375088507&amp;amp;sharer_shareid=59332ea7c33ee752808701f0287171ae#rd" target="_blank" rel="noopener">基于深度学习的FAQ问答系统</a></p><p><a href="https://zhuanlan.zhihu.com/p/357864974" target="_blank" rel="noopener">21个经典深度学习句间关系模型｜代码&amp;技巧</a></p><p><a href="https://blog.csdn.net/iin729/article/details/111942096" target="_blank" rel="noopener">https://blog.csdn.net/iin729/article/details/111942096</a></p><p><a href="https://mp.weixin.qq.com/s/S03iGgtsXj2cy37F8yjCfg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/S03iGgtsXj2cy37F8yjCfg</a></p><p><a href="https://blog.csdn.net/Forlogen/article/details/103610766" target="_blank" rel="noopener">https://blog.csdn.net/Forlogen/article/details/103610766</a></p><p><a href="https://blog.csdn.net/qq_43390809/article/details/114077216" target="_blank" rel="noopener">https://blog.csdn.net/qq_43390809/article/details/114077216</a></p><p><a href="https://blog.csdn.net/xixiaoyaoww/article/details/108525940" target="_blank" rel="noopener">https://blog.csdn.net/xixiaoyaoww/article/details/108525940</a></p><p><a href="https://blog.csdn.net/qq_22472047/article/details/109175911" target="_blank" rel="noopener">https://blog.csdn.net/qq_22472047/article/details/109175911</a></p><p> <a href="https://mp.weixin.qq.com/s/S03iGgtsXj2cy37F8yjCfg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/S03iGgtsXj2cy37F8yjCfg</a></p><p><a href="https://kexue.fm/archives/8321" target="_blank" rel="noopener">https://kexue.fm/archives/8321</a> </p><p><a href="https://km.sankuai.com/page/856931405" target="_blank" rel="noopener">ACL2021｜美团提出基于对比学习的文本表示模型，效果相比BERT-flow提升8%</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度文本匹配survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实体对齐survey</title>
      <link href="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/"/>
      <url>/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-实体对齐">1 实体对齐</span></h2><h3><span id="11-任务定义">1.1 任务定义</span></h3><p>实体对齐(Entity alignment) ，也称为实体匹配（Entity Matching），就是找到两个知识图谱中相同的等价实体。0</p><p>实体对齐任务进行定义：</p><ul><li>用 $G=(E,R,A,T_R,T_A)$表示知识图谱，其中$E,R,A$表示其中的实体、关系和属性。 $T_R$是关系三元组 E−−R−−E 。$T_A$ 是属性三元组 E−−A−−V。</li><li>已知有知识图谱 G1 和 G2，目的是找到两个图谱中的等价实体集合$ S(ei1,ei2)$。</li></ul><p>用处：</p><p>不同的知识库，收集知识的侧重点不同。知识融合的目的就是将不同知识图谱对实体的描述进行整合，从而获得实体的完整描述。</p><p>任务难点：</p><ol><li>计算复杂度（候选实体数量规模）</li><li>数据质量的影响（需要算法对不同的数据情况制定合适的算法）</li><li>算法的设计（如何结合实体信息对实体进行表示）</li></ol><h3><span id="12-相关知识库与数据集">1.2 相关知识库与数据集</span></h3><p><strong>知识库</strong></p><ul><li><p>FreeBase</p><p>Freebase是个类似wikipedia的<a href="https://baike.baidu.com/item/创作共享/7570522" target="_blank" rel="noopener">创作共享</a>类网站，所有内容都由用户添加，采用创意共用许可证，可以自由引用。两者之间最大的不同在于，Freebase中的条目都采用<a href="https://baike.baidu.com/item/结构化数据/5910594" target="_blank" rel="noopener">结构化数据</a>的形式，而wikipedia不是。</p></li><li><p>DBPedia： DBpedia 是一个很特殊的语义网应用范例，它从<a href="https://baike.baidu.com/item/维基百科/106382" target="_blank" rel="noopener">维基百科</a>(Wikipedia)的词条里撷取出结构化的资料，以强化维基百科的搜寻功能，并将其他资料集连结至维基百科</p></li><li><p>维基百科本体知识库</p></li><li><p>Omega</p></li><li><p>YAGO</p></li></ul><ul><li>DBP15K（<a href="http://ws.nju.edu.cn/jape/）" target="_blank" rel="noopener">http://ws.nju.edu.cn/jape/）</a><br>3种跨语言的实体对齐语料</li></ul><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/dbp15k.PNG" alt="img"></p><ul><li><p>DWY100K （<a href="https://github.com/nju-websoft/BootEA）" target="_blank" rel="noopener">https://github.com/nju-websoft/BootEA）</a></p><p>2种大规模的数据集</p></li></ul><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/1619412168183.png" alt="1619412168183"></p><ul><li><p>DFB datasets （<a href="https://github.com/thunlp/IEAJKE）" target="_blank" rel="noopener">https://github.com/thunlp/IEAJKE）</a><br> extracted from Freebase</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/1619412850542.png" alt="1619412850542"></p></li></ul><ul><li>Wk3l60k （《Co-training Embeddings of Knowledge Graphs and Entity Descriptions for Cross-lingual Entity Alignment》）</li></ul><p>extracted from the subset of DBpedia</p><p>用于半监督跨语言学习</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/1619413546413.png" alt="1619413546413"></p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/1619413081309.png" alt="1619413081309"></p><h3><span id="13-评价指标">1.3 评价指标</span></h3><p>Hits@N, 表示排名前 k 的候选中正确对齐的实体的比例。</p><p>MRR ： 是一个国际上通用的对<a href="https://baike.baidu.com/item/搜索算法" target="_blank" rel="noopener">搜索算法</a>进行评价的机制，即第一个结果匹配，分数为1，第二个匹配分数为0.5，第n个匹配分数为1/n，如果没有匹配的句子分数为0。最终的分数为所有得分之和</p><h2><span id="2-模型or方法survey">2. 模型or方法survey</span></h2><p>有几个技术体系：</p><ul><li>TransE系列</li></ul><p>简单来说，TransE就是讲知识图谱中的实体和关系看成两个Matrix。实体矩阵结构为 <img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/equation-1619082537319.svg" alt="[公式]"> ，其中n表示实体数量，d表示每个实体向量的维度，矩阵中的每一行代表了一个实体的词向量；而关系矩阵结构为 <img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/equation.svg" alt="[公式]"> ，其中r代表关系数量，d表示每个关系向量的维度。TransE训练后模型的理想状态是，从实体矩阵和关系矩阵中各自抽取一个向量，进行L1或者L2运算，得到的结果近似于实体矩阵中的另一个实体的向量，从而达到通过词向量表示知识图谱中已存在的三元组 <img src="https://www.zhihu.com/equation?tex=%EF%BC%88h%2C+l%2C+t%EF%BC%89" alt="[公式]"> 的关系。</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-dui-qi-survey/1619082650523.png" alt="1619082650523" style="zoom:50%;"></p><ul><li>TransE + GCN 系列</li></ul><p>联合 知识嵌入模型  和  interaction图模型的结合体</p><ul><li>TransE + 对抗网络系列</li></ul><ul><li>GCN变种</li></ul><ul><li>融合图结构信息与边信息的多维度方法</li></ul><ul><li>预训练模型的方法</li></ul><p>BERT-INT 《 A BERT-based Interaction Model For Knowledge Graph Alignment》</p><h2><span id="reference">Reference</span></h2><p><a href="http://pelhans.com/2019/03/18/entity_alignment/" target="_blank" rel="noopener">http://pelhans.com/2019/03/18/entity_alignment/</a></p><p><a href="https://www.ijcai.org/proceedings/2018/0556.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2018/0556.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2019/0754.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2019/0754.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2020/0506.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2020/0506.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2019/0448.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2019/0448.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2018/0611.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2018/0611.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2017/0595.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2017/0595.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2019/0733.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2019/0733.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2019/0929.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2019/0929.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2019/0754.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2019/0754.pdf</a></p><p><a href="https://www.ijcai.org/proceedings/2018/0556.pdf" target="_blank" rel="noopener">https://www.ijcai.org/proceedings/2018/0556.pdf</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实体对齐survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实体链接survey</title>
      <link href="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/"/>
      <url>/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-实体链接">1 实体链接</span></h2><h3><span id="11-任务定义">1.1 任务定义</span></h3><p>实体链接，即 命名实体识别+命名实体歧义消除（到知识库）。</p><p>例子：</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618990310893.png" alt="1618990310893"></p><p>形式化定义：</p><p>Entity Recognition (ER) and Entity Disambiguation (ED)</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618991552050.png" alt="1618991552050" style="zoom:50%;"></p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618991563611.png" alt="1618991563611" style="zoom:50%;"></p><h3><span id="12-相关数据集">1.2 相关数据集</span></h3><p>AIDA CoNLL-YAGO数据集</p><h4><span id="tac-kbp英语实体链接综合和评估数据2010">TAC KBP英语实体链接综合和评估数据2010</span></h4><h3><span id="13-评价指标">1.3 评价指标</span></h3><p>实体相关度的评价</p><p>nDCG@1 、nDCG@5 、 nDCG@10 、 MAP </p><p>仅实体消歧的评价</p><ul><li>Micro-Precision: Fraction of correctly disambiguated named entities in the full corpus.</li><li>Macro-Precision: Fraction of correctly disambiguated named entities, averaged by document.</li></ul><p>end-to-end 的评价</p><ul><li>Gerbil Micro-F1 - strong matching: micro InKB F1 score for correctly linked and disambiguated mentions in the full corpus as computed using the Gerbil platform. InKB means only mentions with valid KB entities are used for evaluation.</li><li>Gerbil Macro-F1 - strong matching: macro InKB F1 score for correctly linked and disambiguated mentions in the full corpus as computed using the Gerbil platform. InKB means only mentions with valid KB entities are used for evaluation.</li></ul><h3><span id="14-开源工具">1.4 开源工具</span></h3><p><a href="https://github.com/informagi/REL" target="_blank" rel="noopener">https://github.com/informagi/REL</a></p><p><a href="https://github.com/facebookresearch/BLINK" target="_blank" rel="noopener">https://github.com/facebookresearch/BLINK</a></p><h2><span id="2-基础方法">2. 基础方法</span></h2><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618992254711.png" alt="1618992254711"></p><p>EL主要包括两个步骤：</p><ol><li>实体识别，从普通文本中提取出实体mention</li><li>实体消歧，为给定提及预测相应的实体。<br>实体消歧又分为两个步骤：<ol><li>候选实体的生成、可能实体的产生</li><li>实体排序，涉及 上下文/mention-候选实体 相似性分数的计算</li></ol></li></ol><h3><span id="21-候选实体生成">2.1 候选实体生成</span></h3><p>这一步是根据mention给出一个模棱两可的实体列表</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618992439845.png" alt="1618992439845" style="zoom:50%;"></p><p>surface form matching ：大致是根据n-gram、levenshtein distance等进行模糊匹配</p><p>dictionary lookup：构建附加别名的词典（例如使用wiki生成别名字典等）</p><p>prior probability ：预先计算好每个mention链接到entity的超链接数（例如wiki百科词条上的链接）</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618992626717.png" alt="1618992626717"></p><h3><span id="22-context-mention-encoding">2.2 Context-Mention Encoding</span></h3><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618992776331.png" alt="1618992776331" style="zoom:50%;"></p><p>本质上是各种句子级的编码方式：RNN、RNN+Attention、BERT</p><p>不同点在于可能涉及到：用各种方法表示mention向量、让mention之间交互得到全局语义</p><h3><span id="313-entity-encoding">3.1.3. Entity Encoding</span></h3><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618992821368.png" alt="1618992821368" style="zoom:50%;"></p><p>一般情况下，类似于词级别的分布式表示</p><p>根据数据源，实体表示模型可能有很大的不同</p><h3><span id="313-entity-ranking">3.1.3. Entity Ranking</span></h3><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618993429290.png" alt="1618993429290" style="zoom:50%;"></p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618992920721.png" alt="1618992920721" style="zoom:67%;"></p><p>另外，某些mention不存在知识库中的对应实体，需要有方法判断是否不存在：设立阈值、候选实体中额外加一个NIL实体、分类时对是否存在进行额外分类等</p><h2><span id="3-改进方法">3 改进方法</span></h2><h3><span id="31-nrnd联合建模">3.1 NR+ND联合建模</span></h3><h3><span id="32-global-context-architectures">3.2 Global Context Architectures</span></h3><p>对每个mention的候选实体进行关联强度的计算</p><p>弊端是组合数量规模大、关联强度的标注比较困难</p><p><img src="/2021/04/21/nlp-jin-jie/shi-ti-lian-jie-survey/1618994086807.png" alt="1618994086807"></p><h3><span id="33-domain-independent-architectures">3.3 Domain-Independent Architectures</span></h3><p>主要是克服标注数据的问题</p><p>半监督、无监督、远程监督、zero-shot的方法</p><h3><span id="34-cross-lingual-architectures">3.4 Cross-lingual Architectures</span></h3><h2><span id="4-跨空间的实体关联">4 跨空间的实体关联？</span></h2><p>比如又链接到维基百科又链接到百度百科？</p><p>近几年的工作都是从单个实体库里链接实体。因为所有数据集标的都是WikiPedia。</p><p>有zero-shot实体链接，是研究怎么泛化到一个新的实体库的。但是没有那种，可以测试多个实体库的测试数据？</p><p>这种实体库都是那种大而全的吗？有没有那种分类的？基本都是wikidata那种通用实体库</p><h2><span id="reference">Reference</span></h2><p>《Neural Entity Linking: A Survey of Models based on Deep Learning》</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实体链接survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Harbor离线部署</title>
      <link href="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/"/>
      <url>/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/</url>
      
        <content type="html"><![CDATA[<h2><span id="安装docker-docker-compose">安装Docker、Docker-compose</span></h2><p>docker之前安装参考其他</p><p>docker-compose 离线包地址：</p><p><a href="https://github.com/docker/compose/releases" target="_blank" rel="noopener">https://github.com/docker/compose/releases</a></p><p>给予可执行权限：</p><pre><code>cp docker-compose-Linux-x86_64 /usr/bin/docker-composechmod +x /usr/bin/docker-compose</code></pre><p>查看版本：</p><pre><code>docker-compose -v</code></pre><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619510471441.png" alt="1619510471441"></p><h2><span id="harbor安装">Harbor安装</span></h2><p>离线包下载：<a href="https://github.com/goharbor/harbor/releases" target="_blank" rel="noopener">https://github.com/goharbor/harbor/releases</a></p><p>解压：</p><pre><code>tar -zxf harbor-offline-installer-v2.2.1.tgz -C /usr/local/</code></pre><p>修改harbor.yml</p><pre><code>cd /usr/local/harbor cp harbor.yml.tmpl harbor.ymlvi harbor.yml</code></pre><p>注释掉https，修改端口号：</p><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619682950573.png" alt="1619682950573"></p><p>安装</p><pre><code>docker load -i /usr/local/harbor/harbor.v2.2.1.tar.gzcd /usr/local/harbor./prepare ./install.sh</code></pre><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619510673967.png" alt="1619510673967"></p><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619510736486.png" alt="1619510736486"></p><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619510776721.png" alt="1619510776721"></p><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619510785441.png" alt="1619510785441"></p><p>五）harbor的控制</p><pre class=" language-lang-text"><code class="language-lang-text">cd /usr/local/harbor/docker-compose up -d 启动 docker-compose stop 停止 docker-compose restart 重新启动</code></pre><p><img src="/2021/04/06/gong-cheng-bu-shu-xiang-guan/harbor-chi-xian-bu-shu/1619683011292.png" alt="1619683011292"></p><h2><span id="排查故障">排查故障</span></h2><p>端口问题？</p><pre><code>查看端口是否开启netstat -an | grep 748打开端口firewall-cmd --zone=public --add-port=748/tcp --permanent</code></pre><h2><span id="reference">Reference</span></h2><p><a href="https://zhuanlan.zhihu.com/p/280751946" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/280751946</a></p><p><a href="https://blog.csdn.net/ywd1992/article/details/106012742" target="_blank" rel="noopener">https://blog.csdn.net/ywd1992/article/details/106012742</a></p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Harbor离线部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练模型Fine-Tuning微调优化</title>
      <link href="/2021/03/11/nlp-jin-jie/yu-xun-lian-mo-xing-fine-tuning-wei-diao-you-hua/"/>
      <url>/2021/03/11/nlp-jin-jie/yu-xun-lian-mo-xing-fine-tuning-wei-diao-you-hua/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-fine-tuning">1 Fine-Tuning</span></h2><p>预训练语言模型的微调，本质上是迁移学习的一种应用。</p><p>将先进的语言模型在大规模语料上进行预训练后，在下游任务上进行微调，使得预训练语言模型迁移并充分适应下游任务。</p><ol><li>针对微调技巧本身 or 从不同角度对微调做优化</li><li>根据任务特性做优化</li><li>对微调过程或微调后的模型进行分析</li></ol><h2><span id="2-专注微调技巧">2 专注微调技巧</span></h2><h3><span id="2020-acl-smart">2020 ACL </span></h3><p>SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization</p><p>动机：下游任务的数据资源有限，并且预训练模型的复杂性极高，主动微调通常会导致微调模型过度适合下游任务的训练数据，而无法推广到看不见的数据。 为了解决这个问题，提出了一个新的学习框架，对预训练模型进行健壮和高效的微调，以获得更好的泛化性能。</p><p>具体方法：所提出的框架包含两个重要成分：1.平滑度诱导正则化，可有效管理模型的复杂性；  2. Bregman近点优化，它是trustregion方法的一个实例，可以防止主动更新。 </p><p>应用的任务：GLUE, SNLI, SciTail and ANLI等通用benchmark</p><h3><span id="2020-emnlp-recall-and-learn">2020 EMNLP </span></h3><p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.634.pdf" target="_blank" rel="noopener">Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting</a></p><p>动机：微调过程中导致了灾难性遗忘问题，为了减少遗忘，文章提出一种recall and learn 机制</p><p>具体方法：多任务学习共同学习预训练任务和下游任务</p><p>应用的任务：GLUE基准</p><h3><span id="2019-icml-adapter-bert">2019 ICML Adapter BERT</span></h3><p>Parameter-Efficient Transfer Learning for NLP </p><p>存在大量下游任务时，微调的参数效率低，每个任务都需要一个全新的模型。</p><p>论文提出加入adapter模块进行传输，这样每个任务仅添加了一个适配器模块。把最后的<br>task-special layer 放到模型中间，然后冻住预训练模型参数。</p><p><img src="/2021/03/11/nlp-jin-jie/yu-xun-lian-mo-xing-fine-tuning-wei-diao-you-hua/1615447893006.png" alt="1615447893006" style="zoom: 67%;"></p><h3><span id="2018-ulmfit-判别式微调">2018 ULMFiT 判别式微调</span></h3><p>Universal language model fine-tuning for text classification. </p><h3><span id="2020-mixout">2020 Mixout</span></h3><p>Mixout: Effective regularization to finetune large-scale pretrained language models. </p><h2><span id="3-根据任务特性微调优化">3 根据任务特性微调优化</span></h2><h3><span id="2020-acl-adversarial-and-domain-aware-bert">* 2020 ACL </span></h3><p><a href="https://www.aclweb.org/anthology/2020.acl-main.370.pdf" target="_blank" rel="noopener">Adversarial and Domain-Aware BERT for Cross-Domain Sentiment Analysis</a></p><p>解决跨域情感分析，即从源域学到的情感分类器，来预测目标领域的句子情感极性。</p><p>设计了一个post-training方法</p><p>pre-training 任务，MaskLM任务+领域区分。鼓励BERT提取特定领域的特征，利用了无标记数据。</p><p>通过对抗训练提升finetuning。</p><p>应用的任务：Amazon reviews benchmark</p><h3><span id="2020-emnlp-in-and-out-of-distribution-data">2020 EMNLP  </span></h3><p>Calibrated Language Model Fine-Tuning for In- and Out-of-Distribution Data</p><p>动机：由于<strong>过度参数化</strong>（微调过程中少量标记数据的过拟合），针对分布内和分布外（OOD）数据，经过微调的预训练语言模型可能会遭受严重的失调。</p><p>失调的表现：对分布内数据预测的reliability diagrams不均匀，分布外数据预测的高置信度分布占比更大</p><p><img src="/2021/03/11/nlp-jin-jie/yu-xun-lian-mo-xing-fine-tuning-wei-diao-you-hua/1615190901558.png" alt="1615190901558" style="zoom:50%;"></p><p>具体方法：引入了两种类型的正则化以进行更好的校准：（1）流形正则化，它通过在数据流形内进行插值生成伪流形样本。 使用这些伪样本进行增强训练会进行平滑度正则化，以改善分布内校准。  （2）非流形正则化，这鼓励模型输出伪非流形样本的均匀分布，以解决OOD数据的过分置信问题。 </p><p>应用的任务：根据多分类任务数据，构建了分布内训练集、分布内测试集、分布外测试集，在六个数据集上的预期校准误差，误分类检测和OOD检测方面均优于现有的文本分类校准方法。</p><h3><span id="2020-emnlp-meta-fine-tuning">2020 EMNLP </span></h3><p><a href="https://www.aclweb.org/anthology/2020.emnlp-main.250.pdf" target="_blank" rel="noopener">Meta Fine-Tuning Neural Language Models for Multi-Domain Text Mining</a></p><p>动机：微调过程忽略了不同领域中相似NLP任务的相互关联，文章提出一种学习程序MFT，解决一组类似的NLP任务</p><p><img src="/2021/03/11/nlp-jin-jie/yu-xun-lian-mo-xing-fine-tuning-wei-diao-you-hua/1615192345006.png" alt="1615192345006" style="zoom:50%;"></p><p>具体方法：多数据集的多任务学习、获取高度可转移的知识、优化领域corruption loss?</p><p>应用的任务：多个领域的文本挖掘任务（句对分类-蕴含任务、评论检测-分类任务、词对分类）</p><h3><span id="2020-findings-domain-adversarial-fine-tuning">* 2020 Findings </span></h3><p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.278.pdf" target="_blank" rel="noopener">Domain Adversarial Fine-Tuning as an Effective Regularizer</a>   短文</p><p>动机：标准微调会降低预训练期间捕获的通用领域的表示。文章引入一种新的正则化技术作为调节器</p><p><img src="/2021/03/11/nlp-jin-jie/yu-xun-lian-mo-xing-fine-tuning-wei-diao-you-hua/1615193963439.png" alt="1615193963439" style="zoom:50%;"></p><p>具体方法：不同领域样本的对抗训练，外接一个领域分类器，让这个分类器搞不懂sample来源于哪个领域，以提升鲁棒性。<strong>寻求对主任务具有区分性且对域分类器不具有区分性的表示形式。</strong></p><p>应用的任务：部分GLUE基准：CoLA SST-2 MRPC RTE</p><p>还没搞懂：两个规模不一致数据集的pipeline</p><h3><span id="2020-coling-fine-tuning-bert-for-low-resource-nlu">2020 COLING </span></h3><p><a href="https://www.aclweb.org/anthology/2020.coling-main.100.pdf" target="_blank" rel="noopener">Fine-tuning BERT for Low-Resource Natural Language Understanding via Active Learning</a></p><p>动机：低资源（训练数据&lt;1000）的finetune</p><p>具体方法：最大化近似知识获取balabala，分析了微调过程中冻结语言模型层的好处</p><p>应用的任务：GLUE的四个任务</p><h3><span id="2020-findings-tri-train">2020 Findings </span></h3><p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.429.pdf" target="_blank" rel="noopener">Tri-Train: Automatic Pre-Fine Tuning between Pre-Training and Fine-Tuning for SciNER</a></p><p>动机：NER 任务，pretrain-finetune 的模式取决于数据领域和任务之间的相关性。目标领域较小时就不行了。</p><p>具体方法：文章在预训练和微调之间加入 预微调 步骤</p><p>应用的任务：NER的七个基准</p><h3><span id="其他">其他</span></h3><p>2020 COLING</p><p><a href="https://www.aclweb.org/anthology/2020.coling-main.482.pdf" target="_blank" rel="noopener">Unsupervised Fine-tuning for Text Clustering</a></p><p><strong>2020Findings</strong> </p><p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.289.pdf" target="_blank" rel="noopener">Towards Zero-Shot Conditional Summarization with Adaptive Multi-Task Fine-Tuning</a></p><p>条件摘要、zero-shot、多任务学习</p><p><a href="https://www.aclweb.org/anthology/2020.nlpcss-1.17.pdf" target="_blank" rel="noopener">Mapping Local News Coverage: Precise location extraction in textual news content using fine-tuned BERT based language model</a></p><p><a href="https://www.aclweb.org/anthology/2020.wmt-1.118.pdf" target="_blank" rel="noopener">Two-Phase Cross-Lingual Language Model Fine-Tuning for Machine Translation Quality Estimation</a></p><p><a href="https://www.aclweb.org/anthology/2020.wnut-1.20.pdf" target="_blank" rel="noopener">Fine-Tuning MT systems for Robustness to Second-Language Speaker Variations</a></p><p>2020 EMNLP</p><p> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.177.pdf" target="_blank" rel="noopener">Pronoun-Targeted Fine-tuning for NMT</a></p><p>一类条件生成-区分混合损失，用于微调训练有素的机器翻译模型。 </p><p>通过微调改善代词翻译，在代词基准测试集上测试</p><h2><span id="4-微调过程分析-相关工作">4 微调过程分析-相关工作</span></h2><h3><span id="2020-acl-berts-attention-change">2020 ACL  </span></h3><p>How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope</p><p>动机：预训练模型在finetune后如何决策的解释依然很困难，文章提出一个程序和分析方法，该方法关于transformer系列的模型如何编码语义现象提出了假设并进行了验证。</p><h3><span id="2020-acl-automated-essay-scoring">2020 ACL </span></h3><p>Should You Fine-Tune BERT for Automated Essay Scoring?</p><p>动机：分析预训练语言模型是否是自动作文评分的合适技术选择</p><h3><span id="2020-emnlp-a-rigorous-study-on-ner">2020 EMNLP </span></h3><p><strong>A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?</strong></p><p>动机：微调PLM已经在标准NER基准上性能不错，但是开放状态就不行了，也没有这方面数据集。文章建议对标准基准进行随机测试。</p><p>结论：NER任务的一些结论</p><h3><span id="2020-findings-enhancing-automated-essay-scoring-performance">2020 Findings </span></h3><p><a href="https://www.aclweb.org/anthology/2020.findings-emnlp.141.pdf" target="_blank" rel="noopener">Enhancing Automated Essay Scoring Performance via Fine-tuning Pre-trained Language Models with Combination of Regression and Ranking</a></p><p>动机：AES(Automated Essay Scoring) 在最近的工作中，通过各种方法微调并使用浅层神经网络来进行文章表示，捕获文本深层语义较差。文章提出一种新方法来微调AES任务。</p><p><strong>2020 EMNLP workshop</strong></p><p><a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.4.pdf" target="_blank" rel="noopener">What Happens To BERT Embeddings During Fine-tuning?</a></p><p><a href="https://www.aclweb.org/anthology/2020.blackboxnlp-1.7.pdf" target="_blank" rel="noopener">On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers</a></p><h2><span id="reference">Reference</span></h2>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 预训练模型Fine-Tuning微调优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对比学习</title>
      <link href="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/"/>
      <url>/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2><span id="对比学习">对比学习</span></h2><p>核心思想 - 优化目标：</p><p><img src="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/clipboard-1608625450760.png" alt="img"></p><p><img src="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/clipboard-1608625444336.png" alt="img" style="zoom: 50%;"></p><p>典型的比较函数：</p><p><img src="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/clipboard.png" alt="img" style="zoom:50%;"></p><p>研究的方面：</p><ul><li>如何定义目标函数？</li><li>如何构建正负样本、比例等</li></ul><h2><span id="相关工作">相关工作</span></h2><h3><span id="arxiv-2018-cpc">Arxiv - 2018 - CPC</span></h3><p>Representation Learning with Contrastive Predictive Coding</p><p>对比loss infoNCE</p><h3><span id="cvpr-2020-moco">CVPR 2020 - MoCo</span></h3><p>Momentum Contrast for Unsupervised Visual Representation Learning  </p><p><img src="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/image-20201222163030380.png" alt="image-20201222163030380"></p><p>Memory Bank 提出把所有样本的表示都存起来，然后每次随机采样</p><p>MoCo 把负例样本的 encoder 和 mini-batch 大小解耦，用一个 queue 来维护当前的 negative candidates pool，对于负例样本的参数，采用 Momentum update 的方式，来把正例 encoder 的参数copy给负例</p><p><img src="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/image-20201222162948546.png" alt="image-20201222162948546" style="zoom: 67%;"></p><h3><span id="icml-2020-simclr">ICML 2020 - SimCLR</span></h3><p>A Simple Framework for Contrastive Learning of Visual Representations</p><p>SimCLR 着重于构建负例的方式</p><p>有效结论：</p><p>（1） 数据增强的组合在定义有效的预测任务方面起着关键作用；（但是这里是CV）<br>（2） 在表示和对比损失之间引入一个<strong>可学习的非线性变换</strong>，大大提高了学习表示的质量。<br>（3） 与有监督学习相比，对比学习可以从更大的batch size和更多的训练步骤中获益。</p><p><img src="/2020/12/22/nlp-jin-jie/dui-bi-xue-xi/image-20201222164246168.png" alt="image-20201222164246168" style="zoom:67%;"></p><h2><span id="一些论文">一些论文</span></h2><p><strong>2020</strong></p><p>  •Contrastive Representation Learning: A Framework and Review, Phuc H. Le-Khac</p><p>  •Supervised Contrastive Learning, Prannay Khosla, 2020, [pytorch*]</p><p>  •A Simple Framework for Contrastive Learning of Visual Representations, Ting Chen, 2020, [pytroch, tensorflow*]</p><p>  •Improved Baselines with Momentum Contrastive Learning, Xinlei Chen, 2020, [tensorflow]</p><p>  •Contrastive Representation Distillation, Yonglong Tian, ICLR-2020 [pytorch*]</p><p>  •COBRA: Contrastive Bi-Modal Representation Algorithm, Vishaal Udandarao, 2020</p><p>  •What makes for good views for contrastive learning, Yonglong Tian, 2020</p><p>  •Prototypical Contrastive Learning of Unsupervised Representations, Junnan Li, 2020</p><p>  •Contrastive Multi-View Representation Learning on Graphs, Kaveh Hassani, 2020</p><p>  •DeCLUTR: Deep Contrastive Learning for Unsupervised Textual Representations, John M. Giorgi, 2020</p><p>  •On Mutual Information in Contrastive Learning for Visual Representations, Mike Wu, 2020</p><p>  •Semi-Supervised Contrastive Learning with Generalized Contrastive Loss and Its Application to Speaker Recognition, Nakamasa Inoue, 2020</p><p><strong>2019</strong></p><p>  •Momentum Contrast for Unsupervised Visual Representation Learning, Kaiming He, 2019, [pytorch]</p><p>  •Data-Efficient Image Recognition with Contrastive Predictive Coding, Olivier J. Hénaff, 2019</p><p>  •Contrastive Multiview Coding, Yonglong Tian, 2019, [pytorch*]</p><p>  •Learning deep representations by mutual information estimation and maximization, R Devon Hjelm, ICLR-2019, [pytorch]</p><p>  •Contrastive Adaptation Network for Unsupervised Domain Adaptation, Guoliang Kang, CVPR-2019</p><p><strong>2018</strong></p><p>  •Representation learning with contrastive predictive coding, Aaron van den Oord, 2018, [pytorch]</p><p>  •Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination, Zhirong Wu, CVPR-2018, [pytorch*]</p><p>  •Adversarial Contrastive Estimation, Avishek Joey Bose, ACL-2018,</p><p><strong>Before2017 </strong></p><p>  •Time-Contrastive Networks: Self-Supervised Learning from Video, Pierre Sermanet, CVPR-2017</p><p>  •Contrastive Learning for Image Captioning, Bo Dai, NeurIPS-2017, [lua*]</p><p>  •Noise-contrastive estimation for answer selection with deep neural networks, Jinfeng Rao, 2016, [torch]</p><p>  •Improved Deep Metric Learning with Multi-class N-pair Loss Objective, Kihyuk Sohn, NeurIPS-2016, [pytorch]</p><p>  •Learning word embeddings efficiently with noise-contrastive estimation, Andriy Mnih, NeurIPS-2013,</p><p>  •Noise-contrastive estimation: A new estimation principle for unnormalized statistical models, Michael Gutmann, AISTATS 2010, [pytorch]</p><p>  •Dimensionality Reduction by Learning an Invariant Mapping, Raia Hadsell, 2006</p><h2><span id="reference">Reference</span></h2><p><a href="https://zhuanlan.zhihu.com/p/141141365" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/141141365</a></p><p>Representation Learning with Contrastive Predictive Coding</p><p>Momentum Contrast for Unsupervised Visual Representation Learning  </p><p>A Simple Framework for Contrastive Learning of Visual Representations</p><p>A SURVEY ON CONTRASTIVE SELF-SUPERVISED LEARNING</p><p>Unsupervised Reference-Free Summary Quality Evaluation via Contrastive Learning</p><p>Contrastive Learning with Adversarial Examples. NeurIPS 2020</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 对比学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Helm</title>
      <link href="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/"/>
      <url>/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-k8s编排工具helm">1 K8S编排工具——Helm</span></h2><p>Helm 是 Kubernetes 的软件包管理工具，类似于Python的pip centos的yum,主要用来管理 Charts</p><p>Helm Chart是用来封装Kubernetes原生应用程序的一系列YAML文件。可以在你部署应用的时候自定义应用程序的一些Metadata，以便于应用程序的分发。</p><p>对于应用发布者而言，可以通过Helm打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。</p><p>以便于应用程序的分发。对于应用发布者而言，可以通过Helm打包应用、管理应用依赖关系、管理应用版本并发布应用到软件仓库。</p><p>Helm2 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。</p><p>Helm3 移除 Tiller（Helm 2 是一种 Client-Server 结构，客户端称为 Helm，服务器称为 Tiller）。</p><p>Helm 3 只有客户端结构，客户端仍称为 Helm。客户端直接与 Kubernetes API 服务器交互,去除Tiller后关系图如下:</p><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/18259896-4fdc884fbcc3765a.png" alt="img" style="zoom:67%;"></p><h3><span id="11-helm解决的问题">1.1 Helm解决的问题</span></h3><p>在 Kubernetes中部署一个可以使用的应用，需要涉及到很多的 Kubernetes 资源的共同协作。</p><p>Helm是一个用于kubernetes的包管理器。每个包称为一个Chart，一个Chart是一个目录（一般情况下会将目录进行打包压缩，形成name-version.tgz格式的单一文件，方便传输和存储）。</p><p>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</p><p>对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。</p><p>除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。</p><h3><span id="12-helm相关组件">1.2 Helm相关组件</span></h3><p>Helm三个主要部件：Chart、Repoistory、Release</p><ul><li>Chart：为Kubernetes中应用程序所需要的资源的定义。所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源</li><li>Repoistory：Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包</li><li>Release： Kubernetes中运行的chart实例，每个chart可多次安装，每次安装都是一个新版本；使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release</li></ul><h2><span id="2-helm-安装部署">2 Helm 安装部署</span></h2><h3><span id="21-安装">2.1 安装</span></h3><p>​    两种安装方式，一种是手动安装，一种是一键安装脚本（官方公布脚本链接失效了）</p><ol><li>Download desired version](<a href="https://github.com/helm/helm/releases" target="_blank" rel="noopener">https://github.com/helm/helm/releases</a>)</li><li>Unpack it (<code>tar -zxvf helm-v3.0.0-linux-amd64.tar.gz</code>)</li><li>Find the <code>helm</code> binary in the unpacked directory, and move it to its desired destination (<code>mv linux-amd64/helm /usr/local/bin/helm</code>)</li></ol><p>安装完成后：</p><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/image-20201207135255978.png" alt="image-20201207135255978"></p><h3><span id="22-部署deployment">2.2 部署deployment</span></h3><p>先添加常用的chart源</p><pre class=" language-lang-csharp"><code class="language-lang-csharp">helm repo add stable https://kubernetes-charts.storage.googleapis.comhelm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com  helm repo add bitnami https://charts.bitnami.com/bitnami   目前只有这2个用上了helm repo add aliyuncs https://apphub.aliyuncs.com     目前只有这2个用上了</code></pre><p>使用nginx作为例子，查看有哪些版本提供</p><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/image-20201207135933244.png" alt="image-20201207135933244"></p><p>选择aliyuncs/nginx 的chart包 下载chart包</p><pre><code>helm pull aliyuncs/nginx --untar #将nginx包从创库拉到当前目录</code></pre><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/image-20201207140246880.png" alt="image-20201207140246880"></p><p>安装nginx到我们的k8s集群中：</p><pre><code>helm install my-nginx aliyuncs/nginx --set service.type=NodePort --set persistence.enabled=false# 将tomcat的service对外暴露端口的方式改为NodePort# 不启用持久化存储卷</code></pre><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/image-20201207140436701.png" alt="image-20201207140436701"></p><p>查看是否安装成功</p><pre><code>kubectl get all</code></pre><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/image-20201207140546925.png" alt="image-20201207140546925"></p><p>部署完毕，节点IP:端口查看： 192.168.10.55:30804</p><p><img src="/2020/12/07/gong-cheng-bu-shu-xiang-guan/helm/image-20201207140755910.png" alt="image-20201207140755910"></p><h3><span id="23-发布-helm-chart">2.3 发布 Helm Chart</span></h3><p>使用heml创建chart</p><p>使用 helm package 命令对Chart 文件夹进行打包归档</p><p>创建 INDEX 文件</p><p>推送到harbor中</p><p>helm repo list</p><h2><span id="目前的问题">目前的问题</span></h2><p>当前虚拟机环境可以create &amp; push chart，也可以pull chart。</p><p>但是服务器离线环境无法联阿里源，当前实验室的harbor还没测试是否支持chart，待测试。</p><h2><span id="reference">Reference</span></h2><p><a href="https://helm.sh/docs/intro/install/" target="_blank" rel="noopener">https://helm.sh/docs/intro/install/</a></p><p><a href="https://www.jianshu.com/p/b6ebf8fc72f7" target="_blank" rel="noopener">https://www.jianshu.com/p/b6ebf8fc72f7</a></p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Helm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>服务编排-微服务与K8S</title>
      <link href="/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/"/>
      <url>/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-服务编排">1 服务编排</span></h2><h3><span id="11-服务编排">1.1 服务编排</span></h3><p>在微服务体系结构中，可以将应用分解为多个更小颗粒度的服务, 各个服务可以由不同的团队并行独立开发、部署。</p><p>当一个系统采用了微服务架构后，原有的业务可能并没有发生变化，但系统已被拆分成了很多新的微服务，与传统架构相比，微服务架构下会更依赖通过<strong>各微服务之间的协作</strong>来实现一个完整的业务流程，这种<strong>协作</strong>就是<strong>服务编排</strong>。</p><p>具体来说，<strong>服务编排</strong>指的是可以通过一个请求来依次调用多个微服务，并对每个服务的返回结果做数据处理，最终整合成一个大的结果返回给前端。</p><h3><span id="12-服务编排与容器编排">1.2 服务编排与容器编排</span></h3><p>服务编排大多指的是不通过编程，在容器云平台通过配置、映射等方法来实现服务间的调用、组合，部署成为一个新的服务或应用的过程。</p><p>容器编排是根据规则对容器进行调度、配置、组合、部署、回收、迁移等，以提供应用部署、维护、拓展机制等功能。容器编排主要是K8S等容器编排调度框架要考虑的问题。</p><p><strong>目前大多是通过容器编排的方式来实现服务编排。</strong></p><h2><span id="2-微服务的编排方式">2 微服务的编排方式</span></h2><h3><span id="21-orchestration-编制">2.1 Orchestration 编制</span></h3><p><strong>面向可执行的流程</strong>：通过一个可执行的流程来协同内部及外部的服务交互，通过流程来控制总体的目标、涉及的操作、服务调用顺序。</p><p>有一个<strong>流程控制服务</strong>，该服务接收请求，依照业务逻辑规则，依次调用各个微服务，并最终完成处理逻辑。</p><p><strong>优点</strong>：<br> 流程控制服务时时刻刻都知道每一笔业务究竟进行到了什么地步，监控业务成了相对简单的事情。<br> <strong>缺点</strong>：<br> 1）流程控制服务很容易控制了太多的业务逻辑，耦合度过高，变得臃肿。<br> 2）各个微服务退化为单纯的增删改查，失去自身价值。</p><p><img src="/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/9812895-59869d9042a34b9a.jpeg" alt="img" style="zoom: 67%;"></p><h3><span id="22-choreography-编排">2.2 Choreography 编排</span></h3><p><strong>面向合作</strong>：通过消息的交互序列来控制各个部分资源的交互，参与交互的资源都是对等的，没有集中的控制</p><p>Choreography可以看作一种<strong>消息驱动模式</strong>，或者说是订阅发布模式，每笔业务到来后，各个监听该事件的服务，会主动获取消息，处理，并可以按需发布自己的消息。可以把不同队列看作不同种类的消息，微服务看作消息处理函数。</p><p>Choreography实现方案多是<strong>异步</strong>的。</p><p>优点： 自由，灵活，耦合度低，每个服务都可以各司其职<br>缺点：</p><ul><li>难于调试。业务流程是通过订阅的方式来体现的，很难直接监控每笔业务的处理，因此难于调试。</li><li>无法保证流程正确性。由于没有预定义流程，所以很难在事前保证流程正确性，基本靠事后分析数据来判断。</li><li>维护困难。当一个业务流程会嵌入到多个服务中时，维护会很困难。</li></ul><p>左图编制、右图编排：</p><p><img src="/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/orchestration-vs-choreography-097566bf059109c51c8a95faaf3ea77092a626c2a63bc5f06ae0a7ade4a31378.png" alt="Benefits of Microservices - Choreography over Orchestration, Low Coupling  and High Cohesion" style="zoom:50%;"></p><p>两个单独的例子：</p><p><img src="/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/orchestration-vs-choreography-examples-88fe81d21b600c136f594d43421e4f9576552116c178e4fb7e7cf2b8fc5c065f.png" alt="Benefits of Microservices - Choreography over Orchestration, Low Coupling  and High Cohesion"></p><h2><span id="3-k8s-服务编排">3 K8S 服务编排</span></h2><h3><span id="31-k8s-编排背景">3.1 K8S 编排背景</span></h3><ul><li><p>编排基础<br>Kubernetes虽然提供了多种容器编排对象，例如Deployment, DaemonSet, Job等，还有多种基础资源封装例如ConfigMap、Secret、Serivce等，但是一个应用往往有多个服务，有的可能还要依赖持久化存储，这些服务之间直接互相依赖，需要有一定的组合的情况。</p></li><li><p>现状<br>Kubernetes 已经为我们对大量常用的基础资源进行了抽象和封装，我们可以非常灵活地组合、使用这些资源来解决问题，同时它还提供了一系列自动化运维的机制:如 HPA, VPA, Rollback, Rolling Update 等帮助我们进行弹性伸缩和滚动更新，而且上述所有的功能都可以用 <strong>YAML 声明式</strong>进行部署。</p></li><li><p>编排工具使用背景<br>但是这些还是在容器层面的，对于一个大型的应用而言，需要组合大量的 Kubernetes 原生资源，需要非常多的 Services, Deployments, StatefulSets 等，这里面用起来就会比较繁琐，而且其中服务之间的依赖关系需要用户自己解决，缺乏统一的依赖管理机制，所以就可能需要服务编排工具。</p></li></ul><p>服务编排管理工具就是构建在kubernetes的基础<a href="https://jimmysong.io/kubernetes-handbook/concepts/objects.html" target="_blank" rel="noopener">object</a>之上，统筹各个服务之间的关系和依赖的。目前常用到的工具是 <a href="https://github.com/helm/helm" target="_blank" rel="noopener">Helm</a>。</p><h3><span id="32-k8s编排基础">3.2 K8S编排基础</span></h3><p>在 Kubernetes 中有 5 种我们经常会用到的控制器来帮助我们进行容器编排，它们分别是 Deployment, StatefulSet, DaemonSet, CronJob, Job。</p><p>在这 5 种常见资源中，Deployment 经常被作为无状态实例控制器使用; StatefulSet 是一个有状态实例控制器; DaemonSet 可以指定在选定的 Node 上跑，每个 Node 上会跑一个副本，它有一个特点是它的 Pod 的调度不经过调度器，在 Pod 创建的时候就直接绑定 NodeName；最后一个是定时任务，它是一个上级控制器，和 Deployment 有些类似，当一个定时任务触发的时候，它会去创建一个 Job ，具体的任务实际上是由 Job 来负责执行的。</p><p>他们之间的关系如下图：</p><ul><li>无状态应用 = Services + Volumes + Deployment</li><li>有状态应用 = Services + Volumes + StatefulSet</li><li>守护型应用 = Services + Volumes + DaemonSet</li><li>批处理应用 = Services + Volumes + CronJob/Job</li></ul><p><img src="/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/v2-30eeb011a6936a380f0266960db887d5_1440w.jpg" alt="img" style="zoom:50%;"></p><h3><span id="33-yaml编排方式-自动化运维">3.3 YAML编排方式 - 自动化运维</span></h3><p>YAML 是一种简洁的非标记语言。</p><p>在K8S部署一个应用的YAML内容大致分为两部分：</p><p>控制器定义：定义控制器属性</p><p>被控制对象：Pod模板，定义容器属性</p><p><img src="/2020/11/18/gong-cheng-bu-shu-xiang-guan/fu-wu-bian-pai-wei-fu-wu-yu-k8s/1370684-20200722204314304-1610701586.png" alt="img" style="zoom: 33%;"></p><p>一般情况下，沿用模板进行部署和编排（扩容等操作）即可，具有多种命令可以编写yaml文件。</p><h3><span id="34-编排工具helm">3.4 编排工具——Helm</span></h3><p>Helm 是 Kubernetes 的软件包管理工具。</p><p>Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。</p><h4><span id="341-helm解决的问题">3.4.1 Helm解决的问题</span></h4><p>在 Kubernetes中部署一个可以使用的应用，需要涉及到很多的 Kubernetes 资源的共同协作。</p><p>Helm是一个用于kubernetes的包管理器。每个包称为一个Chart，一个Chart是一个目录（一般情况下会将目录进行打包压缩，形成name-version.tgz格式的单一文件，方便传输和存储）。</p><p>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</p><p>对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。</p><p>除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。</p><h4><span id="342-helm相关组件">3.4.2 Helm相关组件</span></h4><p>Helm三个主要部件：Chart、Repoistory、Release</p><ul><li>Chart：为Kubernetes中应用程序所需要的资源的定义。所谓chart就是一系列文件, 它描述了一组相关的 k8s 集群资源</li><li>Repoistory：Helm chart 的仓库，Helm 客户端通过 HTTP 协议来访问存储库中 chart 的索引文件和压缩包</li><li>Release： Kubernetes中运行的chart实例，每个chart可多次安装，每次安装都是一个新版本；使用 helm install 命令在 Kubernetes 集群中部署的 Chart 称为 Release</li></ul><p>Helm有两部分组成：Helm客户端、Tiller服务端</p><ul><li><p>Helm是一个命令行下的客户端工具。主要用于Kubernetes应用程序Chart的创建、打包、发布以及创建和管理本地和远程的Chart仓库。</p></li><li><p>Tiller是Helm的服务端，部署在Kubernetes集群中，Tiller用于接受Helm的请求，并根据Chart生成Kubernetes的部署文件，然后提交给kubernetes创建应用。Tiller还提供了Release的升级、删除、回滚等一系列功能。</p></li></ul><h2><span id="reference">Reference</span></h2><p><a href="https://www.jianshu.com/p/54e2e223dbac" target="_blank" rel="noopener">https://www.jianshu.com/p/54e2e223dbac</a></p><p><a href="https://baijiahao.baidu.com/s?id=1647990186917649762&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1647990186917649762&amp;wfr=spider&amp;for=pc</a></p><p><a href="https://cloud.tencent.com/developer/article/1080207" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1080207</a></p><p><a href="https://zhuanlan.zhihu.com/p/36062500" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36062500</a></p><p><a href="https://www.jianshu.com/p/4bd853a8068b" target="_blank" rel="noopener">https://www.jianshu.com/p/4bd853a8068b</a></p><p><a href="https://blog.csdn.net/weixin_45186298/article/details/104594539" target="_blank" rel="noopener">https://blog.csdn.net/weixin_45186298/article/details/104594539</a></p><p><a href="http://www.docin.com/p-2133339881.html" target="_blank" rel="noopener">http://www.docin.com/p-2133339881.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 服务编排-微服务与K8S </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Faiss</title>
      <link href="/2020/11/18/yu-yan-gong-ju-ji-zhu-deng-wen-dang/faiss/"/>
      <url>/2020/11/18/yu-yan-gong-ju-ji-zhu-deng-wen-dang/faiss/</url>
      
        <content type="html"><![CDATA[<p>需求是，基于句子级别的向量，从海量候选项中挑选最合适的候选。</p><p>一般情况下，是求出query的向量，以及所有候选的向量，计算他们之间的相似度，得到top1。</p><p>奈何太慢~</p><h2><span id="1-faiss">1. Faiss</span></h2><p>Faiss是一个高效的相似性搜索和稠密向量聚类库。</p><p>它包含的算法可以在任意大小的向量集中搜索，只要内存够大向量集就可以多大。</p><p>Faiss用C++编写，带有Python／Numpy的完整包装。一些最有用的算法是在GPU上实现的。它是由Facebook人工智能研究所开发的。</p><p>包括几种相似性搜索方法，欧几里得距离或点积、余弦相似度等来比较。</p><p><img src="/2020/11/18/yu-yan-gong-ju-ji-zhu-deng-wen-dang/faiss/1408825-20190320225405798-259149897.png" alt="img"></p><p>暂时不适合当前使用场景，之后用到再看</p>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Faiss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>灾难性遗忘问题</title>
      <link href="/2020/11/03/nlp-jin-jie/zai-nan-xing-yi-wang-wen-ti/"/>
      <url>/2020/11/03/nlp-jin-jie/zai-nan-xing-yi-wang-wen-ti/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-灾难性遗忘">1 灾难性遗忘</span></h2><h3><span id="11-简介">1.1 简介</span></h3><p>连续学习：根据任务A训练模型后，再根据任务B训练模型，此时对任务A进行测试，还可以维持其重要内容</p><p>人工神经网络的连续学习出现问题：由于当前的神经网络对于任务A的参数与任务B的参数基本无关，使得当任务B训练完成后，该网络无法给出任务A的结果</p><p>灾难消失：在网络顺序训练多重任务时，对先前任务的重要权重无法保留，称之为灾难性消失。</p><h3><span id="12-遗忘的体现">1.2 遗忘的体现</span></h3><p>各种论文是如何体现灾难性遗忘的？</p><ul><li>顺序学习若干任务、若干Atari2600游戏。之前训练的任务效果变化 [3]</li><li>原预训练语言模型与当前fine-tune模型的参数距离 [2]</li></ul><ul><li><strong>[11] Learning and Evaluating General Linguistic Intelligence 2019 arxiv</strong></li></ul><p>Catastrophic forgetting in a continual learning setup on unsupervised −→ SQuAD −→ MNLI (top) and unsupervised −→ SQuAD −→ TriviaQA (bottom).</p><ul><li><strong>[12] Does an LSTM forget more than a CNN? An empirical study of catastrophic forgetting in NLP 2019 ALTA</strong></li></ul><p>旨在了解导致连续学习中遗忘的因素。 主要发现是CNN的遗忘少于LSTM。 实验显示，最大池化是底层的操作，与LSTM相比，它有助于CNN减轻遗忘。 还发现，将艰巨的任务放在任务序列的末尾，可以减少遗忘。 </p><p>本文还分析了微调上下文嵌入对灾难性遗忘的影响，发现在持续学习设置中，使用嵌入作为特征提取器优于微调。</p><h3><span id="13-连续学习方法分类">1.3 连续学习方法分类</span></h3><ul><li><strong>Replay-based or Rehearsal method</strong>   示例重播方法   存储过去的样本，并定期重播它们。在模型学习新任务的同时混合原来任务的数据。</li><li><strong>Parameter isolation-based methods</strong>   通过为每个任务更新一组参数并将其冻结为新任务来避免忘记</li><li><strong>Ensembling:</strong> 当模型学习新任务的时候，<strong>增加新的模型</strong>（可以是显式或者隐式的方式），使得多个任务实质还是对应多个模型，最后把多个模型的预测进行整合。增加子模型的方式固然好，但是每多一个新任务就多一个子模型，对学习效率和存储都是一个很大的挑战。</li><li><strong>Regularization-based methods</strong>  建议使用额外的正则化术语来回忆以前的知识。在网络参数更新的时候<strong>增加限制</strong>，使得网络在学习新任务的时候不影响之前的知识。<ul><li>data-focused 以数据为中心的方法通过从预先训练的模型中进行知识提炼来规范新任务学习</li><li>prior-focused 在学习新任务时，以先验为重点的方法将预训练参数的分布视为先验<ul><li>优先先验的方法，能使模型能够更有效地从预先训练的模型参数中学习更多的常识。</li></ul></li></ul></li></ul><ul><li>NLP预训练模型中的知识遗忘问题主要集中在微调上<ul><li>fine-tuning tricks [2] [4] [5] [7] </li></ul></li></ul><h2><span id="2-具体工作-nn">2 具体工作 - NN</span></h2><h3><span id="21-针对nn">2.1 针对NN</span></h3><ul><li><strong>[3] Overcoming catastrophic forgetting in neural networks    2017 deepmind PNAS</strong>  </li></ul><p>连续学习导致了灾难性遗忘。</p><p>有可能克服这种局限性并训练网络，这些网络可以维持他们长期未经历的任务的专业知识。 </p><p>EWC 通过选择性地减慢对那些任务重要的权重的学习来记住旧任务。 </p><p>在网络参数更新的时候增加限制，使得网络在学习新任务的时候不影响之前的知识。</p><p>下图非立体图：</p><p><img src="/2020/11/03/nlp-jin-jie/zai-nan-xing-yi-wang-wen-ti/image-20201103203946336.png" alt="image-20201103203946336" style="zoom:50%;"></p><p>先训练了一个task1的网络，然后直接在task2上fine-tune（上图蓝色）。</p><p>问题是，学到task2之后，就会基本忘记task1了（catastrophic forgetting）</p><p>那设个限制，在task2上更新网络的时候，不能离第一个网络的参数太远，也就是相对于之前的网络参数上加了l2 regularizaiton（上图绿色）</p><p>这样还不行。考虑到网络中有些参数没用有些参数有用，让重要的参数别变太多，就更新不重要的参数（上图红色）</p><ul><li>[6] A Multi-Task Learning Framework for Overcoming the Catastrophic Forgetting in Automatic Speech Recognition  2019 arxiv </li></ul><p>现有自动语音识别（ASR）系统适合目标领域时，例如微调，再训练，经常使用转移学习。 </p><p>在这些过程中，系统参数可能与先前学习的参数有很大的偏差。 因此，系统训练过程很难从目标领域学习知识，同时又不忘记先前的学习过程中的知识，这被称为灾难性遗忘（CF）。 </p><p>本文提出一种新颖的ASR多任务学习训练框架。认为保留原始知识和学习新知识分别是两个独立的任务。 一方面，我们限制新参数不要偏离原始参数太远，并在忘记原始知识时惩罚新系统。 另一方面，我们强迫新系统快速解决新知识。 然后，采用MTL机制来获得两个任务之间的平衡。 </p><ul><li><strong>[13] An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models 2019 NAACL  -pc</strong></li></ul><p>提出了一种概念上简单有效的转移学习方法，用于解决灾难性遗忘问题，文本分类任务。 </p><p>具体来说，我们将特定于任务的优化函数与辅助语言模型目标结合在一起，该目标可以在培训过程中进行调整。 这样可以保留语言模型捕获的语言规则，同时可以进行充分的调整以解决目标任务。 </p><p>我们的方法不需要预先训练或微调网络的各个组成部分，而我们只需一步就可以对模型进行端到端训练。</p><ul><li><strong>[10] Forget Me Not: Reducing Catastrophic Forgetting for Domain Adaptation in Reading Comprehension 2020 IJCNN</strong></li></ul><p>机器阅读理解任务的灾难性遗忘</p><p>不从源域访问数据，引入辅助惩罚项，规范化微调过程</p><h3><span id="22-预训练模型-fine-tune">2.2 预训练模型 Fine-tune</span></h3><ul><li><strong>[4] Universal Language Model Fine-tuning for Text Classification 2018 ACL</strong></li></ul><p>一种微调方法，文本分类任务，引入判别性微调倾斜的三角形学习率和针对LM的微调逐渐解冻</p><ul><li><strong>[5] Mixout: Effective Regularization to Finetune Large-scale Pretrained Language Models 2020 ICLR</strong></li></ul><p>一种微调方法，提出dropout的方式将预训练的参数随机混合到下游模型中来减少BERT微调中的遗忘。</p><p>只有少量训练实例可用来fine-tune时，会降低性能。</p><p>本文介绍了一种新的正则化技术mixout。</p><ul><li>[7] Side-tuning: Network adaptation via additive side networks  2019 arxiv</li></ul><p>对于fine-tune，本文提出了一种简单的替代方法：side-tuning<br>side-tuning 通过训练轻型side网络来适应预训练的网络，该网络通过求和与<strong>（未更改的）</strong>预训练网络融合。</p><p>这种简单的方法与现有解决方案一样好，甚至更好，它解决了一些 微调，固定功能和其他常见方法的基本问题，特别是，侧调不太容易过度拟合，渐近一致，并且不会在增量学习中遭受灾难性的遗忘。</p><ul><li><strong>[2] Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting EMNLP 2020 <a href="https://arxiv.org/pdf/2004.12651.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2004.12651.pdf</a></strong></li></ul><p>动机：预训练+微调这样的迁移学习会遇到灾难性的遗忘问题。</p><p>主要工作：提出一种 recall-and-learn 机制，共同学习预训练任务和下游任务。</p><p>主要贡献：采用多任务学习的思想解决微调深度预训练语言模型的灾难性遗忘问题；提出预训练仿真和目标转移机制，在没有预训练任务数据的情况下实现多任务微调；提供了开源的优化器</p><pre><code>灾难性遗忘问题是sequential trasfer learning的一个普遍问题，其体现在遗忘之前的知识而在目标领域上过度拟合。目前已有的工作主要集中在fine-tune的trick上，[23] 引入判别性微调倾斜的三角形学习率和针对LM的微调逐渐解冻，[36] 提出dropout的方式将预训练的参数随机混合到下游模型中来减少BERT微调中的遗忘；不同于顺序的进行预训练和下游任务的训练，多任务学习同时进行两者。但是这些多任务学习的工作 1要使用大规模预训练数据 2我们只下游任务的性能。因此本文提出一种Recall—and-learn机制，在fine-tune过程中采用预训练作为辅助学习任务。本文贡献的两方面：Pretraining Simulation解决大规模预训练数据不可获取的问题；Objective Shifting将多任务学习的目标逐渐转移到下游任务目标（本质上就是动态的调整loss比例）</code></pre><p>实验：General Language Understanding Evaluation (GLUE) benchmark （9个任务）</p><p>思考：如果只在乎下游任务，灾难性的遗忘问题好像不太重要？（再调研一下灾难性遗忘的体现与分析）</p><h3><span id="23-其他">2.3 其他</span></h3><p>知识注入PLM的知识遗忘问题</p><ul><li><strong>[1] K-Adapter: Infusing Knowledge into Pre-Trained Models with Adapters</strong></li></ul><p>动机：将知识注入大型预训练模型，现有方法会更新原始参数，遭受灾难性的遗忘。</p><p>主要工作：提出K-Adapter，保持原始预训练模型参数固定，并支持知识注入。</p><p><img src="/2020/11/03/nlp-jin-jie/zai-nan-xing-yi-wang-wen-ti/image-20201028102828012.png" alt="image-20201028102828012"></p><p>之前的工作都更新了预训练模型的参数，导致灾难性遗忘之前注入的知识。</p><p>通过集成紧凑的神经模型（适配器）来实现的</p><p><img src="/2020/11/03/nlp-jin-jie/zai-nan-xing-yi-wang-wen-ti/image-20201028103214978.png" alt="image-20201028103214978"></p><p>比较大的问题：只有实验结果ok，没有分析遗忘程度的减少</p><h2><span id="idea">IDEA?</span></h2><p><strong>一般的灾难性遗忘：</strong></p><ul><li>在任务A上train一个model_1，任务A效果ok；</li><li>然后将model_1在任务B上继续train得到model_2，任务B效果ok；</li></ul><p>然而这个时候model_2在任务A上的效果比model_1差</p><p><strong>预训练模型的灾难性遗忘场景：</strong></p><pre><code>* pretrain 得到PLM* fine-tune 得到 模型fine-tune1   fine-tune过程中，造成了预训练过程学到的内容 灾难性的遗忘，导致目标任务效果没有那么理想但目前没有找到有分析遗忘具体体现在哪的工作，只有通过fine-tune的效果提升来体现</code></pre><pre><code>* pretrain 得到PLM* 任务1上fine-tune 得到 模型fine-tune1* fine-tune1 在任务2 上fine-tune 得到 模型fine-tune2</code></pre><pre><code>model_1(PLM)进行fine-tune，model_4和model_3的效果哪个好？（真实场景？）- 小批量数据A 对PLM进行fine-tune  得到model_2- 又来了一批数据B 使用B对model_2进行fine-tune  得到model_3（真实场景2，但是ab数据集大）- 小批量数据A 对PLM进行fine-tune  得到model_2- 又来了一批数据B 使用A+B对model_2进行fine-tune  得到model_4（不真实的场景）-- 所有数据A+B一口气全到位 对PLM进行fine-tune  得到model_5</code></pre><p>基于 预训练模型的灾难性遗忘问题挖掘了几个研究点：</p><p>针对预训练+fine-tune的过程（遗忘预训练）</p><ol><li>预训练模型fine tune时对之前pretrain内容的遗忘 如何体现？</li><li>预训练模型fine tune后对之前pretrain内容的遗忘问题如何解决？</li></ol><p>针对预训练+多次fine-tune的过程（遗忘预训练+之前的fine-tune）</p><ol><li>预训练模型多次fine-tune过程的遗忘分析（更多的是因为pretrain过程的进一步的遗忘 还是上一fine-tune阶段的遗忘）</li><li>预训练模型多次fine tune时，对之前finetune内容的遗忘问题。（预训练模型迁移学习下的增量学习）</li></ol><p>问题比较大，暂时很难找到一个切入点，暂时搁置一下，目前先做面向匹配任务的预训练模型。</p><h2><span id="reference">Reference</span></h2><p><a href="https://blog.csdn.net/u010195841/article/details/69257897" target="_blank" rel="noopener">https://blog.csdn.net/u010195841/article/details/69257897</a></p><p><a href="https://www.cnblogs.com/chason95/articles/9892560.html" target="_blank" rel="noopener">https://www.cnblogs.com/chason95/articles/9892560.html</a></p><p><a href="https://blog.csdn.net/zyy617532750/article/details/104217399" target="_blank" rel="noopener">https://blog.csdn.net/zyy617532750/article/details/104217399</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 灾难性遗忘问题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度文本匹配survey2020</title>
      <link href="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/"/>
      <url>/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-深度文本匹配">1 深度文本匹配</span></h2><h3><span id="11-方法概述">1.1 方法概述</span></h3><p><strong>传统文本匹配方法</strong></p><p>传统的文本匹配技术有BoW、VSM、TF-IDF、 BM25、Jaccord、SimHash等算法，如BM25算法通过网络字段对查询字段的覆盖程度来计算两者间的匹配得分，得分越高的网页与查询的匹配度更好。主要解决词汇层面的匹配问题，或者说词汇层面的相似度问题。</p><p><strong>深度语义匹配模型</strong></p><ul><li>representation-based method 表示型方法</li><li>interaction-based method  交互型方法</li></ul><p><strong>representation-based method</strong></p><ul><li>将待匹配的两个对象通过深度学习模型进行表示</li><li>计算这两个表示之间的相似度便可输出两个对象的匹配度</li></ul><p>侧重表示层的构建</p><p>匹配度函数：相似度度量函数 or  可学习的匹配度打分模型</p><p><strong>interaction-based method</strong></p><ul><li>首先基于表示层采用与词位置对应的词向量</li><li>然后对两个句子按词对应交互，由此构建两段文本之间的 matching pattern，这里面包括了更细致更局部的文本交互信息</li><li>基于该匹配矩阵，可以进一步使用DNN等来提取更高层次的匹配特征，最后计算得到最终匹配得分。</li></ul><p>该方式更强调待匹配的两个句子得到更充分的交互，以及交互后的匹配。</p><p>Interaction-based 方法匹配建模更加细致、充分，一般来说效果更好，但计算成本增加，更加适合一些效果精度要求高但对计算性能要求不高的场景。</p><h3><span id="12-深度文本匹配的一些应用">1.2 深度文本匹配的一些应用</span></h3><p><strong>QA系统的分类</strong></p><ul><li><p>知识领域分类</p><ul><li>面向限定领域的问答系统</li><li>面向开放领域的问答系统</li><li>面向常用问题集的问答系统（ Frequently Asked Questions, FAQ ）</li></ul></li><li><p>答案来源</p><ul><li>基于结构化数据，例如KBQA（Knowledge Base Question Answering）</li><li>基于自由文本，如机器阅读理解</li><li>基于问答对的问答系统，如FAQ问答</li></ul></li><li><p>答案反馈机制</p><ul><li>基于检索式的问答系统</li><li>基于生成式的问答系统</li></ul></li></ul><p><strong>检索任务的分类</strong></p><ul><li>ad hoc 检索（关键词检索）</li><li>CQA（社区问答 )</li><li>response retrieval（依赖上下文语义）</li></ul><p>例子</p><ul><li>response retrieval      Q：你吃了吗   A：没有</li><li>CQA       Q：心情不好的时候吃什么水果？   A：很多水果都有缓解抑郁的功能，比如。。。</li><li>response retrieval更依赖语义上的联系，上下句之间可能连overlap的词都没有<br>CQA的话相比ad hoc retrieval，query里面的语义信息更丰富，查询的目的也可能更模糊一些</li></ul><p><strong>FAQ</strong></p><p>FAQ：Frequently Asked Questions</p><p>FAQ问答系统看做一个特殊的信息检索系统，问题为查询语句（query），根据query去FAQ知识库中找到最合适的答案</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/640" alt="img"></p><p>具体流程：</p><ol><li>线上收到用户 query 后，初步召回一批候选集作为粗排结果传入下一模块进行进一步精确排序；</li><li>利用matching模型计算用户query和FAQ知识库中问题或答案的匹配程度；</li><li>利用ranking 模型对候选集做 rerank 并返回 topk个候选答案。</li></ol><p>可以看出，FAQ问答系统的核心任务可以抽象为文本匹配任务。</p><p><strong>基本策略</strong></p><p>FAQ有两种解决思路</p><ul><li>相似问题匹配：对比用户问题与现有FAQ知识库中问题的相似度，返回用户问题对应的最准确的答案</li><li>问题答案对匹配：对比用户问题与FAQ知识库中答案的匹配度，返回用户问题对应的最准确的答案</li></ul><h2><span id="2-文本匹配任务与数据集">2 文本匹配任务与数据集</span></h2><ul><li><p>中文数据集</p><ul><li>语义文本相似度<ul><li>LCQMC  - <a href="http://icrc.hitsz.edu.cn/info/1037/1146.htm" target="_blank" rel="noopener">http://icrc.hitsz.edu.cn/info/1037/1146.htm</a></li><li>AFQMC（无测试集）: <a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li><li>BQ Corpus  - 银行领域  - <a href="http://icrc.hitsz.edu.cn/Article/show/175.html" target="_blank" rel="noopener">http://icrc.hitsz.edu.cn/Article/show/175.html</a></li></ul></li><li>文本蕴含<ul><li>SNLI：</li><li>XNLI： <a href="https://github.com/facebookresearch/XNLI" target="_blank" rel="noopener">https://github.com/facebookresearch/XNLI</a></li><li>OCNLI: <a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li><li>CMNLI: <a href="https://github.com/CLUEbenchmark/CLUE" target="_blank" rel="noopener">https://github.com/CLUEbenchmark/CLUE</a></li></ul></li></ul></li></ul><ul><li>英文数据集 (<a href="https://github.com/nyu-mll/GLUE-baselines/edit/master/download_glue_data.py" target="_blank" rel="noopener">https://github.com/nyu-mll/GLUE-baselines/edit/master/download_glue_data.py</a>)<ul><li>语义文本相似度<ul><li>QQP-Quora（glue没有测试集，有一篇工作《Bilateral Multi-Perspective Matching for Natural Language Sentences》进行了划分：arxiv.org/pdf/1702.03814.pdf ）（<a href="https://zhiguowang.github.io/#services" target="_blank" rel="noopener">https://zhiguowang.github.io/#services</a> 数据集链接）</li><li>The Microsoft Research Paraphrase dataset (MSRP) （微软官方可下载带标签的训练集和测试集，glue将训练集分割划分一部分为验证集但是其测试集没有标签）（使用glue的训练集和验证集，使用官方的测试集）</li><li>TwitterURL</li></ul></li><li>文本蕴含<ul><li>SciTail   </li><li>XNLI： <a href="https://github.com/facebookresearch/XNLI" target="_blank" rel="noopener">https://github.com/facebookresearch/XNLI</a> （多语言）</li></ul></li></ul></li><li><p>Answer Selection</p><ul><li>TrecQ </li><li>The SemEval CQA dataset </li><li>WikiQA  </li></ul></li></ul><ul><li>检索 - 排序<ul><li>Tweet Search  <ul><li>TREC Microblog  2013–2014  [1]</li></ul></li><li>对话<ul><li>the Neurips ConvAI2 competition  [9]</li><li>DSTC7 challenge, Track 1   [9]</li></ul></li><li>IR<ul><li>Ubuntu V2 corpus   [9]</li><li>Wikipedia Article Search task    [9]</li></ul></li></ul></li></ul><h2><span id="3-现状分析">3 现状分析</span></h2><p>深度文本匹配的调研现状</p><ul><li><p>有关联的任务极其之多，相关方向非常多且混乱</p></li><li><p>不同任务之间可借鉴的工作较多</p></li><li><p>各个方向极度内卷，例如文本匹配中的交互匹配工作非常多</p></li></ul><p>目前将近几年的现有工作划分成以下几类：</p><ol><li>从新的角度来提高文本匹配性能（ACL2019 结合语义匹配和相关性匹配、ACL2020 将子词信息融入匹配框架、SIGIR2020针对FAQ问题将候选答案辅助query和候选问题的匹配…）</li><li>从新的模型角度提高文本匹配模型（搭更好看或者说更复杂的积木）</li><li><strong>基于预训练模型的匹配方法</strong>（tBERT结合主题信息和BERT做语义相似性检测、DC-BERT 和ColBERT 采用双BERT做交互匹配做检索…）</li><li>训练方式优化（针对匹配、检索任务训练方式的局限性，进行训练相关包括方式数据等的调优）（RocketQA）</li><li>结合辅助信息（结合用户反馈提高对话检索、使用知识图谱增强匹配模型…）</li></ol><p>策略：</p><ul><li>1、2 搭老款积木意义不大，3搭新款积木或许也得多考虑老方法</li><li>5 辅助信息严重依赖数据，结合实用场景暂时没有合适的</li></ul><p>更有意义：基于预训练模型的方法来更好的解决问题</p><p>优势：任务明确，实用性强；相关工作都在近几年，好上手；可以顺便接触前沿预训练语言模型</p><h2><span id="4-深度文本匹配论文记录">4 深度文本匹配论文记录</span></h2><h3><span id="41-任务特性探索">4.1 任务特性探索</span></h3><p><span style="color:red">这一类大致是利用匹配任务的性质or特性，或者说从新的角度去提升匹配任务的性能</span></p><ul><li><strong>[1] Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling     ACL 2019 -code py2.7+tf</strong>  </li></ul><p>动机：相关性匹配和语义匹配的gap，两者信息融合提升效果</p><ul><li>IR中的相关性匹配看重关键词的匹配，NLP中的语义匹配看重词汇信息和语句的组成结构</li><li>相关性匹配采用基于交互的设计，从相似性矩阵上进行操作；语义匹配则需要更多的语义理解和上下文推理</li></ul><p>贡献点：</p><ul><li>讨论了相关性匹配和语义匹配的区别（模型之间是否适配，关联性和语义匹配的信号是否互补）</li><li>提出一个新的交互式的模型（一个RM一个SM，和一个结合的）</li><li>3个NLP任务和2个IR数据集上进行实验；关联性和语义匹配信号在许多问题上互补</li></ul><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027103133258.png" alt="image-20201027103133258"></p><ul><li><strong>[2] FAQ Retrieval Using Attentive Matching 2019 SIGIR短文 </strong></li></ul><p>解决的问题：FAQ</p><p>贡献点：提出了针对FAQ检索设计的多种结构，将query-question和query-answer结合；证明了注意力机制在汇总两者时明显优于其他汇总方法</p><ul><li><strong>[3] Match²: A Matching over Matching Model for Similar Question Identification SIGIR 2020</strong></li></ul><p>CQA问题，利用答案的方式</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201025182145072.png" alt="image-20201025182145072"></p><p>提出了一种相似问题的二次匹配模型，将 archived question 的回答作为连接二者的桥梁，辅助判定 archived question 是否与 user question 相似</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027110335904.png" alt="image-20201027110335904"></p><ul><li><strong>[4] Neural Graph Matching Networks for Chinese Short Text Matching ACL 2020</strong> </li></ul><p>中文短文本匹配问题</p><p>动机：通常使用分词后的序列，分词造成误差</p><p>方法：神经图匹配网络，处理多粒度输入信息的匹配框架。</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201026154234577.png" alt="image-20201026154234577" style="zoom: 33%;"></p><ul><li><strong>[5] FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance SIGIR 2019 短文</strong></li></ul><ul><li><strong>[6] Unsupervised FAQ Retrieval with Question Generation and BERT ACL 2020</strong> </li></ul><p>FAQ检索任务 无监督的方法</p><p>query和QA对的监督信号基本不可得</p><p>此篇文章 使用远程监督，使用BM25对初始池排序，利用FAQ对训练两个BERT，分别将query和Q和A进行匹配。</p><p>自动生成问题复述来克服缺数据的问题，然后使用无监督的融合方法结合三种方法的结果。</p><h3><span id="42-深度匹配模型">4.2 深度匹配模型</span></h3><p><span style="color:red">也是从新的角度去提高匹配性能，但是更专注于匹配模型结构</span></p><p><strong>[7] Simple and Effective Text Matching with Richer Alignment Features. ACL 2019 - code-tf+py</strong></p><p>动机：推理速度快，性能ok的文本匹配方法</p><p>贡献点：三个任务中的四个数据集达到与SOTA相当的水平；最少的参数数量和最快的推理速度；消融实验和替代方案对比</p><p>模型组成：一个encoder；一个增强的残差连接；一个对齐层；一个融合层</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027103336691.png" alt="image-20201027103336691" style="zoom: 67%;"></p><p><strong>[8] Leveraging Entanglement Entropy for Deep Understanding of Attention Matrix in Text Matching</strong>      2020ICLR reviewing</p><p>神经网络和量子力学？？？？！</p><p>quantum many-body 系统中的entanglement entropy(纠缠熵？)可以为任务指导网络结构和参数的设计。</p><p>动机：目前还没有在两个对象的匹配（问题-答案 对）中的充分研究；匹配矩阵的指数增长导致的不饿能定性计算纠缠熵</p><p>先放放，先放放</p><p><strong>[9] Poly-encoders: architectures and pre-trainingstrategies for fast and accurate multi-sentence scoring ICLR 2020 </strong></p><p>动机：Cross-encoders方法相比于Bi-encoders效果好但是慢；</p><p>贡献：提出Poly-encoders，详细比较三种方法，包括与预训练和微调策略的比较等；更先进且更快。</p><p>模型核心思想：对每一个query用多个attention模块获取 query 中一词多义或切词带来的不同语义信息</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027104845341.png" alt="image-20201027104845341"></p><ul><li><strong>[10] Rationalizing Text Matching:Learning Sparse Alignments via Optimal Transport ACL 2020 摘要都看不懂系列</strong> </li></ul><p>文本匹配可解释性</p><p>文本匹配选择性合理化，</p><p>基本原理用来进行联合建模和优化</p><p>在所选基本原理之间产生可解释的对齐方式</p><p>任务特性：</p><p>显性的：将两个句子的对齐方式明确为M*N的矩阵</p><p>稀疏的、可信的</p><p>使用optimal transport（OT）的优化目标进行文本匹配任务的词对齐</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201030150913127.png" alt="image-20201030150913127" style="zoom:50%;"></p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201030151106633.png" alt="image-20201030151106633"></p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201030151250912.png" alt="image-20201030151250912"></p><ul><li><strong>[11] Distilling Knowledge for Fast Retrieval-based Chat-bots SIGIR 2020</strong> </li></ul><p>对话检索</p><p>cross-encoders+attention 相比  bi-encoders，前者好但是慢</p><p>提出一种新的cross-encoders结构，将知识从cross-encoders蒸馏到bi-encoders</p><h3><span id="43-基于预训练模型的改进">4.3 基于预训练模型的改进</span></h3><p><span style="color:red">利用预训练模型进行各种任务上的改进，包括效率、性能等</span></p><h4><span id="431-基于预训练模型的改进">4.3.1 基于预训练模型的改进</span></h4><p><strong>[9] Poly-encoders: architectures and pre-trainingstrategies for fast and accurate multi-sentence scoring ICLR 2020 </strong></p><p>动机：Cross-encoders方法相比于Bi-encoders效果好但是慢；</p><p>贡献：提出Poly-encoders，详细比较三种方法，包括与预训练和微调策略的比较等；更先进且更快。</p><p>模型核心思想：对每一个query用多个attention模块获取 query 中一词多义或切词带来的不同语义信息</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027104845341.png" alt="image-20201027104845341"></p><ul><li><strong>[12] Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.  2019 EMNLP - code</strong></li></ul><p>动机：BERT和RoBERTa导致了巨大的计算开销（需要将比较的两个句子都传入模型中计算）</p><p>贡献：提出sentence-BERT，效率++</p><p>核心思想：通过SBERT模型获取到的句子embedding，可以直接通过cos相似度计算两个句子的相似度，大大减少了计算量</p><p>可以重点借鉴的：微调结构、微调任务</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027112101304.png" alt="image-20201027112101304" style="zoom:50%;"></p><p>主要涉及的任务：大规模的语义相似度比较，聚类，通过语义搜索的信息检索。</p><p>出现的问题：大规模检索中，求得一个问题的检索答案需要：N(N-1)/2；聚类任务和语义搜索中，BERT映射成的向量效果一般</p><ul><li><strong>[18] DC-BERT: Decoupling Question and Document for Efficient Contextual Encoding SIGIR2020</strong></li></ul><p>解决问题：将问题和检索到的每个文档进行拼接作为 BERT 的输入，这需要对大量检索文档进行重编码，非常耗时</p><p>核心思想： DC-BERT 提出具有双重 BERT 模型的解耦上下文编码框架：在线的 BERT 只对问题进行一次编码，而离线的 BERT 对所有文档进行预编码并缓存它们的编码。</p><p>效果：DC-BERT 在文档检索上实现了 10 倍的加速，同时与最先进的开放域 QA 方法相比，保留了大部分(约98%)的 QA 问答性能。</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027140930258.png" alt="image-20201027140930258" style="zoom:67%;"></p><ul><li><strong>[19] ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT SIGIR2020</strong></li></ul><p>解决的问题：匹配模型的双塔结构，加query和doc的交互</p><p>核心思想：一种新颖的后期交互范式，兼顾匹配的效率和doc中的上下文信息，基于上下文的后期交互的排序模型。</p><p>贡献：新的后期的交互体系结构，效果好，检索速度快。</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027141518194.png" alt="image-20201027141518194" style="zoom: 67%;"></p><ul><li><strong>[20] Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention ACL 2020 - code model</strong></li></ul><p>动机：中文预训练模型中以字符为基本单位，而忽略了词的语义。</p><p>贡献：提出一种词对齐注意机制，显式利用词的信息</p><p>关键问题：如何将分词信息无缝集成到预训练模型中的基于字符的attention模块；分词工具带来的cascading(级联)噪声</p><p>本质上：字符级别的表示模型，进行了包括6个数据集的5个任务。</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027163729424.png" alt="image-20201027163729424" style="zoom:50%;"></p><ul><li><strong>[15] tBERT: Topic Models and BERT Joining Forcesf or Semantic Similarity Detection ACL 2020</strong> </li></ul><p>解决的任务：语义相似性检测</p><p>核心思想：融合了主题模型和BERT，基于BERT的上下文与主题信息结合</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027140300811.png" alt="image-20201027140300811"></p><ul><li><strong>SKEP: Sentiment Knowledge Enhanced Pre-training for Sentiment Analysis</strong>  <strong>ACL 2020 -code-tf</strong></li></ul><p>情感分析方法中广泛使用了情感知识，但在预训练过程中却忽略了他们。</p><p>借助自动挖掘的知识，构建了三个情感知识预测目标，将单词、极性、方面情感分析嵌入到预训练模型的情感表示中。</p><p>通过无监督点互信息（PMI）的方式标记情感词及情感词的极性，mask掉情感词来进行预测；mask掉属性词-情感词对 进行预测。</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201030160846710.png" alt="image-20201030160846710"></p><p>知识获取：情感词集合；属性词-情感词 对（情感词附近不超过3距离的名词）</p><p>mask内容：mask句子中的属性词-情感词对；mask句子中的情感词；如果前两者mask掉的词不超过10%，再随机mask句子中的词。</p><p>预测内容：Sentiment Word（x9）；Word Polarity（x6 x9）；Aspect-Sentiment pairs（x1 CLS）</p><h4><span id="432-基于预训练模型改进-检索">4.3.2 基于预训练模型改进-检索</span></h4><ul><li><strong>[13] Context-Aware Document Term Weighting for Ad-Hoc Search WWW 2020 </strong></li></ul><p>检索第一阶段的初筛任务。</p><p>词袋文档表示在现代搜索引擎中起着基本作用，但其功能受到基于浅频率的术语加权方案的限制。 </p><p>贡献：文章提出了HDCT，基于上下文的文本词项权重生成方法，对初步检索具有较大意义；文章解决BERT的长文档问题：文档切割成段落+预测之后融合。</p><p>核心思想：通过BERT得到段落级词项权重，然后聚合各段落，使用多个弱监督方法构建标签（文本内容信号、相关性信号和伪反馈信号作为标签）训练模型。</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201027111638137.png" alt="image-20201027111638137"></p><ul><li><strong>[14] Context-Aware Term Weighting For First Stage Passage Retrieval SIGIR 2020短文 </strong> </li></ul><p>检索第一阶段的初筛任务。</p><p>词频(TF)是一种用于识别文档中术语重要性的常用方法。 但是术语频率忽略了术语与其文本上下文的交互方式，这是估计特定于文档的术语权重的关键。 </p><p>本文提出了一个深度上下文术语权重框架（DeepCT），该框架将BERT的上下文术语表示映射到上下文感知术语权重以进行段落检索。 可以将新的深项权重存储在普通的反向索引中，以进行有效检索。</p><ul><li><strong>[17] Unsupervised FAQ Retrieval with Question Generation and BERT ACL 2020</strong> </li></ul><p>FAQ检索任务 无监督的方法</p><p>query和QA对的监督信号基本不可得</p><p>此篇文章 使用远程监督，使用BM25对初始池排序，利用FAQ对训练两个BERT，分别将query和Q和A进行匹配。</p><p>自动生成问题复述来克服缺数据的问题，然后使用无监督的融合方法结合三种方法的结果。</p><h3><span id="44-训练方式优化">4.4 训练方式优化</span></h3><p><span style="color:red">针对匹配、检索任务训练方式的局限性，进行训练相关包括方式数据等的调优</span></p><ul><li><strong>[26] RocketQA: An Optimized Training Approach to Dense Passage Retrievalfor Open-Domain Question Answering</strong>  </li></ul><p>动机：开放域QA中，双编码器体系结构的诸多问题：训练(某个候选被选中的概率)和推理(全部候选中选择positive)之间的差异、大量无标注的正例、有限的训练数据。（这些应该是更多出现在第一阶段检索）</p><p>贡献：一种优化的训练方法RocketQA，三项主要技术贡献：cross-batch random negatives；排名靠前的结果去除假阴性；无监督数据进行数据扩充（伪标签），效率低下但有效</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201030110924561.png" alt="image-20201030110924561" style="zoom: 50%;"></p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201030111122817.png" alt="image-20201030111122817"></p><ul><li><strong>[27] Approximate Nearest Neighbor Negative ContrastiveLearning for Dense Text Retrieval</strong>   </li></ul><p>第一阶段检索任务</p><p>端到端学习的dense retrieval有很多优点，但是通常不如word-base的sparse retrieval？？（在第一阶段检索）</p><p>贡献：密集检索的学习瓶颈是由于局部采样的非信息性负数占主导；提出近似最近邻居负面对比学习方法（一种训练 学习机制）</p><p><strong>[28] SentPWNet: A Unified Sentence Pair Weighting Network forTask-specific Sentence Embedding</strong></p><p>动机：pair-based 指标学习中，采样的句子偏离所有句子对的真实分布时，句子表示可能会有偏差。</p><p>贡献：提出一个统一的局部加权和学习框架。</p><p><strong>[29] Strategy of the Negative Sampling for Training Retrieval-Based Dialogue Systems.  PerCom Workshops 2019</strong></p><p><strong>[30] Generating Natural Language Adversarial Examples through Probability Weighted Word Saliency 2019ACL</strong></p><p>文本分类的对抗样本问题</p><h3><span id="45-结合辅助信息">4.5 结合辅助信息</span></h3><ul><li><p>[31] Dialogue Response Ranking Training with Large-Scale Human Feedback Data EMNLP 2020 <a href="https://arxiv.org/pdf/2009.06978.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2009.06978.pdf</a></p><p>用户反馈提高对话检索</p><p><img src="/2020/10/22/nlp-jin-jie/shen-du-wen-ben-pi-pei-survey2020/image-20201025180941523.png" alt="image-20201025180941523" style="zoom:50%;"></p></li></ul><ul><li><p>[32] <a href="https://mp.weixin.qq.com/s/y4n6Ufh8PvSzXyftsXWw1g" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/y4n6Ufh8PvSzXyftsXWw1g</a> 引入知识图谱相关信息</p></li><li><p>[33] FAQ-based Question Answering via Knowledge Anchors NLPCC2020 <a href="https://arxiv.org/pdf/1911.05930.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1911.05930.pdf</a>  </p></li></ul><h3><span id="46-其他">4.6 其他</span></h3><ul><li>数据集相关   [35] Selection Bias Explorations and Debias Methods for Natural Language Sentence Matching Datasets <a href="https://arxiv.org/pdf/1905.06221.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1905.06221.pdf</a></li></ul><h2><span id="reference">Reference</span></h2><p><a href="https://mp.weixin.qq.com/s?__biz=MzI0NTg1MTI1NQ==&amp;amp;mid=2247484166&amp;amp;idx=1&amp;amp;sn=99e78d78013092e6b712d5a4e5efd487&amp;amp;chksm=e949761ede3eff080f381dfb5b618282d76a2349c1b3d664d05a1d39878e5d1f744219e6cd7e&amp;amp;mpshare=1&amp;amp;scene=23&amp;amp;srcid=1022VqCPD7HGFwpBSOtBqvWe&amp;amp;sharer_sharetime=1603375088507&amp;amp;sharer_shareid=59332ea7c33ee752808701f0287171ae#rd" target="_blank" rel="noopener">基于深度学习的FAQ问答系统</a></p><p>文本匹配技术前沿研究进展调研 - zhiqliu</p><p>文本匹配前沿研究文献阅读 - Yangwei</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度文本匹配survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S部署</title>
      <link href="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/"/>
      <url>/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-部署概览">1 部署概览</span></h2><p>平台规划</p><ul><li>单master，master直接管理多个node节点</li><li>多master，master和node之间多一个负载均衡组件（高可用）</li></ul><p>硬件配置要求</p><ul><li>测试环境：master-<strong>2核</strong>4G20G，node-4核8G40G</li></ul><p>集群部署方式</p><ul><li><p>kubeadm：一个K8S部署工具</p><ul><li>创建一个Master节点： kubeadm init</li><li>将node节点加入到当前集群中： kubeadm join <master的ip和端口></master的ip和端口></li><li>快速部署，但屏蔽了很多细节，遇到问题较难排查。</li></ul></li><li><p>二进制包</p><ul><li>二进制包，手动部署每个组件</li><li>手动部署非常麻烦，更可控，也利于维护</li></ul></li></ul><h2><span id="2-虚拟机环境-快速搭建">2 虚拟机环境 - 快速搭建</span></h2><h3><span id="21-kubeadm方式">2.1 Kubeadm方式</span></h3><p>准备环境 - 安装软件 - 部署过程 - 测试</p><h4><span id="211-准备环境">2.1.1 准备环境</span></h4><p>准备三个虚拟机，配置好ip</p><ul><li><p>| 角色   | IP            |<br>| ——— | ——————- |<br>| PC     | 192.168.10.18 |<br>| master | 192.168.10.3  |<br>| node1  | 192.168.10.44 |<br>| node2  | 192.168.10.55 |</p><pre><code># 关闭防火墙查看版本： firewall-cmd --version显示状态： firewall-cmd --statesystemctl start firewalldsystemctl stop firewalldsystemctl disable firewalld systemctl start firewalld# 关闭selinux（Linux 的一个安全子系统）sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config  # 永久setenforce 0  # 临时# 关闭swapswapoff -a  # 临时sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab    # 永久# 根据规划设置主机名hostnamectl set-hostname &lt;hostname&gt;hostnamectl set-hostname Masterhostnamectl set-hostname Node2hostnamectl set-hostname Node3# 在master添加hostscat &gt;&gt; /etc/hosts &lt;&lt; EOF192.168.10.3 Master192.168.10.44 Node2192.168.10.55 Node3EOF# 配置路由参数,防止kubeadm报路由警告cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system  # 生效</code></pre></li></ul><p>时间同步</p><pre><code>  yum install ntpdate -y  ntpdate time.windows.com</code></pre><h4><span id="222-软件安装">2.2.2 软件安装</span></h4><p><strong>Docker相关</strong></p><p>安装docker - 使用安装脚本自动安装</p><pre><code>curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun或者：curl -sSL https://get.daocloud.io/docker | sh</code></pre><p>设置开机启动</p><pre><code>systemctl enable docker &amp;&amp; systemctl start dockerdocker --version</code></pre><p>docker镜像源</p><pre><code>vi /etc/docker/daemon.json systemctl restart docker.service$ cat &gt; /etc/docker/daemon.json &lt;&lt; EOF{  &quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]}EOF</code></pre><p><strong>安装kubernetes源</strong></p><pre><code>$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF</code></pre><p><strong>安装kubeadm，kubelet和kubectl，并开机启动</strong></p><pre><code># 由于版本更新频繁，这里指定版本号部署：$ yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0$ systemctl enable kubelet</code></pre><h4><span id="223-部署过程">2.2.3 部署过程</span></h4><p><strong>部署Kubernetes Master</strong></p><p>在Master 192.168.10.3 执行。</p><pre><code>$ kubeadm init \  --apiserver-advertise-address=192.168.10.3 \  --image-repository registry.aliyuncs.com/google_containers \  --kubernetes-version v1.18.0 \  --service-cidr=10.96.0.0/12 \  --pod-network-cidr=10.244.0.0/16</code></pre><p>由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里指定阿里云镜像仓库地址。</p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020160006708.png" alt="image-20201020160006708"></p><p>使用kubectl工具：</p><pre class=" language-lang-bash"><code class="language-lang-bash">mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config$ kubectl get nodes</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020160146561.png" alt="image-20201020160146561"></p><p>如果装乱了重来：</p><pre><code># 卸载服务kubeadm reset# 删除rpm包rpm -qa|grep kube*|xargs rpm --nodeps -e# 删除容器及镜像docker images -qa|xargs docker rmi -f</code></pre><p>加入Kubernetes Node</p><p>在要被添加的Node节点虚拟机中执行。</p><p>向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：</p><pre><code>$ kubeadm join 192.168.10.3:6443 --token lgxrim.idkgqpiowuzdbch9 \    --discovery-token-ca-cert-hash sha256:9266780f8008eb60d6d154d09bd20e71fd50455f1b4d7bb8c6308f0c483a3ef7</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020161052754.png" alt="image-20201020161052754"></p><p>默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下：</p><pre><code>kubeadm token create --print-join-command</code></pre><p><strong>部署CNI网络插件</strong></p><p>部署CNI网络插件</p><pre><code>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><p>默认镜像地址无法访问，sed命令修改为docker hub镜像仓库。</p><pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml$ kubectl get pods -n kube-system</code></pre><p>多等一会才会启动完成：</p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020161446118.png" alt="image-20201020161446118"></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020161726697.png" alt="image-20201020161726697"></p><h4><span id="224-部署应用-测试kubernetes集群">2.2.4 部署应用-测试kubernetes集群</span></h4><p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p><pre><code>$ kubectl create deployment nginx --image=nginx通过yaml文件来部署$ kubectl create deployment web --image=nginx --dry-run -o yaml &gt; web.yaml$ kubectl apply -f web.yaml</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020161900890.png" alt="image-20201020161900890"></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020162021491.png" alt="image-20201020162021491"></p><p>pod创建不成功时，查看日志：</p><pre><code>kubectl describe pod nginx</code></pre><p>删除pod和deployment</p><pre><code>[root@k8smaster ~]# kubectl get  pod[root@k8smaster ~]# kubectl delete pod nginx-86c57db685-shkkq[root@k8smaster ~]# kubectl get deployment[root@k8smaster ~]# kubectl delete deployment nginx</code></pre><pre><code>对外暴露端口$ kubectl expose deployment nginx --port=80 --type=NodePort$ kubectl get pod,svc</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020162225756.png" alt="image-20201020162225756"></p><p>访问地址：<a href="http://NodeIP:Port" target="_blank" rel="noopener">http://NodeIP:Port</a>  <a href="http://192.168.10.44:30695" target="_blank" rel="noopener">http://192.168.10.44:30695</a></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020162447346.png" alt="image-20201020162447346"></p><h3><span id="22-二进制包的方式部署和搭建">2.2 二进制包的方式部署和搭建</span></h3><ol><li>准备环境</li><li>安装软件</li><li>为etcd和apiserver自签证书</li><li>部署etcd集群</li><li>部署master组件<ol><li>kube-apiserver</li><li>kube-controller-manager</li><li>kube-scheduler</li><li>etcd</li></ol></li><li>部署node组件<ol><li>kubelet</li><li>kube-proxy</li><li>docker</li><li>etcd</li></ol></li><li>部署集群中网络插件</li></ol><h2><span id="3-生产环境部署">3 生产环境部署</span></h2><h3><span id="31-准备环境">3.1 准备环境</span></h3><p>各个节点的IP</p><ul><li>| 角色                | IP         |<br>| —————————- | ————— |<br>| K8S_Node1（Master） | 10.60.1.96 |<br>| K8S_Node3           | 10.60.1.98 |</li></ul><p>防火墙与端口</p><pre><code># 生产环境关闭防火墙不安全，所以开放几个端口(未测试是否生效，因为被服务器的网络搞的心态崩了索性把防火墙关了，服务器不连外网所以问题不大)查看版本： firewall-cmd --version显示状态： firewall-cmd --statesystemctl stop firewalldsystemctl disable firewalldMaster节点：TCP    入站    6443*    Kubernetes API 服务器    所有组件TCP    入站    2379-2380    etcd server client API    kube-apiserver, etcdTCP    入站    10250    Kubelet API    kubelet 自身、控制平面组件TCP    入站    10251    kube-scheduler    kube-scheduler 自身TCP    入站    10252    kube-controller-manager    kube-controller-manager 自身Node节点：TCP    入站    10250    Kubelet API    kubelet 自身、控制平面组件TCP    入站    30000-32767    NodePort 服务**    所有组件查看版本： firewall-cmd --version显示状态： firewall-cmd --state     systemctl status firewalld查看已经开放的端口：firewall-cmd --list-ports开放端口：firewall-cmd --zone=public --add-port=80/tcp --permanent firewall-cmd --zone=public --add-port=6443/tcp --permanent firewall-cmd --zone=public --add-port=2379/tcp --permanent firewall-cmd --zone=public --add-port=2380/tcp --permanent firewall-cmd --zone=public --add-port=10250/tcp --permanent firewall-cmd --zone=public --add-port=10251/tcp --permanent firewall-cmd --zone=public --add-port=10252/tcp --permanent systemctl reload firewalld重启失败的话可以重新开启   systemctl start firewalldfirewall-cmd --list-ports</code></pre><pre><code># 关闭selinux（Linux 的一个安全子系统） sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config  # 永久setenforce 0  # 临时# 关闭swapswapoff -a  # 临时 sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab    # 永久# 根据规划设置主机名hostname 查看主机名hostname -i：查看本机对应的IPhostnamectl set-hostname &lt;hostname&gt; hostnamectl set-hostname K8SNode1 hostnamectl set-hostname K8SNode2 hostnamectl set-hostname K8SNode3 hostnamectl set-hostname K8SNode4# 在master添加hosts cat &gt;&gt; /etc/hosts &lt;&lt; EOF10.60.1.96 K8SNode110.60.1.98 K8SNode3EOF# 配置路由参数,防止kubeadm报路由警告cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system  # 生效# 时间同步yum install ntpdate -yntpdate time.windows.com遇到：16 Nov 17:07:33 ntpdate[416478]: no server suitable for synchronization found开放防火墙123端口无效</code></pre><h3><span id="32-软件安装与部署">3.2 软件安装与部署</span></h3><p>Docker 参考之前的</p><p>内网服务器不通外网，kubeadm无法拉取docker镜像，很头疼</p><p>这里尝试了一些离线部署方式：<a href="https://github.com/haiker2011/k8s-offline-install" target="_blank" rel="noopener">https://github.com/haiker2011/k8s-offline-install</a></p><p>sealos 需要ssh到root： <a href="https://github.com/fanux/sealos" target="_blank" rel="noopener">https://github.com/fanux/sealos</a> </p><p><a href="https://www.cnblogs.com/straycats/p/8506959.html" target="_blank" rel="noopener">https://www.cnblogs.com/straycats/p/8506959.html</a> 中分享了一个离线包但是版本太旧了</p><pre><code>tar -xjvf k8s_images.tar.bz2 docker load &lt;k8s_images/docker_images/etcd-amd64_v3.1.10.tar docker load &lt;k8s_images/docker_images/flannel_v0.9.1-amd64.tar docker load &lt;k8s_images/docker_images/k8s-dns-dnsmasq-nanny-amd64_v1.14.7.tar docker load &lt;k8s_images/docker_images/k8s-dns-kube-dns-amd64_1.14.7.tar docker load &lt;k8s_images/docker_images/k8s-dns-sidecar-amd64_1.14.7.tar docker load &lt;k8s_images/docker_images/kube-apiserver-amd64_v1.9.0.tar docker load &lt;k8s_images/docker_images/kube-controller-manager-amd64_v1.9.0.tar docker load &lt;k8s_images/docker_images/kube-scheduler-amd64_v1.9.0.tar docker load &lt;k8s_images/docker_images/kube-proxy-amd64_v1.9.0.tar docker load &lt;k8s_images/docker_images/pause-amd64_3.0.tar docker load &lt;k8s_images/kubernetes-dashboard_v1.8.1.tar</code></pre><pre><code>cd k8s_images rpm -ivh socat-1.7.3.2-2.el7.x86_64.rpm rpm -ivh kubernetes-cni-0.6.0-0.x86_64.rpm   rpm -ivh kubelet-1.9.9-9.x86_64.rpm rpm -ivh kubectl-1.9.0-0.x86_64.rpm rpm -ivh kubeadm-1.9.0-0.x86_64.rpm</code></pre><p>yum remove kube*</p><p>新找的离线包进行部署：</p><pre><code> rpm -ivh  ...</code></pre><p>遇到的问题：获取的截获离线安装包在安装过程中缺少依赖，离线安装了一些缺少的依赖后：1. 一些依赖不生效； 2. 一些依赖包网上找不到。</p><p>总体而言，离线部署的问题：</p><ol><li>缺乏服务器环境适配的离线包和依赖包。</li><li>没有一个详尽的文档，踩的坑很多，无从下手。</li></ol><p>最终要来了root账号，使用sealos来部署。。。</p><h2><span id="4-sealos-生产环境部署">4 Sealos 生产环境部署</span></h2><h3><span id="41-准备工作">4.1 准备工作</span></h3><p>各个节点的IP</p><ul><li>| 角色       | IP         |<br>| ————— | ————— |<br>| K8S_Master | 10.60.1.97 |<br>| K8S_Node   | 10.60.1.99 |</li></ul><pre><code># 下载并安装sealos, sealos是个golang的二进制工具，直接下载拷贝到bin目录即可, release页面也可下载$ wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/latest/sealos &amp;&amp; \    chmod +x sealos &amp;&amp; cp sealos /usr/bin # 下载离线资源包$ wget -c https://sealyun.oss-cn-beijing.aliyuncs.com/7b6af025d4884fdd5cd51a674994359c-1.18.0/kube1.18.0.tar.gz# 安装一个单 master的kubernetes集群$ sealos init --passwd &#39;dcy2020@ict.com&#39; --master 10.60.1.97  --node 10.60.1.99        --pkg-url /home/nlper/huotengfei/kube1.18.0.tar.gz     --version v1.18.0</code></pre><h3><span id="42-bugs">4.2 Bugs</span></h3><pre><code>sealos init 执行无响应</code></pre><p>重新安装之前的版本（例如3.3.8）解决问题</p><pre><code>[ERROR SystemVerification]: unsupported graph driver: vfs</code></pre><p>需要系统文件格式为ext4，需要格式化系统；</p><p>或者将docker的文件存储格式改为overlay2，然而docker就启动不起来了，可能是因为linux内核版本为3，需要升级内核</p><pre><code>vi /etc/docker/daemon.json, 添加如下 &quot;storage-driver&quot;: &quot;overlay2&quot;,  &quot;storage-opts&quot;: [    &quot;overlay2.override_kernel_check=true&quot;  ]</code></pre><p>docker存储驱动改成overlay2后出现的问题： 无法启动docker</p><pre><code>$ systemctl start dockerJob for docker.service failed because the control process exited with error code. See &quot;systemctl status docker.service&quot; and &quot;journalctl -xe&quot; for details.</code></pre><pre><code>查看错误信息$ systemctl status docker.service -lfailed to start daemon: error initializing graphdriver: overlay2: the backing xfs filesystem is formatted without d_type support, which leads to incorrect behavior. Reformat the filesystem with ftype=1 to enable d_type support. Backing filesystems without d_type support are not supported.</code></pre><pre><code>无效的尝试vi daemon.json{  &quot;storage-driver&quot;: &quot;overlay2&quot;,  &quot;storage-opts&quot;: [    &quot;overlay2.override_kernel_check=true&quot;  ]}无效的尝试2https://www.jianshu.com/p/93518610eea1Nov 16 15:45:52 k8snode2 dockerd[382923]: time=&quot;2020-11-16T15:45:52.943234353+08:00&quot; level=warning msg=&quot;could not use snapshotter zfs in metadata plugin&quot; error=&quot;path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter&quot;无效的尝试3https://www.cnblogs.com/ayanmw/p/10258171.html</code></pre><p>将docker的文件存储格式改为overlay，device-mapper 试试： 也是一样，都不支持</p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201118152727459.png" alt="image-20201118152727459"></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201125123127574.png" alt="image-20201125123127574"></p><h2><span id="5-离线包生产环境部署">5 离线包生产环境部署</span></h2><h3><span id="51-准备工作">5.1 准备工作</span></h3><p>各个节点的IP</p><ul><li>| 角色      | IP         |<br>| ————- | ————— |<br>| k8snode2  | 10.60.1.99 |<br>| k8smaster | 10.60.1.97 |</li></ul><p>防火墙与端口</p><pre><code># 生产环境关闭防火墙不安全，所以开放几个端口(未测试是否生效，因为被服务器的网络搞的心态崩了索性把防火墙关了，服务器不连外网所以问题不大)查看版本： firewall-cmd --version显示状态： firewall-cmd --statesystemctl stop firewalldsystemctl disable firewalldMaster节点：TCP    入站    6443*    Kubernetes API 服务器    所有组件TCP    入站    2379-2380    etcd server client API    kube-apiserver, etcdTCP    入站    10250    Kubelet API    kubelet 自身、控制平面组件TCP    入站    10251    kube-scheduler    kube-scheduler 自身TCP    入站    10252    kube-controller-manager    kube-controller-manager 自身Node节点：TCP    入站    10250    Kubelet API    kubelet 自身、控制平面组件TCP    入站    30000-32767    NodePort 服务**    所有组件查看版本： firewall-cmd --version显示状态： firewall-cmd --state     systemctl status firewalld查看已经开放的端口：firewall-cmd --list-ports开放端口：firewall-cmd --zone=public --add-port=80/tcp --permanent firewall-cmd --zone=public --add-port=6443/tcp --permanent firewall-cmd --zone=public --add-port=2379/tcp --permanent firewall-cmd --zone=public --add-port=2380/tcp --permanent firewall-cmd --zone=public --add-port=10250/tcp --permanent firewall-cmd --zone=public --add-port=10251/tcp --permanent firewall-cmd --zone=public --add-port=10252/tcp --permanent systemctl reload firewalld重启失败的话可以重新开启   systemctl start firewalldfirewall-cmd --list-ports</code></pre><pre><code># 关闭selinux（Linux 的一个安全子系统） sed -i &#39;s/enforcing/disabled/&#39; /etc/selinux/config  # 永久setenforce 0  # 临时ls# 关闭swapswapoff -a  # 临时 sed -ri &#39;s/.*swap.*/#&amp;/&#39; /etc/fstab    # 永久# 根据规划设置主机名hostname 查看主机名hostname -i：查看本机对应的IPhostnamectl set-hostname &lt;hostname&gt; hostnamectl set-hostname K8SNode2 hostnamectl set-hostname K8SNode4# 在master添加hosts cat &gt;&gt; /etc/hosts &lt;&lt; EOF10.60.1.96 K8SNode110.60.1.98 K8SNode3EOF# 配置路由参数,防止kubeadm报路由警告cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1EOFsysctl --system  # 生效# 时间同步yum install ntpdate -yntpdate time.windows.com遇到：16 Nov 17:07:33 ntpdate[416478]: no server suitable for synchronization found开放防火墙123端口无效</code></pre><p>配置harbor： 证书、host</p><h3><span id="52-软件安装">5.2 软件安装</span></h3><p>k8s 离线软件<strong>kubeadm、kubelet、kubectl</strong></p><pre><code> rpm -ivh xxx.rpm</code></pre><p>load镜像</p><pre><code>tar -xjvf k8s_images.tar.bz2 docker load &lt;k8s_images/docker_images/etcd-amd64_v3.1.10.tar</code></pre><p>修改kubelet的配置文件 - 关于docker</p><h3><span id="53-搭建集群">5.3 搭建集群</span></h3><p>kubeadm init</p><pre><code>kubeadm init --kubernetes-version=v1.16.4  --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=10.60.1.97</code></pre><p>使用kubectl工具在master节点进行如下操作：</p><pre class=" language-lang-bash"><code class="language-lang-bash"> rm -rf $HOME/.kube mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config$  kubectl get nodes</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020160146561.png" alt="image-20201020160146561"></p><p><strong>加入Kubernetes Node</strong></p><p>在要被添加的Node节点虚拟机中执行。</p><p>向集群添加新节点，执行在kubeadm init输出的kubeadm join命令：</p><pre><code>kubeadm join 10.60.1.97:6443 --token p1fxnc.p41jgp16q33sw04r \    --discovery-token-ca-cert-hash sha256:a88cb283e9768f031acbb400f98dd523e10ab8ca1d657b42c672d69fd9cb936e</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020161052754.png" alt="image-20201020161052754"></p><p>默认token有效期为24小时，当过期之后，该token就不可用了。这时就需要重新创建token，操作如下（还没试过）：</p><pre><code>kubeadm token create --print-join-command# 永久token?kubeadm token create --ttl 0kubeadm token list</code></pre><p><strong>部署CNI网络插件</strong></p><p>部署CNI网络插件</p><pre><code>wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</code></pre><pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml$ kubectl get pods -n kube-system</code></pre><p>发现官网给的命令拉镜像失败</p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201204111138177.png" alt="image-20201204111138177" style="zoom:67%;"></p><p>查看kube-flannel.yml文件发现本地镜像名字没对应上，于是修改镜像名称：</p><pre><code>docker tag quay.io/coreos/flannel:v0.11.0-amd64  quay.io/coreos/flannel:v0.13.1-rc1</code></pre><p>发现节点not ready,在节点查看日志，定位问题</p><pre><code>journalctl -f -u kubelet</code></pre><p>Failed to start ContainerManager failed to initialize top level QOS containers:</p><pre><code> systemctl stop kubepods.slice systemctl restart kubeletfrom: http://www.ichenfu.com/2019/12/06/kubelet-failed-to-initialize-top-level-qos-containers/</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201204193252438.png" alt="image-20201204193252438"></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201204193304895.png" alt="image-20201204193304895"></p><h3><span id="54-部署应用-测试kubernetes集群">5.4 部署应用-测试kubernetes集群</span></h3><p>在Kubernetes集群中创建一个pod，验证是否正常运行：</p><pre><code>$  kubectl create deployment nginx --image=nginx通过yaml文件来部署$  kubectl create deployment nginx --image=nginx --dry-run -o yaml &gt; nginx.yaml修改yaml让其添加本地镜像：apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx  namespace: defaultspec:  selector:    matchLabels:      app: nginx  replicas: 3  template:    metadata:      labels:        app: nginx    spec:      containers:        - name: nginx          image: hub.ict.ac.cn/wde/nginx:latest          imagePullPolicy: Never          ports:            - containerPort: 80          securityContext:            privileged: true$  kubectl apply -f nginx.yaml</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020161900890.png" alt="image-20201020161900890"></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201020162021491.png" alt="image-20201020162021491"></p><pre><code>对外暴露端口$  kubectl expose deployment nginx --port=80 --type=NodePort$  kubectl get pod,svc</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201204193501504.png" alt="image-20201204193501504"></p><p>访问地址：<a href="http://NodeIP:Port" target="_blank" rel="noopener">http://NodeIP:Port</a>  <a href="http://10.60.1.99:32546" target="_blank" rel="noopener">http://10.60.1.99:32546</a></p><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201204193534427.png" alt="image-20201204193534427"></p><h2><span id="5目前的进展与问题">5目前的进展与问题</span></h2><h3><span id="1109-问题">11.09 问题</span></h3><p>离线部署问题</p><ul><li><p>服务器离线环境，即使挂上代理后</p><ul><li>k8S插件kubelet、 kubeadm、 kubectl 网络or镜像问题yum不下来，配置了k8S阿里源也不行</li><li>插件需要的镜像也拉不下来，配置了阿里源也不行</li></ul></li><li><p>使用离线包下载</p><ul><li>官方网站不公开离线rpm包</li><li>k8S节点部署所需要的镜像离线包也没有</li><li>在一些博客分享的离线包版本不一致或too old</li></ul></li></ul><p><strong>请教老师后</strong></p><p>k8S的离线安装受到网络环境限制，体现在yum源插件获取、插件安装过程需要拉取的docker镜像</p><ol><li>推荐了sealos离线部署方式<br>可行性很大，但是尝试过程中需要ssh到root账号（非）<br>不知是否可以提供？或者可以另开2台没有其他服务的服务器，以免影响其他进程</li></ol><ol><li><p>获取了ZK老师截获的离线rpm包和habor上的镜像<br>推荐了ansiable来控制集群，也提供了一套3master集群方案。但是需要和系统版本和软件生态对应（例如需要系统版本centos7.6，现有版本为7.2）</p><p>关于版本问题，张凯老师建议我部署到能重装系统的虚拟机</p><p>可以尝试，但结果不可控</p><p>离线包部署非常复杂，时间耗费多</p></li><li><p>张凯老师当时在闲置的windows上的虚拟机上部署</p></li></ol><h3><span id="1124-问题">11.24 问题</span></h3><p><strong>离线包手动部署：</strong></p><p>过程中的很多问题提issue都不知道该往哪提</p><p>安装现有的rpm包过程中，系统提示缺少核心依赖，找到一部分，其中一部分始终没有找到</p><p>离线部署并没有一个完整详尽的文档，要踩的坑还很多，终日不断的试错却没有进展让人 心力交瘁，身心折磨。</p><p><strong>离线工具部署 - sealos</strong></p><p>过程中解决了很多问题，issue都提了很多个</p><p><a href="https://github.com/fanux/sealos/issues/527" target="_blank" rel="noopener">https://github.com/fanux/sealos/issues/527</a> 出现存储格式不兼容问题无法继续</p><p>解决的办法：1. 将系统文件格式改为ext4，需要格式化系统；2. 将docker的文件存储格式改为overlay2或其他，然而docker就启动不起来了，因为linux内核版本过低且存储系统不支持d_type，需要升级内核。。。</p><p><strong>建议换到一个可升级内核重装系统(可联外网就更好了)的部署环境</strong>or <strong>请天玑熟悉这块的运维老师来部署</strong></p><font size="4" color="red"> ...</font> <h3><span id="1208-进展">12.08 进展</span></h3><p>已经搭建好了k8s 集群，目前是单master多node的集群。</p><p>总结：</p><p>安装工具所需的软件依赖与yum源配置、安装工具所需的docker镜像、集群搭建的各种bug通过查看日志来定向解决（例如CNI网络插件、cgroup配置等)</p><p>接下来就是把现有的服务部署一下。</p><h2><span id="6-常用命令">6 常用命令</span></h2><h3><span id="61-检查日志和错误等">6.1 检查日志和错误等</span></h3><pre><code> journalctl -xefu kubeletsystemctl status kubeletdocker ps | grep NAMEkubectl get cs kubectl get nodes #获取集群状态 kubectl get pods -n kube-system</code></pre><h3><span id="62-重启与恢复">6.2 重启与恢复</span></h3><pre><code> systemctl daemon-reload  systemctl restart kubelet</code></pre><h3><span id="63-部署相关">6.3 部署相关</span></h3><p>重置系统（慎用！！！）</p><pre><code># 卸载服务kubeadm reset# 删除rpm包rpm -qa|grep kube*|xargs rpm --nodeps -e# 删除容器及镜像docker images -qa|xargs docker rmi -f</code></pre><p>重置集群（慎用！）</p><pre><code> kubeadm reset systemctl stop kubelet systemctl stop docker rm -rf /var/lib/cni/ rm -rf /var/lib/kubelet/* rm -rf /etc/cni/ ifconfig cni0 down ifconfig flannel.1 down ifconfig docker0 down ip link delete cni0 ip link delete flannel.1 systemctl start docker systemctl start kubectl</code></pre><p>删除pod（慎用）</p><pre><code> kubectl get  pod kubectl delete pod nginx-86c57db685-shkkq kubectl get deployment kubectl delete deployment nginx kubectl get svc -n default kubectl delete svc nginx -n default</code></pre><h3><span id="常见bug">常见bug</span></h3><p>kubeadm init 报错可能的解决方式</p><pre><code>sed -i &quot;s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g&quot; /etc/systemd/system/kubelet.service.d/10-kubeadm.confsystemctl daemon-reload &amp;&amp; systemctl restart kubelet</code></pre><p>[dcy@k8smaster ~]$  kubectl get nodes<br>Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of “crypto/rsa: verification error” while trying to verify candidate authority certificate “kubernetes”</p><pre><code> rm -rf $HOME/.kube 部署之前没删干净 from: https://blog.csdn.net/woay2008/article/details/93250137</code></pre><p>Failed to start ContainerManager failed to initialize top level QOS containers: root container [kube</p><pre><code>vi /etc/sysconfig/kubeletKUBELET_EXTRA_ARGS=--feature-gates SupportPodPidsLimit=false --feature-gates SupportNodePidsLimit=false再重启 systemctl daemon-reload &amp;&amp; systemctl restart kubeletfrom: https://blog.csdn.net/weixin_44144334/article/details/103356026</code></pre><p>Failed to start ContainerManager failed to initialize top level QOS containers: failed to update top level BestEffort QOS cgroup :</p><pre><code> systemctl stop kubepods.slice systemctl restart kubeletfrom: http://www.ichenfu.com/2019/12/06/kubelet-failed-to-initialize-top-level-qos-containers/</code></pre><p><img src="/2020/10/21/gong-cheng-bu-shu-xiang-guan/k8s-bu-shu/image-20201214161200028.png" alt="image-20201214161200028"></p><p>Failed to start ContainerManager failed to initialize top level QOS containers: root container [kubepods] doesn’t exist:</p><pre><code>解决办法参考这里，在kubelet 的启动参数中加上下面参数并重启--cgroups-per-qos=false  --enforce-node-allocatable=&quot;&quot; vi /etc/systemd/system/kubelet.service.d/10-kubeadm.confEnvironment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroups-per-qos=false  --enforce-node-allocatable=&quot;&quot; &quot;Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --cgroups-per-qos=false  --enforce-node-allocatable=&#39;&#39; &quot;</code></pre><p>failed to run Kubelet: no client provided, cannot use webhook authentication</p><pre><code>上面的配置 vi /etc/systemd/system/kubelet.service.d/10-kubeadm.conf 语法问题？...双引号内的&quot;&quot; 改为单引号</code></pre><p>CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container “ce9f100a2c6d81cde318b8491336a78eaf388af0255fce6885c7729944861f84</p><pre><code></code></pre><p>failed to collect filesystem stats - rootDiskErr: <nil>, extraDiskErr: could not stat “/var/lib/docker/containers/e5d4ab897e065554fa1cd14764031589f17d064269cdb81fc6079daa23509441” to get inode usage: stat /var/lib/docker/containers/e5d4ab897e065554fa1cd14764031589f17d064269cdb81fc6079daa23509441: no such file or directory</nil></p><pre><code></code></pre><p>failed to collect filesystem stats - rootDiskErr: <nil>, extraDiskErr: could not stat</nil></p><pre><code></code></pre><p>[root@k8smaster ~]# kubectl apply -f <a href="https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml" target="_blank" rel="noopener">https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</a><br>Unable to connect to the server: unexpected EOF</p><pre><code>把这个yml文件整到本地去run</code></pre><h2><span id="reference">Reference</span></h2><p><a href="https://www.bilibili.com/video/BV1GT4y1A756?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1GT4y1A756?p=2</a></p><p><a href="https://github.com/haiker2011/k8s-offline-install" target="_blank" rel="noopener">https://github.com/haiker2011/k8s-offline-install</a></p><p><a href="https://github.com/fanux/sealos" target="_blank" rel="noopener">https://github.com/fanux/sealos</a></p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K8S部署 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S</title>
      <link href="/2020/10/16/gong-cheng-bu-shu-xiang-guan/k8s/"/>
      <url>/2020/10/16/gong-cheng-bu-shu-xiang-guan/k8s/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-k8s">1 K8S</span></h2><h3><span id="11-前置内容">1.1 前置内容</span></h3><ul><li>Linux操作</li><li>Docker</li></ul><p>推荐上手顺序</p><ul><li>概念与架构扫盲</li><li><p>从零搭建简单的K8S集群</p><ul><li>基于客户端工具kubeadm</li><li>基于二进制包方式</li></ul></li><li><p>核心概念</p><ul><li>Pod、Controller、service、ingress</li><li>RABC、Helm等</li></ul></li><li><p>搭建集群监控平台系统</p></li><li>搭建高可用K8S集群</li><li>在集群环境部署项目</li></ul><h3><span id="12-概述和特性">1.2 概述和特性</span></h3><p>kubernetes， 简称 K8s， 是用 8 代替 8 个字符“ubernete” 而成的缩写。  </p><p>传统的应用部署：插件和脚本安装应用，应用与当前操作系统绑定</p><p>新的方式：部署容器，容器之间互相隔离，容器与底层设施解耦</p><p>K8S 使部署容器化应用更加简洁和高效，自动化部署和管理应用的多个实例</p><p>K8S特性</p><ul><li>自动装箱：自动化部署应用容器</li><li>自我修复：容器失败时会对容器进行重启；所部署的 Node 节点有问题时， 会对容器进行重新部署和重新调度 ；当容器未通过监控检查时， 会关闭此容器直到容器正常运行时， 才会对外提供服务  </li><li>水平扩展：对应用容器规模扩大和缩小</li><li>服务发现（负载均衡）：节点的调度和分配等</li><li>滚动更新：例如加新应用时，要更新功能：当检测到可用时才会更新</li><li>版本回退</li><li>密钥和配置管理  </li><li>存储编排  </li><li>批处理  </li></ul><h3><span id="13-架构组件">1.3 架构组件</span></h3><ul><li>Master  主控节点<ul><li>API server：集群统一入口（以restful方式，交给etcd存储）</li><li>controler-manager：处理集群中的常规后台任务，一个资源对应一个控制器</li><li>scheduler：节点的调度</li><li>etcd：存储系统，保存集群中的相关数据（pod数据等）</li></ul></li><li>Node  工作节点<ul><li>kebelet：master派到node的代表，管理节点中容器相关的部分</li><li>kube-proxy：提供网络代理，用于负载均衡等操作</li></ul></li></ul><p><img src="/2020/10/16/gong-cheng-bu-shu-xiang-guan/k8s/image-20201016112700213.png" alt="image-20201016112700213"></p><h3><span id="14-核心概念">1.4 核心概念</span></h3><p>Service统一入口进行访问，由controller创建pod进行部署</p><ul><li>Pod<ul><li>最小部署单元，一组容器的集合，容器共享网络，短暂生命周期</li></ul></li><li>Controller<ul><li>确定预期的pod副本数量</li><li>有状态应用部署（依赖网络，需要存储），无状态应用部署</li><li>确保所有node运行同一个pod</li></ul></li><li>Service<ul><li>定义一组pod的访问规则</li></ul></li></ul><h2><span id="2-搭建k8s集群">2 搭建K8S集群</span></h2><p>平台规划</p><ul><li>单master，master直接管理多个node节点</li><li>多master，master和node之间多一个负载均衡组件（高可用）</li></ul><p>硬件配置要求</p><ul><li>测试环境：master-2核4G20G，node-4核8G40G</li></ul><p>集群部署方式</p><ul><li><p>kubeadm：一个K8S部署工具</p><ul><li>创建一个Master节点： kubeadm init</li><li>将node节点加入到当前集群中： kubeadm join <master的ip和端口></master的ip和端口></li><li>快速部署，但屏蔽了很多细节，遇到问题较难排查。</li></ul></li><li><p>二进制包</p><ul><li>二进制包，手动部署每个组件</li><li>手动部署非常麻烦，更可控，也利于维护</li></ul></li></ul><p>详细搭建步骤见另一篇。</p><h2><span id="3-命令行工具-kubectl">3 命令行工具 kubectl</span></h2><p>基本语法： </p><pre><code>$ kubectl [command] [TYPE] [NAME] [flags]command: 对资源执行的操作create get等TYPE： 资源类型例如 node pod等NAME: 资源名称，不指定则显示全部flags: 可选参数 例如 -s 指定kubernetes API server的地址和端口$ kubectl --help 查看帮助文档</code></pre><p><img src="/2020/10/16/gong-cheng-bu-shu-xiang-guan/k8s/image-20201021111636185.png" alt="image-20201021111636185"></p><h2><span id="4-资源编排-yaml文件">4 资源编排 YAML文件</span></h2><p><strong>资源清单文件</strong>： 把资源对象的操作编辑到 YAML 格式文件中</p><p>资源管理和资源对象编排部署，都可以通过声明样式（ YAML） 文件来解决</p><p>通过 kubectl 命令直接使用资源清单文件，就可以对大量资源对象进行编排部署</p><ul><li>语法格式： 键值对的集合<ul><li>通过缩进控制格式（空格可以tab不行）</li><li>字符后要缩进一个空格</li><li>‘—-’ 代表新的yaml文件</li><li>‘#’ 代表注释</li></ul></li><li>文件组成部分<ul><li>控制器定义</li><li>被控制对象</li></ul></li></ul><ul><li><p>快速编写yaml文件</p><ul><li><p>使用kubectl create 命令生成yaml文件，然后进行修改</p><pre><code># 不执行，输出yaml文件kubectl create deployment web --image=nginx -o yaml --dry-run &gt; test.yaml</code></pre><p><img src="/2020/10/16/gong-cheng-bu-shu-xiang-guan/k8s/image-20201021135146772.png" alt="image-20201021135146772"></p></li><li><p>针对部署好的项目，导出yaml文件</p><pre><code>kubectl get deploy nginx -o=yaml --export &gt;test2.yaml</code></pre></li></ul></li></ul><h2><span id="5-核心技术的应用">5 核心技术的应用</span></h2><h3><span id="51-pod">5.1 Pod</span></h3><ul><li>基本概念<ol><li>最小部署单元</li><li>包含多个容器（一组容器的集合）</li><li>一个pod中容器共享网络命名空间</li><li>短暂的</li></ol></li><li>Pod概念思考<ul><li>每个 Pod 都是应用的一个实例， 有专用的 IP  </li><li>一个 Pod 可以有多个容器， 彼此间共享网络和存储资源， 每个 Pod 中有一个 Pause 容器保存所有的容器状态， 通过管理 pause 容器， 达到管理 pod 中所有容器的效果  </li><li>同一个 Pod 中的容器总会被调度到相同 Node 节点， 不同节点间 Pod 的通信基于虚拟二层网络技术实现  </li></ul></li><li>pod存在的意义<ol><li>创建容器使用docker，一个docker对应一个容器，一个容器有进程，一个容器运行一个应用程序</li><li>docker单进程，pod运行多个容器、也是多个应用程序、多进程</li><li>pod的存在是为了亲密性应用<ol><li>应用之间有交互</li><li>网络之间调用（不再需要ip，而是localhost + socket）</li></ol></li></ol></li><li>Pod实现机制<ul><li>共享网络<ul><li>每个pod都会创建一个根容器（Pause）</li><li>每次创建业务容器就会加入到pause容器中（？），从而让所有业务容器放在同一个命名空间中</li></ul></li><li>共享存储 <ul><li>pod持久化存储数据（例如日志和业务数据等）</li><li>引入数据卷概念Volumn，使用数据卷进行持久化存储</li></ul></li></ul></li><li>Pod镜像拉取策略：IfNotPresent  Always  Never</li><li>资源限制：设置能使用的计算资源限额 - CPU和Memory</li><li>Pod重启策略：Always  OnFailure Never</li><li>健康检查：检查容器 - 存活检查 就绪检查</li><li>调度策略<ul><li>调度过程（API server、 etcd存储、Scheduler、kubelet、 Docker）<ul><li>master的调度过程</li><li>node节点的调度过程</li></ul></li><li>影响到调度的属性 - 调度到哪个node<ul><li>pod资源的限制</li><li>节点选择器的标签</li></ul></li></ul></li></ul><h3><span id="52-controller">5.2 Controller</span></h3><p>在集群上管理和运行容器的对象</p><p>pod和controller的关系：pod通过controller实现应用的运维，通过label标签建立关系（pod是抽象的，deployment是真实存在的对象）</p><ul><li><p>deployment应用场景(更多的是无状态应用：web、微服务等)</p><ul><li>部署无状态应用</li><li>管理pod和replicaSet</li><li>部署，滚动升级等功能</li></ul></li><li><p>升级回滚</p><pre><code>应用升级$ kubectl set image deployment web nginx=nginx:1.15查看升级状态$ kubectl rollout status deployment web查看升级版本$ kubectl rollout history deployment web回滚到上一版本$ kubectl rollout undo deployment web回滚到特定版本$ kubectl rollout undo deployment web --to-revision=2</code></pre></li><li><p>弹性伸缩</p><pre><code>kubectl scale deployment web --replicas=10</code></pre></li></ul><h3><span id="53-service">5.3 Service</span></h3><p>服务发现：防止pod失联</p><p>负载均衡：定义一组pod访问策略</p><p>常用service类型：ClusterIP（集群内部使用）、NodePort（外部）、LocalBalancer（外部，公有云）</p><h3><span id="54-部署有状态应用">5.4 部署有状态应用</span></h3><p>部署无状态应用：pod都一样、没有顺序要求、不用考虑在哪个node上运行、随意进行伸缩和扩展</p><p>部署有状态应用：考虑无状态的这些因素：每个pod都是独立的、保持pod启动顺序和唯一性（唯一的网络标识符，持久存储；有序的）</p><p>无头service：ClusterIP的值为None</p><p>使用StatefulSet部署有状态应用</p><p>查看pod时，每个pod都有唯一的名称</p><p>查看service时，ClusterIP 是 None</p><p>每个pod的唯一域名：主机名称.service名称.名称空间.svc.cluster.local</p><p>部署守护进程：让所有Node在同一个Pod中运行 </p><p>DaemonSet</p><p>一次性任务或定时任务</p><h3><span id="55-secret">5.5 Secret</span></h3><p>作用：加密数据存在etcd中，让pod容器以挂载volume方式进行访问</p><h2><span id="6-高可用的k8s搭建">6 高可用的K8S搭建</span></h2><p>多node与多master。涉及master的负载均衡。</p><p>master1需要部署的：</p><ol><li>keepalived</li><li>haproxy</li><li>初始化操作</li><li>安装docker和网络插件</li></ol><p>master2需要部署的：</p><ol><li>keepalived</li><li>haproxy</li><li>添加master2节点到集群</li><li>安装docker和网络插件</li></ol><p>node需要部署的：</p><ol><li>加入到集群中</li><li>安装docker和网络插件</li></ol><p>详细搭建步骤见另一篇。</p><h2><span id="7-在k8s中部署项目">7 在K8S中部署项目</span></h2><ol><li>开发代码阶段<ol><li>编写代码</li><li>测试</li><li>编写DockerFile文件</li></ol></li><li>持续交付/集成<ol><li>代码编译打包</li><li>制作镜像</li><li>上传镜像仓库</li></ol></li><li>应用部署<ol><li>环境准备</li><li>部署项目</li></ol></li><li>运维<ol><li>监控</li><li>故障排查</li><li>升级优化</li></ol></li></ol><p>K8S部署项目流程</p><ol><li>制作镜像（DockerFile）</li><li>推送镜像仓库</li><li>通过控制器Deployment部署镜像</li><li>对外暴露应用（service、ingress）</li><li>运维 </li></ol><h2><span id="reference">Reference</span></h2><p><a href="https://www.bilibili.com/video/BV1GT4y1A756?p=2" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1GT4y1A756?p=2</a></p><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K8S </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>K8S插件</title>
      <link href="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/"/>
      <url>/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-监控插件">1 监控插件</span></h2><p>监控可帮助您确保Kubernetes应用程序平稳运行并排除可能出现的任何问题。</p><p><strong>Prometheus</strong>是一种流行的开源监视工具，许多公司都使用它来监视其IT基础结构。</p><p>还有许多其他监视工具可用：Grafana、cAdvisor、Fluentd、Jaeger、Telepresence、Zabbix</p><h3><span id="11-prometheus结构">1.1 Prometheus结构</span></h3><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/architecture.png" alt="普罗米修斯建筑"></p><p>最左边这块就是采集的，一些短周期的任务，或者一些持久性的任务。</p><p>中间上面这块是服务发现，有很多被监控端时，需要自动发现新加入的节点。k8s内置了服务发现的机制，也就是它会连接k8s的API，去发现你部署的哪些应用，哪些pod，通通暴露出去，方便监控。</p><p>右上角是Prometheus的告警，它告警实现是有一个组件的，Alertmanager,这个组件是接收prometheus发来的告警就是触发了一些预值，会通知Alertmanager,而Alertmanager来处理告警相关的处理，然后发送给接收人，可以是email,也可以是企业微信或者钉钉等。</p><p>中间下面这块是Prometheus本身，内部是有一个TSDB的数据库。内部的采集和展示 Prometheus它都可以完成，但是展示这块UI比较low，所以借助于开源的Grafana来展示。</p><p>所有的被监控端暴露完指标之后，Prometheus会主动的抓取这些指标，存储到自己TSDB数据库里面，提供给Web UI或者Grafana，或者API clients通过PromQL来调用这些数据，PromQL相当于Mysql的SQL，主要是查询这些数据的。</p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/image-20201218145702252.png" alt="image-20201218145702252"></p><h3><span id> </span></h3><p>组件关系： </p><p>Prometheus会抓取被监控端指标，存储到自己TSDB数据库里面，提供给Grafana。</p><p>Node Exporter 负责抓取集群机器本身信息，可被Prometheus获取。</p><h3><span id="12-k8s-监控指标">1.2 K8S 监控指标</span></h3><ul><li><p>Kubernetes本身监控</p><ul><li>Node数量 </li><li>Node资源利用率 </li><li>Pods数量</li><li>资源对象状态 ：比如pod，service,deployment,job这些资源状态，统计</li></ul></li><li><p>Pod监控</p><ul><li><p>Pod数量</p></li><li><p>容器资源利用率 ：每个容器消耗了多少资源，用了多少CPU，用了多少内存</p></li><li><p>应用程序：偏应用程序本身的指标了，一般运维很难拿到，所以需要开发去暴露出来</p></li></ul></li></ul><p>如果想监控node的资源，就可以放一个node_exporter,这是监控node资源的，node_exporter是Linux上的采集器，你放上去你就能采集到当前节点的CPU、内存、网络IO，等待都可以采集的。</p><p>如果想监控容器，k8s内部提供cAdvisor采集器，pod呀，容器都可以采集到这些指标，都是内置的，不需要单独部署，只知道怎么去访问这个Cadvisor就可以了。</p><p>如果想监控k8s资源对象，会部署一个kube-state-metrics这个服务，它会定时的API中获取到这些指标，帮你存取到Prometheus里，要是告警的话，通过Alertmanager发送给一些接收方，通过Grafana可视化展示。</p><h2><span id="2-prometheus-grafana部署">2 Prometheus+ Grafana部署</span></h2><h3><span id="21-prometheus-server部署">2.1 Prometheus Server部署</span></h3><pre><code>cd /root/workspace/visual_file/download...  https://prometheus.io/download/tar -zxvf prometheus-2.23.0.linux-amd64.tar.gzmv prometheus-2.23.0.linux-amd64 prometheus/root/workspace/visual_file/prometheus/prometheus --config.file=/root/workspace/visual_file/prometheus/prometheus.yml &amp;</code></pre><p>使用systemd来启停prometheus</p><pre><code>vim /etc/systemd/system/prometheus.service[Unit]Description=Prometheus Monitoring SystemDocumentation=Prometheus Monitoring System[Service]ExecStart=/root/workspace/visual_file/prometheus/prometheus --config.file=/root/workspace/visual_file/prometheus/prometheus.yml --web.listen-address=:9090[Install]WantedBy=multi-user.target</code></pre><p>启动 &amp; 停止</p><pre><code>systemctl daemon-reloadsystemctl start prometheussystemctl status prometheussystemctl enable prometheushttp://10.60.1.97:9090</code></pre><h3><span id="22-grafana部署">2.2 Grafana部署</span></h3><h4><span id="221-安装-amp-配置">2.2.1 安装 &amp; 配置</span></h4><p>RPM包方式下载安装和启动（可不看）</p><pre><code>download...   https://grafana.com/grafana/download?platform=linuxrpm -ivh --nodeps grafana-7.3.5-1.x86_64.rpm启动/bin/systemctl daemon-reload/bin/systemctl enable grafana-server.service/bin/systemctl start grafana-server.servicehttp://10.60.1.97:3000</code></pre><p>二进制包下载安装和启动（推荐）</p><pre><code># download...  https://grafana.com/grafana/downloadwget https://dl.grafana.com/oss/release/grafana-7.3.6.linux-amd64.tar.gztar -zxvf grafana-7.3.6.linux-amd64.tar.gzmv grafana-7.3.6  grafana</code></pre><pre><code>可修改配置文件vi /root/workspace/visual_file/grafana/conf/defaults.ini</code></pre><p>把grafana-server添加到systemd中</p><pre><code>vi /etc/systemd/system/grafana-server.service[Unit]Description=GrafanaAfter=network.target[Service]ExecStart=/root/workspace/visual_file/grafana/bin/grafana-server -homepath  /root/workspace/visual_file/grafana/Restart=on-failure[Install]WantedBy=multi-user.target</code></pre><p>启动 &amp; 停止</p><pre><code>systemctl start  grafana-serversystemctl status  grafana-serversystemctl enable  grafana-serverhttp://10.60.1.97:3000</code></pre><h4><span id="222-界面配置">2.2.2 界面配置</span></h4><p>启动后的界面：</p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/QQ%E5%9B%BE%E7%89%8720201216203138.png" alt="QQ图片20201216203138" style="zoom: 80%;"></p><p>添加数据源，选择 <strong>prometheus</strong></p><p>Dashboards页面 import “Prometheus 2.0 Stats”</p><p><a href="http://10.60.1.97:9090" target="_blank" rel="noopener">http://10.60.1.97:9090</a></p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/KADG7NZ0LZ8P%606VB8QNEI%606.png" alt="KADG7NZ0LZ8P`6VB8QNEI`6"></p><p>查看数据</p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/KADG7NZ0LZ8P%606VB8QNEI%606-1608122881403.png" alt="KADG7NZ0LZ8P`6VB8QNEI`6"></p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/image-20201218160927860.png" alt="image-20201218160927860"></p><h3><span id="223-配置kubernetes数据源">2.2.3 配置kubernetes数据源</span></h3><p>启动插件：<code>Plugins</code> → <code>kubernetes</code> → <code>Enable</code></p><h2><span id="3-node-exporter采集主机运行数据">3 Node Exporter—采集主机运行数据</span></h2><h3><span id="31-安装部署">3.1 安装部署</span></h3><p>与传统的监控zabbix来对比的话，prometheus-server就像是mysql，负责存储数据。只不过这是时序数据库而不是关系型的数据库。数据的收集还需要其他的客户端，在prometheus中叫做exporter。针对不同的服务，有各种各样的exporter，就好比zabbix的zabbix-agent一样。</p><p>这里为了能够采集到主机的运行指标如CPU, 内存，磁盘等信息。我们可以使用 Node Exporter</p><p>下载：<a href="https://github.com/prometheus/node_exporter/releases" target="_blank" rel="noopener">https://github.com/prometheus/node_exporter/releases</a></p><pre><code>cd /root/workspace/visual_file/被监控的机器安装node-exportertar -xf node_exporter-1.0.1.linux-amd64.tar.gz新建一个目录专门安装各种exportermkdir prometheus_exportermv node_exporter-1.0.1.linux-amd64 prometheus_exporter/cd prometheus_exporter/mv node_exporter-1.0.1.linux-amd64/ node_exporter</code></pre><p>直接打开node_exporter的可执行文件即可启动 node export，默认会启动9100端口。建议使用systemctl来启动</p><pre><code>直接启动/root/workspace/visual_file/node_exporter/node_exporter &amp;配置systemctl启动vim /etc/systemd/system/node_exporter.service [Unit]Description=node_exporterAfter=network.target[Service]Restart=on-failureExecStart=/root/workspace/visual_file/prometheus_exporter/node_exporter/node_exporter[Install]WantedBy=multi-user.target加入开机启动systemctl enable node_exportersystemctl start node_exporter</code></pre><p>命令</p><pre><code>systemctl start node_exportersystemctl status node_exportersystemctl stop node_exporter</code></pre><h3><span id="32-配置prometheus">3.2 配置Prometheus</span></h3><p>配置Prometheus，收集node exporter的数据</p><p>node exporter启动后也就是暴露了9100端口，并没有把数据传到prometheus，还需要在prometheus中配置，让prometheus去pull这个接口的数据。</p><pre><code>vi /root/workspace/visual_file/prometheus/prometheus.ymlscrape_configs:  - job_name: &#39;prometheus&#39;    static_configs:    - targets: [&#39;localhost:9090&#39;]   #采集node exporter监控数据  - job_name: &#39;master&#39;    static_configs:    - targets: [&#39;localhost:9100&#39;]  - job_name: &#39;node1&#39;    static_configs:    - targets: [&#39;10.60.1.99:9100&#39;]</code></pre><p>配置好之后，prometheus 9090 就能看到了</p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/image-20201218144459377.png" alt="image-20201218144459377"></p><p>导入模板</p><p>从官网查找模板 <a href="https://grafana.com/" target="_blank" rel="noopener">https://grafana.com/</a></p><p>下载json，</p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/image-20201218141606045.png" alt="image-20201218141606045" style="zoom:50%;"></p><p><img src="/2020/10/15/gong-cheng-bu-shu-xiang-guan/k8s-cha-jian/image-20201218152732797.png" alt="image-20201218152732797"></p><h2><span id="命令综合">命令综合</span></h2><p>prometheus 启动</p><pre><code>systemctl start prometheussystemctl start  grafana-serversystemctl start node_exporterhttp://10.60.1.97:9090http://10.60.1.97:3000lsof -i:9090lsof -i:9100lsof -i:3000</code></pre><p> 状态查看</p><pre><code>systemctl status prometheussystemctl status  grafana-serversystemctl status node_exporter</code></pre><p>停止</p><pre><code>systemctl stop prometheussystemctl stop  grafana-serversystemctl stop node_exportersystemctl daemon-reload</code></pre><h2><span id="bugs">Bugs</span></h2><p>level=error ts=2020-12-16T12:38:35.749Z caller=main.go:808 err=”error starting web server: listen tcp 0.0.0.0:9090: bind: address already in use”</p><pre><code>lsof -i:9090kill pid</code></pre><p>level=error ts=2020-12-18T02:42:47.814Z caller=node_exporter.go:194 err=”listen tcp :9100: bind: address already in use”</p><pre><code>netstat -pan | grep 9100kill pid</code></pre><p>systemd 启动失败 Failed at step EXEC spawning /root/workspace/visual_file/prometheus: Permission denied</p><pre><code>配置用户名的字段去掉</code></pre><p>Warning: grafana-server.service changed on disk. Run ‘systemctl daemon-reload’ to reload units.</p><pre><code>修改了配置文件需要重新reloadsystemctl daemon-reload</code></pre><h2><span id="reference">Reference</span></h2><p><a href="https://www.cnblogs.com/shawhe/p/11833368.html" target="_blank" rel="noopener">https://www.cnblogs.com/shawhe/p/11833368.html</a></p><p><a href="https://blog.csdn.net/ywd1992/article/details/85989259" target="_blank" rel="noopener">https://blog.csdn.net/ywd1992/article/details/85989259</a></p><p><a href="https://blog.51cto.com/14143894/2438026" target="_blank" rel="noopener">https://blog.51cto.com/14143894/2438026</a></p><p><a href="https://blog.csdn.net/vic_qxz/article/details/109347645" target="_blank" rel="noopener">https://blog.csdn.net/vic_qxz/article/details/109347645</a></p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K8S插件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>springboot</title>
      <link href="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/"/>
      <url>/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-技术栈总结">1 技术栈总结</span></h2><h3><span id="11-之前的技术栈">1.1 之前的技术栈</span></h3><p>JavaSE（servlet tomcat)</p><p>mysql + jdbc</p><p>html + css +jquery + 框架</p><p>javaweb: 原始的MVC三层架构的网站</p><p>SSM: Spring + Spring MVC + MyBatis</p><p>war包：tomcat运行</p><p>Spring 再简化： SpringBoot；微服务架构</p><p>jar包：内嵌tomcat</p><p>服务越来越多：springcloud</p><h3><span id="12-学习路线">1.2 学习路线</span></h3><ul><li>springBoot<ul><li>是什么</li><li>配置如何编写，yaml</li><li>自动装配原理（重要）</li><li>集成web开发：业务的核心</li><li>集成数据库 Druid</li><li>分布式开发：Dubbo（RPC） +  zookeeper</li><li>swagger：接口文档</li><li>任务调度</li><li>springSecurity: shiro</li></ul></li><li><p>springBoot 部署</p></li><li><p>springcloud</p><ul><li>微服务</li><li>springcloud入门</li><li>Restful（调用接口）</li><li>Eureka（服务注册中心)</li><li>Ribbon or Feign（负载均衡）</li><li>HtStrix（服务容灾）</li><li>Zuul（路由网关）</li><li>SpringCloud config ~git</li></ul></li></ul><p>Spring 是为了解决企业级应用开发的复杂性而创建的，简化开发。</p><p>集成第三方框架，开箱即用</p><p>maven，spring，springmvc，springboot</p><h2><span id="2-开始搭建">2 开始搭建</span></h2><h3><span id="21-环境">2.1 环境</span></h3><p>实际版本</p><pre><code>jdk: 1.8maven 3.6.3</code></pre><p>配置jdk和maven</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010154359057.png" alt="image-20201010154359057"></p><h3><span id="22-创建一个spring-boot项目">2.2 创建一个spring boot项目</span></h3><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010163631093.png" alt="image-20201010163631093"></p><p>按照习惯配置</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010163842322.png" alt="image-20201010163842322"></p><p>可以直接添加一些依赖，也可以创建之后再手动加</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010163927993.png" alt="image-20201010163927993"></p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010164054037.png" alt="image-20201010164054037"></p><p>三个方框：主入口，配置文件，单元测试程序</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010164200914.png" alt="image-20201010164200914"></p><h3><span id="23-hello-world测试">2.3 Hello World测试</span></h3><p>main的同级目录下建包</p><pre><code>1、Application.java 建议放到根目录下面,主要用于做一些框架配置2、model 目录主要用于实体与数据访问层（Repository）3、service 层主要是业务类代码4、controller 负责页面访问控制</code></pre><p>controller包内新建一个class，写一个接口</p><p><code>@RestController</code> 的意思就是 Controller 里面的方法都以 json 格式输出，不用再写什么 jackjson 配置的了</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010164401315.png" alt="image-20201010164401315"></p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010164813429.png" alt="image-20201010164813429"></p><p>启动主程序，打开浏览器访问 <code>http://localhost:8080/hello</code>，就可以看到效果了</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201012141910847.png" alt="image-20201012141910847"></p><h3><span id="24-jar包打包并脱离idea执行">2.4 jar包打包并脱离IDEA执行</span></h3><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010162337263.png" alt="image-20201010162337263"></p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010162258977.png" alt="image-20201010162258977"></p><p>修改端口</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010164934088.png" alt="image-20201010164934088"></p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010165017456.png" alt="image-20201010165017456"></p><p>修改banner</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201010165726261.png" alt="image-20201010165726261"></p><h2><span id="3-springboot-入门">3 SpringBoot 入门</span></h2><h3><span id="31-springboot自动装配原理">3.1 SpringBoot自动装配原理</span></h3><p>pom.xml</p><ul><li><pre><code>spring-boot-starter-parent   spring-boot-dependencies: 核心依赖在父工程中在引入一些springBoot依赖时，不需指定版本，因为有对应的版本仓库</code></pre></li><li><pre><code>启动器，就是springboot的启动场景        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;自动导入所有依赖。springboot会将所有场景都变成一个个的启动器想使用什么功能，就找对应的启动器 `starter`</code></pre></li></ul><p>主程序</p><ul><li><pre><code>SpringApplication.run(SpringbootHelloworldApplication.class, args); 启动命令注解：@SpringBootApplication  标注这个类是一个springboot的应用这是一个组合注解：包括配置，导入等</code></pre></li></ul><h3><span id="32-yaml语法">3.2 Yaml语法</span></h3><p>两种配置方式 用于修改默认配置：</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201012110154739.png" alt="image-20201012110154739" style="zoom: 67%;"></p><p>properties基本语法（只能保存key-value对）</p><pre><code>server.port=8080</code></pre><p>yaml基本语法</p><pre><code>server.port:8080yaml 还能存对象student:   （对空格要求敏感）  name: balala  age: 18存对象的行内写法： student:{ name: balala, age: 18}存数组pets: [cat, dog, pig]</code></pre><h3><span id="33-给属性赋值的方法">3.3 给属性赋值的方法</span></h3><p>编写一个实体类，用 @Value 给bean注入属性值的！</p><pre><code>@Component  //注册bean到容器中public class Dog {    @Value(&quot;阿肖&quot;)    private String name;    @Value(&quot;18&quot;)    private Integer age;    @Override    public String toString() {        return &quot;Dog{&quot; +                &quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +                &quot;, age=&quot; + age +                &#39;}&#39;;    }}</code></pre><p>在SpringBoot的测试类下注入狗狗输出一下；</p><pre><code>@SpringBootTestclass SpringbootHelloworldApplicationTests {    @Autowired //将狗狗自动注入进来    Dog dog;    @Test    void contextLoads() {        System.out.println(dog); //打印看下狗狗对象    }}</code></pre><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201012152245403.png" alt="image-20201012152245403"></p><p>使用yaml配置的方式进行注入</p><pre><code>编写一个Person类，更复杂一些@Component //注册bean到容器中public class Person {    private String name;    private Integer age;    private Boolean happy;    private Date birth;    private Map&lt;String,Object&gt; maps;    private List&lt;Object&gt; lists;    private Dog dog;    @Override    public String toString() {        return &quot;Person{&quot; +                &quot;name=&#39;&quot; + name + &#39;\&#39;&#39; +                &quot;, age=&quot; + age +                &quot;, happy=&quot; + happy +                &quot;, birth=&quot; + birth +                &quot;, maps=&quot; + maps +                &quot;, lists=&quot; + lists +                &quot;, dog=&quot; + dog +                &#39;}&#39;;    }}</code></pre><p>application.yml中：</p><pre><code>person:  name: qinjiang  age: 3  happy: false  birth: 2000/01/01  maps: {k1: v1,k2: v2}  lists:    - code    - girl    - music  dog:    name: 阿肖    age: 3</code></pre><p>缺少的依赖可以添加：</p><p>方框处是将属性注入类中</p><p><img src="/2020/10/10/gong-cheng-bu-shu-xiang-guan/springboot/image-20201012152727857.png" alt="image-20201012152727857"></p><h2><span id="4-springboot-web开发">4 SpringBoot Web开发</span></h2><p>Spring Boot Web 开发包括常用的 json 输出、filters、property、log 等</p><h3><span id="41-json-接口开发">4.1 json 接口开发</span></h3><p>只需要为类添加 @RestController ，类中的方法都会以json的格式返回</p><pre><code>@RestControllerpublic class HelloController {    @GetMapping(&quot;/hello&quot;)    public String hello(){        return &quot;Hello World!&quot;;    }}</code></pre><h3><span id="42-自定义-filter">4.2 自定义 Filter</span></h3><ol><li>实现 Filter 接口，实现 Filter 方法</li><li>添加<code>@Configuration</code> 注解，将自定义Filter加入过滤链</li></ol><h3><span id="43-配置文件property">4.3 配置文件Property</span></h3><p>application.properties 中：</p><pre><code>com.neo.title=纯洁的微笑com.neo.description=分享生活和技术</code></pre><p>自定义配置类</p><pre><code>@Componentpublic class NeoProperties {    @Value(&quot;${com.neo.title}&quot;)    private String title;    @Value(&quot;${com.neo.description}&quot;)    private String description;    }</code></pre><h3><span id="44-log配置">4.4 log配置</span></h3><pre><code>logging.path=/user/local/loglogging.level.com.favorites=DEBUGlogging.level.org.springframework.web=INFOlogging.level.org.hibernate=ERROR</code></pre><p>path 为本机的 log 地址，<code>logging.level</code> 后面可以根据包路径配置不同资源的 log 级别</p><h3><span id="45-数据库操作">4.5 数据库操作</span></h3><p>添加jar包（第一个暂时没添加上）</p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-data-jpa&lt;/artifactId&gt;&lt;/dependency&gt; &lt;dependency&gt;    &lt;groupId&gt;mysql&lt;/groupId&gt;    &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;&lt;/dependency&gt;</code></pre>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> springboot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>微服务</title>
      <link href="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/"/>
      <url>/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-需求">1 需求</span></h2><p>a)调研微服务框架，形成调研报告。重点包括：</p><ul><li>框架核心思想</li><li>适用场景</li><li>主要实现方案及特点</li><li>已有使用微服务的案例情况</li></ul><p>b)搭建微服务框架环境，形成基于框架开发指导手册。</p><p>c)微服务框架环境支撑能力评估，形成支持数量、应用运行性能、扩展能力等评估报告。</p><h2><span id="2-传统微服务架构">2 传统微服务架构</span></h2><h3><span id="21-单体应用架构">2.1 单体应用架构</span></h3><p>分层架构（逻辑上的分层，而非物理上的分层）</p><ul><li><p>所有的代码最终还是运行在同一个进程中。而这就是所谓的单块架构</p><ul><li>表示层、业务逻辑层和数据访问层，层与层之间互相连接、互相协作，构成整体<ul><li>表示层： 用户使用应用程序时，看到的、听见的、输入的或者交互的部分。</li><li>业务逻辑层： 根据用户输入的信息，进行逻辑计算或者业务处理的部分。</li><li>数据访问层： 关注有效地操作原始数据的部分，如将数据存储到存储介质（如数据库、文件系统）及从存储介质中读取数据等。</li></ul></li><li>优点：易于开发；易于测试；易于部署；易于水平伸缩（发布到新服务器节点） </li><li>缺点：维护成本大（bug连串，不容易协作）；交付周期长；新人上手慢；技术选型引入成本高；可拓展性差</li></ul></li></ul><p>通俗的来讲，all in one 是将一个应用中的所有应用服务都封装在一个应用中。<br>例如，数据库访问，web访问等等各个功能放在同一个war包里。</p><h3><span id="22-集群结构">2.2 集群结构</span></h3><p>单机处理到达瓶颈的时候，就把单机复制几份，这样就构成了一个“集群”。</p><p>集群中每台服务器就叫做这个集群的一个“节点”，所有节点构成了一个集群。</p><p>每个节点都提供相同的服务，那么这样系统的处理能力就相当于提升了好几倍。</p><p>用户的请求究竟由哪个节点来处理？负载均衡服务器来分配</p><h3><span id="23-微服务架构">2.3 微服务架构</span></h3><p>微服务就是将一个完整的系统，按照业务功能，拆分成一个个独立的子系统，在微服务结构中，每个子系统就被称为“服务”。这些子系统能够独立运行在web容器中，它们之间通过RPC方式通信。</p><ul><li><p>是一种开发软件的架构模式，相比整体式架构，其提倡将单一的应用程序划分成一组小的服务</p></li><li><ul><li>将应用程序构建为独立的组件，并将每个应用程序进程作为一项服务运行</li></ul></li><li><p>微服务的特性</p><ul><li><p>单一职责：高内聚、低耦合，每个服务是单一职责原则的单元，不同的服务进行组合构建系统。</p></li><li><p>轻量级通信：轻量级是指与语言和平台无关的交互（格式一般是XML，JSON这种，协议基于HTTP）</p></li><li><p>独立：可独立开发测试和部署<br><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/clipboard-1602221847948.png" alt="img" style="zoom:50%;"><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/clipboard-1602221853533.png" alt="img" style="zoom:50%;"></p></li></ul></li></ul><ul><li>进程隔离：单块架构整个系统运行在同一进程中，部署时需操作整个应用，无法独立部署；微服务中由多个服务组成，每个服务可以独立运行在不同进程中。<br><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/clipboard-1602221835327.png" alt="img" style="zoom:50%;"><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/clipboard.png" alt="img" style="zoom:70%;"></li></ul><h3><span id="23-开源框架">2.3 开源框架</span></h3><p>当前的微服务框架有很多，但是最多的主要是两种：</p><p> <strong><a href="https://link.zhihu.com/?target=https%3A//www.oschina.net/p/spring-cloud">Spring Cloud</a></strong></p><p>Spring Cloud 为开发者提供了分布式系统配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性 Token、全局锁、决策竞选、分布式会话与集群状态等的开发工具。使用 Spring Cloud 开发者可以快速实现上述这些模式。</p><pre><code>在基于Spring Cloud进行微服务开发时，在项目中就会通过引入“spring-cloud-starter-parent”父依赖来实现其他框架及组件的快速引入。Spring Cloud集成并封装netfix中的ribbon，eureka，hystrix，feign和zull等十分出色的项目，为我们提供了服务注册与发现，客户端的负载均衡和路由分发等功能。Spring Cloud除了一些核心的项目外，还有很多实现特定功能的组件框架，如Sleuth、Turbine等。基础功能：服务治理： Spring Cloud Eureka客户端负载均衡： Spring Cloud Ribbon服务容错保护： Spring Cloud Hystrix声明式服务调用： Spring Cloud FeignAPI网关服务：Spring Cloud Zuul分布式配置中心： Spring Cloud Config高级功能：消息总线： Spring Cloud Bus消息驱动的微服务： Spring Cloud Stream分布式服务跟踪： Spring Cloud Sleuth</code></pre><p> <strong><a href="https://link.zhihu.com/?target=https%3A//www.oschina.net/p/dubbo">Dubbo</a></strong></p><p>Dubbo 是阿里开源的一款高性能 RPC 框架，特性包括基于透明接口的 RPC、智能负载均衡、自动服务注册和发现、可扩展性高、运行时流量路由与可视化的服务治理。</p><pre><code>Dubbo是一套微服务系统的协调者，在它这套体系中，一共有三种角色，分别是：服务提供者（下面简称提供者）、服务消费者（下面简称消费者）、注册中心。你在使用的时候需要将Dubbo的jar包引入到你的项目中，也就是每个服务都要引入Dubbo的jar包。然后当这些服务初始化的时候，Dubbo就会将当前系统需要发布的服务、以及当前系统的IP和端口号发送给注册中心，注册中心便会将其记录下来。这就是服务发布的过程。与此同时，也是在系统初始化的时候，Dubbo还会扫描一下当前系统所需要引用的服务，然后向注册中心请求这些服务所在的IP和端口号。接下来系统就可以正常运行了。当系统A需要调用系统B的服务的时候，A就会与B建立起一条RPC信道，然后再调用B系统上相应的服务。</code></pre><h3><span id="24-框架对比和选型">2.4 框架对比和选型</span></h3><ul><li>从社区活跃度和功能完整度，再对照业务需求和团队状况进行选型。</li></ul><p>从整体架构上来看：二者模式接近，都需要服务提供方，注册中心，服务消费方。</p><p>从核心要素来看：Spring Cloud 更胜一筹，在开发过程中只要整合 Spring Cloud 的子项目就可以顺利的完成各种组件的融合，而 Dubbo 却需要通过实现各种 Filter 来做定制，开发成本以及技术难度略高。</p><p>通信协议和性能：Dubbo 支持各种通信协议，而且消费方和服务方使用长链接方式交互，通信速度上略胜 Spring Cloud，如果对于系统的响应时间有严格要求，长链接更合适。</p><p>服务依赖：Dubbo 服务依赖略重，需要有完善的版本管理机制，但是程序入侵少。</p><p>组件运行：业务部署方式相同，都需要前置一个网关来隔绝外部直接调用原子服务的风险。</p><p>可能的选择：</p><ol><li>基于SpringBoot快速开发、基于<strong>Dubbo</strong>的微服务化、基于Docker的容器化部署、基于Jenkins的自动化构建、用docker-compose进行服务编排</li><li>基于SpringBoot快速开发、基于<strong>Spring Cloud</strong>的微服务化、基于Docker的容器化部署、基于Jenkins的自动化构建、用docker-compose进行服务编排</li></ol><p>服务编排：</p><pre><code>在微服务系统的架构中，所有的服务都是无状态的。容器编排就是针对这些无状态的服务进行的一系列操作，什么操作呢：弹性伸缩、自动扩容、健康检查、服务发现、负载均衡、自动恢复、滚动升级等等。通俗的说，你通过编写配置文件告诉容器编排系统，我要user这个微服务初始化启动3个节点。当tps大于2000的时候自动扩容，最大可以扩容到10个节点，这个服务最大可用内存、cpu是多少等等，容器编排系统完成你需求的过程就是编排</code></pre><h2><span id="3-k8s">3 K8S</span></h2><h3><span id="31-docker">3.1 Docker</span></h3><p>Docker 的意义</p><ul><li>统一了基础设施环境（硬件、操作系统版本、运行时环境）</li><li>统一了程序打包（装箱）的方式：所有语言的程序都全部变 docker镜像</li><li>统一了部署方式 docker容器：docker run</li></ul><p>单机使用docker的缺点：</p><ul><li>无法有效集群</li><li>容器数量上升的同时，管理成本上升</li><li>没有容灾机制</li><li>没有编排模板，无法大规模快速调度</li><li>没有统一的配置管理中心工具</li><li>没有容器生命周期的管理工具</li><li>没有图形化运维管理工具</li></ul><p>Docker支持任何语言的镜像</p><p>Docker与宿主机进行隔离，使用平台的可移植性</p><p>当Docker服务变得越来越多之后如何进行更好的管理？：实现对Docker管理——Kubernetes</p><h3><span id="32-k8s">3.2 K8S</span></h3><p>基于Docker容器引擎的开源容器编排工具：K8S</p><p>Kubernetes，就是基于Docker的集群管理平台，它的全称，是kubernetes。</p><p>一个K8S系统，通常称为一个K8S集群（Cluster）</p><p>这个集群主要包括两个部分：一个Master节点（主节点）、一群Node节点（计算节点）</p><p>利用 K8S，能够达成以下目标：</p><ul><li>跨多台主机进行容器编排。</li><li>更加充分地利用硬件，最大程度获取运行企业应用所需的资源。</li><li>有效管控应用部署和更新，并实现自动化操作。</li><li>挂载和增加存储，用于运行有状态的应用。</li><li>快速、按需扩展容器化应用及其资源。</li><li>对服务进行声明式管理，保证所部署的应用始终按照部署的方式运行。</li><li>利用自动布局、自动重启、自动复制以及自动扩展功能，对应用实施状况检查和自我修复。</li></ul><h2><span id="4-如何选型">4 如何选型</span></h2><p><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/20190716135704386.jpg" alt="Dubbo、Spring Cloud和K8S横向对比"><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/20190716135737799.jpg" alt="Dubbo、Spring Cloud和K8S横向对比"></p><p><img src="/2020/10/09/gong-cheng-bu-shu-xiang-guan/wei-fu-wu/20190716135755655.jpg" alt="Dubbo、Spring Cloud和K8S优劣比对"></p><div class="table-container"><table><thead><tr><th><strong>服务框架</strong></th><th><strong>生态</strong></th><th><strong>开发复杂度</strong></th><th><strong>运维复杂度</strong></th><th><strong>完成度</strong></th></tr></thead><tbody><tr><td>Dubbo</td><td>一般</td><td>简单</td><td>简单</td><td>开发中，不成熟</td></tr><tr><td>Spring  Cloud</td><td>丰富</td><td>简单</td><td>复杂</td><td>正式版，成熟</td></tr><tr><td>Kubernetes</td><td>非常丰富</td><td>简单</td><td>非常复杂</td><td>趋于成熟</td></tr></tbody></table></div><p>没有深入使用来对比就有一点点片面，目前综合多方面的调查和现有的对比评测，总结如下：</p><ul><li>Dubbo 和 SpringCloud<ul><li>Dubbo的关注点在于服务治理，并不能算是一个真正的微服务框架，不能完整覆盖微服务的各项功能需求。</li><li>Spring Cloud通过集成各种组件的方式来实现微服务，因此理论上可以集成目前业内的绝大多数的微服务相关组件，从而实现微服务的全部功能。</li><li>对Dubbo而言，如果一定要应用到微服务的使用场景中的话，上表中欠缺的大多数功能都可以通过集成第三方应用和组件的方式来实现，跟Spring Cloud相比主要的缺陷在于集成过程中的便利性和兼容性等问题。</li></ul></li><li>SpringCloud 和 K8S<ul><li>Spring Cloud是一个基于Java语言的微服务开发框架，更多的是面向有Spring开发经验的Java语言开发者</li><li>Kubernetes是一个针对容器应用的自动化部署、伸缩和管理的开源系统，它兼容多种语言且提供了创建、运行、伸缩以及管理分布式系统的原语。它不是一个针对开发者的平台，它的本质是DevOps（Development and Operations）工具。</li></ul></li><li>传统微服务架构 和 K8S<ul><li>传统微服务架构需要开发者在自己的应用中集成微服务框架 SDK，从而具有服务治理相关功能</li><li>Kubernetes 使用 kube-dns 以及 kube-proxy 配合 service 的概念支持了服务的注册与发现，才有了可以在 Kubernetes 上构建微服务的基础。实际生产中，需要对流量进行更精细的掌控，Service Mesh工具（例如istio） 可以被使用。</li><li>Service Mesh工具 在对K8S 有更深入的了解之后再去熟悉吧</li></ul></li></ul><p>目前的结论</p><ul><li><p>选型：Spring Boot + Docker + K8S</p><ul><li>全面覆盖微服务关注点</li><li>语言栈无关，非 to JAVA</li><li>社区活跃，不过时的资源多</li></ul></li><li><p>使用Spring Boot提供应用的打包，Docker和Kubernetes提供应用的部署和调度。</p></li></ul><h2><span id="reference">Reference</span></h2><p><a href="https://www.zhihu.com/question/65502802" target="_blank" rel="noopener">https://www.zhihu.com/question/65502802</a></p><p><a href="https://www.zhihu.com/question/65502802/answer/1371300276" target="_blank" rel="noopener">https://www.zhihu.com/question/65502802/answer/1371300276</a></p><p><a href="https://www.zhihu.com/question/45413135" target="_blank" rel="noopener">https://www.zhihu.com/question/45413135</a></p><p><a href="https://zhuanlan.zhihu.com/p/33296468" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/33296468</a></p><p><a href="https://blog.csdn.net/huanglitao0912/article/details/82314123" target="_blank" rel="noopener">https://blog.csdn.net/huanglitao0912/article/details/82314123</a></p><p><a href="https://www.zhihu.com/question/283286745/answer/763040709" target="_blank" rel="noopener">https://www.zhihu.com/question/283286745/answer/763040709</a></p><p><a href="https://zhuanlan.zhihu.com/p/53260098" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/53260098</a></p><p><a href="https://blog.csdn.net/qq_30364013/article/details/78148016" target="_blank" rel="noopener">https://blog.csdn.net/qq_30364013/article/details/78148016</a></p><p><a href="https://blog.csdn.net/weixin_43834662/article/details/96131720" target="_blank" rel="noopener">https://blog.csdn.net/weixin_43834662/article/details/96131720</a></p><p><a href="https://www.cnblogs.com/doit8791/p/9979193.html" target="_blank" rel="noopener">https://www.cnblogs.com/doit8791/p/9979193.html</a></p><p>多组件通用HTTP框架演进_最终版-孙海洲</p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 微服务 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二分类多分类多标签分类的评估指标计算与实现</title>
      <link href="/2020/09/12/shen-du-xue-xi-yu-nlp-ji-chu/er-fen-lei-duo-fen-lei-duo-biao-qian-fen-lei-de-ping-gu-zhi-biao-ji-suan-yu-shi-xian/"/>
      <url>/2020/09/12/shen-du-xue-xi-yu-nlp-ji-chu/er-fen-lei-duo-fen-lei-duo-biao-qian-fen-lei-de-ping-gu-zhi-biao-ji-suan-yu-shi-xian/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-二分类">1 二分类</span></h2><h3><span id="11-二分类例子">1.1 二分类例子</span></h3><pre><code>reference_list =  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]prediciton_list = [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]</code></pre><h3><span id="12-指标计算">1.2 指标计算</span></h3><div class="table-container"><table><thead><tr><th style="text-align:center">预测 ↓       真实  →</th><th style="text-align:center">True</th><th style="text-align:center">False</th></tr></thead><tbody><tr><td style="text-align:center"><strong>True</strong></td><td style="text-align:center">3（TP）</td><td style="text-align:center">3（FP）</td></tr><tr><td style="text-align:center"><strong>False</strong></td><td style="text-align:center">2（FN）</td><td style="text-align:center">2（TN）</td></tr></tbody></table></div><pre><code>micro_accuracy = 5/10 = 0.5micro_precision = TP/(TP+FP) = 3/6 = 0.5micro_recall = TP/(TP+FN) = 3/5 = 0.6micro_f1 = 2 * P * R / (P + R) = 0.545</code></pre><h3><span id="13-sklearn实现">1.3 sklearn实现</span></h3><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_scoredef classification2(reference_list, prediciton_list):    micro_accuracy = accuracy_score(reference_list, prediciton_list)    micro_precision = precision_score(reference_list, prediciton_list)    micro_recall = recall_score(reference_list, prediciton_list)    micro_f1 = f1_score(reference_list, prediciton_list)    return micro_accuracy, micro_precision, micro_recall, micro_f1reference_list = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]prediciton_list = [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]print(classification2(reference_list, prediciton_list))  # (0.5, 0.5, 0.6, 0.5454545454545454)</code></pre><h3><span id="14-python实现">1.4 python实现</span></h3><pre class=" language-lang-python"><code class="language-lang-python">def evaluate_2(y_true, y_pred):    tp = sum(1 for a, b in zip(y_true, y_pred) if a == 1 and b == 1)    fp = sum(1 for a, b in zip(y_true, y_pred) if a == 0 and b == 1)    fn = sum(1 for a, b in zip(y_true, y_pred) if a == 1 and b == 0)    # tp = ((y_true==1) & (y_pred==1)).sum()    # fp = ((y_true==0) & (y_pred==1)).sum()    # fn = ((y_true==1) & (y_pred==0)).sum()    if tp == 0:        return 0.0    precision = tp / (tp + fp)    recall = tp / (tp+fn)    f1 = 2 * (precision * recall) / (precision + recall)    return precision, recall, f1reference_list = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]prediciton_list = [0, 0, 1, 1, 1, 0, 0, 1, 1, 1]print(evaluate_2(reference_list, prediciton_list))  # (0.5, 0.6, 0.5454545454545454)</code></pre><h2><span id="2-n分类">2 N分类</span></h2><h3><span id="21-三分类例子">2.1 三分类例子</span></h3><pre><code>真实标签与预测值（10条数据，3个分类）reference_list = [1, 1, 2, 2, 2, 3, 3, 3, 3, 3]prediciton_list = [1, 2, 2, 2, 3, 1, 2, 3, 3, 3]</code></pre><h3><span id="22-micro系列指标计算">2.2 micro系列指标计算</span></h3><p>总体来说，就是求得整体的TP、FP、FN值，得到指标结果。</p><p>分别计算各个类别的TP、FP、FN值，相加后得到所有的TP、FP、FN值：</p><pre><code>分类1:reference_list = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]prediciton_list = [1, 0, 0, 0, 0, 1, 0, 0, 0, 0]TP = 1  FP = 1 FN = 1分类2:reference_list =  [0, 0, 1, 1, 1, 0, 0, 0, 0, 0]prediciton_list = [0, 1, 1, 1, 0, 0, 1, 0, 0, 0]TP = 2  FP = 2 FN = 1分类3:reference_list =  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]prediciton_list = [0, 0, 0, 0, 1, 0, 0, 1, 1, 1]TP = 3  FP = 1 FN = 2相加：TP = 6  FP = 4 FN = 4</code></pre><p>要不就，列一个表格直接瞅：</p><div class="table-container"><table><thead><tr><th style="text-align:center">预测 ↓  真实  →</th><th style="text-align:center">1</th><th style="text-align:center">2</th><th style="text-align:center">3</th></tr></thead><tbody><tr><td style="text-align:center">1</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">1</td><td style="text-align:center">2</td><td style="text-align:center">1</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">0</td><td style="text-align:center">1</td><td style="text-align:center">3</td></tr></tbody></table></div><pre><code>TP = 1 + 2 + 3 =6FP = (0+1) + (1+1) + (0+1) = 4FN = (1+0) + (0+1) + (1+1) = 4micro_precision = TP/(TP+FP) = 6/10 = 0.6micro_recall = TP/(TP+FN) = 6/10 = 0.6micro_f1 = 2 * P * R / (P + R) = 0.6</code></pre><h3><span id="23-macro系列指标计算">2.3 macro系列指标计算</span></h3><p>总体来说，就是计算所有类别的TP、FP、FN值，进而计算各个类别的指标，再对每个类别的指标平均</p><pre><code>分类1:reference_list = [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]prediciton_list = [1, 0, 0, 0, 0, 1, 0, 0, 0, 0]macro_precision_1 = TP/(TP+FP) = 1/2macro_recall_1 = TP/(TP+FN) = 1/2 macro_f1_1 = 2 * P * R / (P + R) = 2/3分类2:reference_list =  [0, 0, 1, 1, 1, 0, 0, 0, 0, 0]prediciton_list = [0, 1, 1, 1, 0, 0, 1, 0, 0, 0]macro_precision_2 = TP/(TP+FP) = 2/4 = 1/2macro_recall_2 = TP/(TP+FN) = 2/3macro_f1_2 = 2 * P * R / (P + R) = 4/5分类3:reference_list =  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]prediciton_list = [0, 0, 0, 0, 1, 0, 0, 1, 1, 1]macro_precision_3 = TP/(TP+FP) = 3/4macro_recall_3 = TP/(TP+FN) = 3/5macro_f1_3 = 2 * P * R / (P + R) = 18/20   /  27/20 = 2/3最终macro_precision = (macro_precision_1 + macro_precision_2 + macro_precision_3)/3 = 0.5833macro_recall = ( + + )/3 = 0.5889macro_f1 = ( + + )/3 = 0.5794</code></pre><h3><span id="24-weighted系列指标计算">2.4 weighted系列指标计算</span></h3><p>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.</p><pre><code>reference_list = [1, 1, 2, 2, 2, 3, 3, 3, 3, 3]类别123的样本数：2 3 5分类1:macro_precision_1 = TP/(TP+FP) = 1/2macro_recall_1 = TP/(TP+FN) = 1/2 macro_f1_1 = 2 * P * R / (P + R) = 1/2分类2:macro_precision_2 = TP/(TP+FP) = 2/4 = 1/2macro_recall_2 = TP/(TP+FN) = 2/3macro_f1_2 = 2 * P * R / (P + R) = 4/7分类3:macro_precision_3 = TP/(TP+FP) = 3/4macro_recall_3 = TP/(TP+FN) = 3/5macro_f1_3 = 2 * P * R / (P + R) = 18/20   /  27/20 = 2/3最终：macro_precision = (macro_precision_1 *0.2  + macro_precision_2*0.3 + macro_precision_3*0.5) = 0.625macro_recall = (macro_recall_1 *0.2  + macro_recall_2*0.3 + macro_recall_3*0.5) = 0.6macro_f1 = (macro_f1_1 *0.2  + macro_f1_2*0.3 + macro_f1_3*0.5)  = 0.6047619047619047</code></pre><h3><span id="25-sklearn实现">2.5 sklearn实现</span></h3><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_scoredef classificationN(reference_list, prediciton_list):    micro_accuracy = accuracy_score(reference_list, prediciton_list)    micro_precision = precision_score(reference_list, prediciton_list, average="micro")    micro_recall = recall_score(reference_list, prediciton_list, average="micro")    micro_f1 = f1_score(reference_list, prediciton_list, average="micro")    macro_accuracy = accuracy_score(reference_list, prediciton_list)    macro_precision = precision_score(reference_list, prediciton_list, average="macro")    macro_recall = recall_score(reference_list, prediciton_list, average="macro")    macro_f1 = f1_score(reference_list, prediciton_list, average="macro")    weighted_accuracy = accuracy_score(reference_list, prediciton_list)    weighted_precision = precision_score(reference_list, prediciton_list, average="weighted")    weighted_recall = recall_score(reference_list, prediciton_list, average="weighted")    weighted_f1 = f1_score(reference_list, prediciton_list, average="weighted")    return (micro_accuracy, micro_precision, micro_recall, micro_f1), (macro_accuracy, macro_precision, macro_recall, macro_f1), (weighted_accuracy, weighted_precision, weighted_recall, weighted_f1)reference_list = [1, 1, 2, 2, 2, 3, 3, 3, 3, 3]prediciton_list = [1, 2, 2, 2, 3, 1, 2, 3, 3, 3]print(classificationN(reference_list, prediciton_list))# ((0.6, 0.6, 0.6, 0.6), (0.6, 0.5833333333333334, 0.5888888888888889, 0.5793650793650794), (0.6, 0.625, 0.6, 0.6047619047619047))</code></pre><h3><span id="26-python实现">2.6 python实现</span></h3><pre class=" language-lang-python"><code class="language-lang-python">def evaluate_N(y_true, y_pred, N, average=None):    tp_list,fp_list, fn_list = [0 for i in range(N)],[0 for i in range(N)],[0 for i in range(N)]    for i in range(1, N+1):        y_true_tmp = [1 if j==i else 0 for j in y_true]        y_pred_tmp = [1 if j==i else 0 for j in y_pred]        # tp, fp, fn = count_tp_fp_fn(y_true_tmp, y_pred_tmp)        tp = sum(1 for a, b in zip(y_true_tmp, y_pred_tmp) if a == 1 and b == 1)        fp = sum(1 for a, b in zip(y_true_tmp, y_pred_tmp) if a == 0 and b == 1)        fn = sum(1 for a, b in zip(y_true_tmp, y_pred_tmp) if a == 1 and b == 0)        tp_list[i-1]=tp        fp_list[i-1]=fp        fn_list[i-1]=fn    if average == 'micro':        tp = sum(tp_list)        fp = sum(fp_list)        fn = sum(fn_list)        precision = tp / (tp + fp)        recall = tp / (tp + fn)        if tp == 0:            f1 = 0.0        else:            f1 = 2 * (precision * recall) / (precision + recall)        return precision, recall, f1    elif average == 'macro':        precision_list, recall_list, f1_list = [0 for i in range(N)],[0 for i in range(N)],[0 for i in range(N)]        for i in range(1, N+1):            precision_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fp_list[i-1] )            recall_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fn_list[i-1] )            if (precision_list[i-1] + recall_list[i-1]) == 0:                f1_list[i-1] = 0.0            else:                f1_list[i-1] = 2 * (precision_list[i-1] * recall_list[i-1]) / (precision_list[i-1] + recall_list[i-1])        return sum(precision_list) / N, sum(recall_list) / N, sum(f1_list) / N    elif average == 'weighted':        precision_list, recall_list, f1_list = [0 for i in range(N)],[0 for i in range(N)],[0 for i in range(N)]        num_list = [0 for i in range(N)]        for i in range(1, N+1):            precision_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fp_list[i-1] )            recall_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fn_list[i-1] )            if (precision_list[i-1] + recall_list[i-1]) == 0:                f1_list[i-1] = 0.0            else:                f1_list[i-1] = 2 * (precision_list[i-1] * recall_list[i-1]) / (precision_list[i-1] + recall_list[i-1])            num_list[i-1] = sum(1 for a in y_true if a == i)        assert sum(num_list) == len(y_true) == len(y_pred)        percent_list = [a/len(y_true) for a in num_list]        func = lambda x, y: x * y        return sum(map(func, precision_list, percent_list)), sum(map(func, recall_list, percent_list)), sum(map(func, f1_list, percent_list))    else:        print('wrong average !')        exit()reference_list = [1, 1, 2, 2, 2, 3, 3, 3, 3, 3]prediciton_list = [1, 2, 2, 2, 3, 1, 2, 3, 3, 3]print(evaluate_N(reference_list, prediciton_list, 3,average='micro'))print(evaluate_N(reference_list, prediciton_list, 3,average='macro'))print(evaluate_N(reference_list, prediciton_list, 3,average='weighted'))# (0.6, 0.6, 0.6)# (0.5833333333333334, 0.5888888888888889, 0.5793650793650794)# (0.625, 0.6, 0.6047619047619047)</code></pre><h2><span id="3-多标签分类">3 多标签分类</span></h2><h3><span id="31-三标签分类例子">3.1 三标签分类例子</span></h3><pre><code>真实标签与预测值（5条数据，3个分类）reference_list =  [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1]]prediciton_list = [[1,0,0],[1,0,0],[1,1,1],[1,0,0],[0,1,1]]</code></pre><h3><span id="32-micro系列指标计算">3.2 micro系列指标计算</span></h3><p>分别计算各个标签的TP、FP、FN值，相加后得到所有的TP、FP、FN值：</p><pre><code>分类1:reference_list =  [1, 0, 0, 1, 1]prediciton_list = [1, 1, 1, 1, 0]TP = 2  FP = 2 FN = 1分类2:reference_list =  [0, 1, 0, 1, 0]prediciton_list = [0, 0, 1, 0, 1]TP = 0  FP = 2 FN = 2分类3:reference_list =  [0, 0, 1, 0, 1]prediciton_list = [0, 0, 1, 0, 1]TP = 2  FP = 0 FN = 0相加：TP = 4  FP = 4 FN = 3</code></pre><pre><code>micro_precision = TP/(TP+FP) = 4/8 = 0.5micro_recall = TP/(TP+FN) = 4/7 = 0.5714micro_f1 = 2 * P * R / (P + R) = 0.5333</code></pre><h3><span id="33-macro系列指标计算">3.3 macro系列指标计算</span></h3><p>总体来说，就是计算所有类别的TP、FP、FN值，进而计算各个类别的指标，再对每个类别的指标平均</p><pre><code>分类1:reference_list =  [1, 0, 0, 1, 1]prediciton_list = [1, 1, 1, 1, 0]TP = 2  FP = 2 FN = 1macro_precision_1 = TP/(TP+FP) = 1/2macro_recall_1 = TP/(TP+FN) = 2/3macro_f1_1 = 2 * P * R / (P + R) = 4/7分类2:reference_list =  [0, 1, 0, 1, 0]prediciton_list = [0, 0, 1, 0, 1]TP = 0  FP = 2 FN = 2macro_precision_2 = TP/(TP+FP) = 0macro_recall_2 = TP/(TP+FN) = 0macro_f1_2 = 2 * P * R / (P + R) = 0分类3:reference_list =  [0, 0, 1, 0, 1]prediciton_list = [0, 0, 1, 0, 1]TP = 2  FP = 0 FN = 0macro_precision_3 = TP/(TP+FP) = 1macro_recall_3 = TP/(TP+FN) = 1macro_f1_3 = 2 * P * R / (P + R) = 1最终macro_precision = (macro_precision_1 + macro_precision_2 + macro_precision_3)/3 = 0.5macro_recall = ( + + )/3 = 5/9 = 0.5556macro_f1 = ( + + )/3 = 11/21 = 0.5238</code></pre><h3><span id="34-weighted系列指标计算">3.4 weighted系列指标计算</span></h3><p>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.</p><pre><code>reference_list =  [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1]]reference_list_1 =  [1, 0, 0, 1, 1]reference_list_2 =  [0, 1, 0, 1, 0]reference_list_3 =  [0, 0, 1, 0, 1]类别123的样本数：3 2 2分类1:macro_precision_1 = TP/(TP+FP) = 1/2macro_recall_1 = TP/(TP+FN) = 2/3macro_f1_1 = 2 * P * R / (P + R) = 4/7分类2:macro_precision_2 = TP/(TP+FP) = 0macro_recall_2 = TP/(TP+FN) = 0macro_f1_2 = 2 * P * R / (P + R) = 0分类3:macro_precision_3 = TP/(TP+FP) = 1macro_recall_3 = TP/(TP+FN) = 1macro_f1_3 = 2 * P * R / (P + R) = 1最终：macro_precision = (macro_precision_1 *3/7  + macro_precision_2*2/7 + macro_precision_3*2/7) = 7/14 = 0.5macro_recall = (macro_recall_1 *3/7   + macro_recall_2*2/7 + macro_recall_3*2/7) = 12/21 = 4/7 = 0.5714macro_f1 = (macro_f1_1  *3/7  + macro_f1_2*2/7 + macro_f1_3*2/7)  = 26/49 = 0.5306</code></pre><h3><span id="35-sklearn实现">3.5 sklearn实现</span></h3><pre class=" language-lang-python"><code class="language-lang-python">from sklearn.metrics import accuracy_score,f1_score,recall_score,precision_scorefrom sklearn.metrics import classification_reportdef classificationM(reference_list, prediciton_list):    print(classification_report(reference_list, prediciton_list))    f1_score(reference_list, prediciton_list, average='micro')    micro_accuracy = accuracy_score(reference_list, prediciton_list)    micro_precision = precision_score(reference_list, prediciton_list, average="micro")    micro_recall = recall_score(reference_list, prediciton_list, average="micro")    micro_f1 = f1_score(reference_list, prediciton_list, average="micro")    macro_accuracy = accuracy_score(reference_list, prediciton_list)    macro_precision = precision_score(reference_list, prediciton_list, average="macro")    macro_recall = recall_score(reference_list, prediciton_list, average="macro")    macro_f1 = f1_score(reference_list, prediciton_list, average="macro")    weighted_accuracy = accuracy_score(reference_list, prediciton_list)    weighted_precision = precision_score(reference_list, prediciton_list, average="weighted")    weighted_recall = recall_score(reference_list, prediciton_list, average="weighted")    weighted_f1 = f1_score(reference_list, prediciton_list, average="weighted")    return (micro_accuracy, micro_precision, micro_recall, micro_f1), (macro_accuracy, macro_precision, macro_recall, macro_f1), (weighted_accuracy, weighted_precision, weighted_recall, weighted_f1)reference_list = [[1,0,0],[0,1,0],[0,0,1],[1,1,0],[1,0,1]]prediciton_list = [[1,0,0],[1,0,0],[1,1,1],[1,0,0],[0,1,1]]print(classificationN(reference_list, prediciton_list))# ((0.2, 0.5, 0.5714285714285714, 0.5333333333333333), (0.2, 0.5, 0.5555555555555555, 0.5238095238095238), (0.2, 0.5, 0.5714285714285714, 0.5306122448979592))</code></pre><h3><span id="36-python实现">3.6 python实现</span></h3><pre class=" language-lang-python"><code class="language-lang-python">def evaluate_Multi(y_true, y_pred, N, average=None):    # reference_list = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]]    # prediciton_list = [[1, 0, 0], [1, 0, 0], [1, 1, 1], [1, 0, 0], [0, 1, 1]]    tp_list,fp_list, fn_list = [0 for i in range(N)],[0 for i in range(N)],[0 for i in range(N)]    for i in range(1, N+1):        y_true_tmp = [1 if j[i-1]==1 else 0 for j in y_true]        y_pred_tmp = [1 if j[i-1]==1 else 0 for j in y_pred]        # print("y_true_tmp: ",y_true_tmp)        # print("y_pred_tmp: ",y_pred_tmp)        tp = sum(1 for a, b in zip(y_true_tmp, y_pred_tmp) if a == 1 and b == 1)        fp = sum(1 for a, b in zip(y_true_tmp, y_pred_tmp) if a == 0 and b == 1)        fn = sum(1 for a, b in zip(y_true_tmp, y_pred_tmp) if a == 1 and b == 0)        tp_list[i-1]=tp        fp_list[i-1]=fp        fn_list[i-1]=fn    if average == 'micro':        tp = sum(tp_list)        fp = sum(fp_list)        fn = sum(fn_list)        if tp ==0:            return 0.0, 0.0, 0.0        precision = tp / (tp + fp)        recall = tp / (tp + fn)        if (precision + recall)== 0:            f1 = 0.0        else:            f1 = 2 * (precision * recall) / (precision + recall)        return precision, recall, f1    elif average == 'macro':        precision_list, recall_list, f1_list = [0 for i in range(N)],[0 for i in range(N)],[0 for i in range(N)]        for i in range(1, N+1):            precision_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fp_list[i-1] )            recall_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fn_list[i-1] )            if (precision_list[i-1] + recall_list[i-1]) == 0:                f1_list[i-1] = 0.0            else:                f1_list[i-1] = 2 * (precision_list[i-1] * recall_list[i-1]) / (precision_list[i-1] + recall_list[i-1])        return sum(precision_list) / N, sum(recall_list) / N, sum(f1_list) / N    elif average == 'weighted':        precision_list, recall_list, f1_list = [0 for i in range(N)],[0 for i in range(N)],[0 for i in range(N)]        num_list = [0 for i in range(N)]        for i in range(1, N+1):            precision_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fp_list[i-1] )            recall_list[i-1] = tp_list[i-1] / ( tp_list[i-1] + fn_list[i-1] )            if (precision_list[i-1] + recall_list[i-1]) == 0:                f1_list[i-1] = 0.0            else:                f1_list[i-1] = 2 * (precision_list[i-1] * recall_list[i-1]) / (precision_list[i-1] + recall_list[i-1])            # print('y_true: ',y_true)            num_list[i-1] = sum(1 for a in y_true if a[i-1] ==1)        # assert sum(num_list) == len(y_true) == len(y_pred)        # print('num_list: ', num_list)        percent_list = [a/sum(num_list) for a in num_list]        func = lambda x, y: x * y        return sum(map(func, precision_list, percent_list)), sum(map(func, recall_list, percent_list)), sum(map(func, f1_list, percent_list))    else:        print('wrong average !')        exit()reference_list = [[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [1, 0, 1]]prediciton_list = [[1, 0, 0], [1, 0, 0], [1, 1, 1], [1, 0, 0], [0, 1, 1]]print(evaluate_Multi(reference_list, prediciton_list, 3, average='micro'))print(evaluate_Multi(reference_list, prediciton_list, 3, average='macro'))print(evaluate_Multi(reference_list, prediciton_list, 3, average='weighted'))        (0.5, 0.5714285714285714, 0.5333333333333333)(0.5, 0.5555555555555555, 0.5238095238095238)(0.5, 0.5714285714285714, 0.5306122448979592)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 二分类多分类多标签分类的评估指标计算与实现 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP中的对偶学习</title>
      <link href="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/"/>
      <url>/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-介绍">1 介绍</span></h2><h3><span id="11-2016-nips-dual-learning-for-machine-translation">1.1 2016-NIPS-Dual Learning for Machine Translation</span></h3><p>神经机器翻译（NMT）的训练需要数以千万计的双语句子对。为了解决此培训数据瓶颈，开发了一种双重学习机制，该机制可使NMT系统通过双重学习自动从未标记的数据中学习。 </p><p>这种机制是受以下观察启发的：任何机器翻译任务都有双重任务，例如，英语到法语翻译（主要）与法语到英语翻译（双重）； 即使没有人工标记，原始任务和双重任务也可以形成一个闭环，并生成信息反馈信号来训练翻译模型。 在双重学习机制中，我们使用一个代理来代表主要任务的模型，并使用另一个代理来代表双重任务的模型，然后要求他们通过强化学习过程互相教导。</p><h3><span id="12-2017-icml-dual-supervised-learning">1.2 2017-ICML-Dual supervised learning</span></h3><p>许多有监督学习任务以双重形式出现，是对偶的。例如，英语到法语的翻译与法语到英语的翻译，语音识别与文本到语音的转换，图像分类与图像的生成。</p><p>由于其模型之间的概率相关性，两个双重任务之间具有内在联系。Dual supervised learning  则同时训练两个双重任务的模型，并明确利用它们之间的概率相关性来规范训练过程。 </p><p>概率的对偶性</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200707163830395.png" alt="image-20200707163830395" style="zoom:67%;"></p><p>任务目标</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200707163912656.png" alt="image-20200707163912656" style="zoom:67%;"></p><p>将概率对偶性的约束转化为下面的正则项</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200707164221757.png" alt="image-20200707164221757"></p><p>算法流程</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200707164338168.png" alt="image-20200707164338168" style="zoom:67%;"></p><h2><span id="2-nlg-ampnlu">2 NLG &amp;NLU</span></h2><h3><span id="21-2019-inlg-semi-supervised-neural-text-generation-by-joint-learning-of-natural-language-generation-and-natural-language-understanding-models">2.1 2019-INLG-Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models</span></h3><p>提出了一种半监督式深度学习方案，该方案可以从未注释的数据和可用的注释数据中学习。 </p><p>它使用NLG和自然语言理解（NLU）序列到序列模型，这些模型可以共同学习以弥补标注数据的不足。 </p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200707175315658.png" alt="image-20200707175315658" style="zoom:67%;"></p><h3><span id="22-2019-acl-dual-supervised-learning-for-natural-language-understanding-and-generation">2.2 2019-ACL-Dual Supervised Learning for Natural Language Understanding and Generation</span></h3><p>自然语言理解（NLU）和自然语言生成（NLG）都是NLP和对话领域中的关键研究主题。 自然语言理解是从给定的话语中提取核心语义，而自然语言的生成则相反，其目的是根据给定的语义构造相应的句子。 </p><p>这种双重关系尚未在文献中进行研究。 本文提出了一种在双重监督学习基础上自然语言理解和生成的新颖学习框架，为利用双重性提供了一种方法。 </p><p>提出了一种基于双重监督学习的自然语言理解和生成的新型训练框架，该框架首先利用了NLU和NLG之间的对偶性，并将其引入学习目标中作为正则化项。</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200713180452504.png" alt="image-20200713180452504"></p><p>文章还解释了如何对$p(x)$和$p(y)$建模，最后表明，带上正则项的模型比两任务迭代训练的好一点。这篇是个短文，实验量不大。</p><h3><span id="23-2020-acl-towards-unsupervised-language-understanding-and-generation-by-joint-dual-learning">2.3 2020-ACL-Towards Unsupervised Language Understanding and Generation by Joint Dual Learning</span></h3><p>在模块化对话系统中，自然语言理解（NLU）和自然语言生成（NLG）是两个关键组成部分，其中NLU从给定的文本中提取语义，而NLG则根据输入的语义表示构造相应的自然语言句子。</p><p>先前的工作（Su et al。，2019）是首次尝试利用NLU和NLG之间的双重性通过双重监督学习框架来提高绩效。 但是，先前的工作仍然以监督的方式学习了这两个组成部分。 </p><p>取而代之的是，本文引入了一种通用的学习框架来有效地利用这种对偶性，并提供了将有监督和无监督学习算法结合在一起以联合方式训练语言理解和生成模型的灵活性。 </p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200713180729134.png" alt="image-20200713180729134"></p><p>训练算法，分为两部分：Primal Cycle and Dual Cycle  。 前者是让x生成y，生成的y再变回x，后者完全相反。其中，x是semantic representation ， y是 natural language sentences 。</p><p>论文提出了一些强化学习的优化目标，包括Explicit Reward  和Implicit Reward。</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200713182645456.png" alt="image-20200713182645456"></p><p>论文还提出了一些学习的范式，用来联通两个模型一起训练。Straight-Through Estimator，Distribution as Input  。</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200713183152237.png" alt="image-20200713183152237"> </p><p>反正我打我自己就完事了。</p><h3><span id="24-2020-acl-a-generative-model-for-joint-natural-language-understanding-and-generation">2.4 2020-ACL-A Generative Model for Joint Natural Language Understanding and Generation</span></h3><p>也是利用利用NLU和NLG之间的双重性。</p><p>提出了一个生成模型，该模型通过共享的潜在变量耦合NLU和NLG。 这种方法使我们能够探索自然语言和形式表示的空间，并促进通过潜在空间的信息共享，最终使NLU和NLG受益。<br>还表明通过利用未标记的数据以半监督的方式训练模型可提高模型的性能。</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200707180527625.png" alt="image-20200707180527625" style="zoom:67%;"></p><p>NLG 和 NLU本质上是一对 VAE模型。通过x 和 y 可以得到z， 再通过x、z得到y 以及y、z得到x。</p><p><strong>监督学习</strong>情况下的优化目标是：</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714145953409.png" alt="image-20200714145953409"></p><p>可以转化为：</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150020533.png" alt="image-20200714150020533" style="zoom:67%;"></p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150026339.png" alt="image-20200714150026339" style="zoom:67%;"></p><p><strong>无监督情况下</strong>的优化目标：</p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150115230.png" alt="image-20200714150115230"></p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150132883.png" alt="image-20200714150132883"></p><p><img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150137802.png" alt="image-20200714150137802"></p><p>最后的几种训练情况：</p><p>监督学习：<img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150401634.png" alt="image-20200714150401634"></p><p>半监督：<img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150430817.png" alt="image-20200714150430817"></p><p>无监督：<img src="/2020/07/07/nlp-jin-jie/nlp-zhong-de-dui-ou-xue-xi/image-20200714150450655.png" alt="image-20200714150450655"></p><h2><span id="reference">Reference</span></h2><p>[1] Dual Supervised Learning  </p><p>[2] Towards Unsupervised Language Understanding and Generation</p><p>by Joint Dual Learning. ACL 2020</p><p>[3] Semi-Supervised Neural Text Generation by Joint Learning of Natural</p><p>Language Generation and Natural Language Understanding Models ACL 2019</p><p>[4] A Generative Model for Joint Natural Language Understanding and Generation ACL 2020</p><p>[5] Dual Supervised Learning for Natural Language Understanding and Generation 2019 ACL</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP中的对偶学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ElasticSearch</title>
      <link href="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/"/>
      <url>/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/</url>
      
        <content type="html"><![CDATA[<p>2020.07.18再次完善</p><p>本文倾向于讨论涉及ES检索应用过程的原理和特性，对底层和架构不过多关注。</p><h2><span id="1-elasticsearch">1. ElasticSearch</span></h2><p>Elasticsearch 是一个实时的分布式存储、搜索、分析的引擎。</p><p>相比于数据库为什么要用Elasticsearch：Elasticsearch的强大之处就是可以<strong>模糊查询</strong></p><h3><span id="11-术语">1.1 术语</span></h3><p><strong>Index</strong>：相当于数据库的Table</p><p><strong>Document</strong>：Document相当于数据库的一行记录</p><p><strong>Field</strong>：相当于数据库的Column的概念</p><p><strong>Mapping</strong>：相当于数据库的Schema的概念</p><p><strong>DSL</strong>：相当于数据库的SQL</p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/v2-814655822e4782d6277b7bc0b0c420d6_720w.jpg" alt="img" style="zoom:50%;"></p><h3><span id="12-dsl">1.2 DSL</span></h3><p>包括query（查询）、size（返回数量）等，最常用的还是query</p><h4><span id="121-简单单一查询">1.2.1 简单单一查询</span></h4><ul><li>match（匹配查询）</li><li>range（范围查询）</li><li>term（确切查询）</li><li>…</li></ul><p>简单单一查询 - 匹配查询 - match：</p><pre><code># 检索product_name字段包含“toothbrush&#39;的数据dsl_1 = {  &quot;query&quot;: {    &quot;match&quot;: {      &quot;product_name&quot;: &quot;toothbrush&quot;    }  }}</code></pre><pre><code># 检索product_name字段包含“PHILIPS”或“toothbrush&#39;的数据，越多越靠前dsl_2 = {  &quot;query&quot;: {    &quot;match&quot;: {      &quot;product_name&quot;: &quot;PHILIPS toothbrush&quot;    }  }}# 检索product_name字段包含“PHILIPS toothbrush&quot;的数据 - match_phrasedsl_3 = {  &quot;query&quot;: {    &quot;match_phrase&quot;: {      &quot;product_name&quot;: &quot;PHILIPS toothbrush&quot;    }  }}本质上dsl_3等价于以下dsl_4，dsl_4的and改为or等价于dsl_2dsl_4 = {  &quot;query&quot;: {    &quot;match&quot;: {      &quot;product_name&quot;: {        &quot;query&quot;: &quot;PHILIPS toothbrush&quot;,        &quot;operator&quot;: &quot;and&quot;      }     }   }}</code></pre><p>简单单一查询 - 范围匹配 - range：</p><pre><code># 检索price字段包含&gt;=30的数据：dsl = {  &quot;query&quot;: {    &quot;range&quot;: {      &quot;price&quot;: {        &quot;gte&quot;: 30      }    }  }}</code></pre><p>简单单一查询 - 完全匹配 - term terms：</p><pre><code># 检索product_name字段等于“PHILIPS&#39;的数据dsl_1 = {  &quot;query&quot;: {    &quot;term&quot;: {      &quot;product_name&quot;: &quot;PHILIPS&quot;    }  }}# 检索product_name字段等于“PHILIPS&#39;或“Shell&quot;的数据dsl_1 = {  &quot;query&quot;: {    &quot;terms&quot;: {      &quot;product_name&quot;: [          &quot;PHILIPS&quot;,          &quot;Shell&quot;      ]    }  }}</code></pre><p>还有很多，比如match_all、multi_match、match_phrase、exists、missing、prefix等等</p><h4><span id="122-复合查询">1.2.2 复合查询</span></h4><p>复合查询 - bool</p><ul><li><p>must（必须满足）</p></li><li><p>must_not（必须不满足）</p></li><li><p>should（尽量满足）</p></li><li><p>filter（筛选） ： filter底下本质上也是个query，其下的是单一查询字段match、range、term等</p></li><li><p>…</p></li></ul><pre><code>dsl = {    &#39;query&#39;: {        &#39;bool&#39;: {            &quot;must&quot;: {                &quot;product_name&quot;: &quot;toothbrush&quot;            },            &quot;must_not&quot;: {                  &quot;match&quot;: {                    &quot;product_ID&quot;: &quot;HX6730&quot;                  }            },            &quot;should&quot;: {                  &quot;match&quot;: {                    &quot;product_desc&quot;: &quot;刷头&quot;                  }            }            &quot;filter&quot;: {                &quot;range&quot;: {                    &quot;price&quot;: {                            &quot;gt&quot;: 400                    }                }                &quot;term&quot;: {                      &quot;product_name&quot;: &quot;PHILIPS&quot;                }            }        }    }}</code></pre><h3><span id="13-检索原理">1.3 检索原理</span></h3><h4><span id="131-底层数据原理">1.3.1 底层数据原理</span></h4><p>底层数据结构基于 树状的term+倒排索引</p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/v2-ada6a56e22eaa7541552527f64e59e17_720w.jpg" alt="img"></p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/v2-570f46821186218748e6f6e34658f568_720w.jpg" alt="img"></p><p>插入数据时，先对数据进行分词，分出来的词的集合叫Term Dictionary；</p><p>Term Dictionary抽象出一个词前缀 Term Index，放入内存中，方便查找且速度快。</p><p>每个term会对应一个文档列表Posting list， 会根据一些编码技术等对数据压缩和求交并集等；</p><h4><span id="132-结构化查询原理">1.3.2 结构化查询原理</span></h4><p>结构化查询（Structured search） 是指对那些具有内在结构数据的查询，是对查询的另外一个层次划分</p><p>比如日期、时间和数字都是结构化的，因为它们有精确的格式，常用于范围以及值大小的比较查询</p><p>有些文本在某些场景下也是结构化的，是有限的离散的词语的集合，比如性别只有”男“和”女“</p><p>结构化查询的结果：要么存于集合之中，要么存在集合之外</p><p>结构化查询没有”相似“的概念，所以一般情况下不会给该类查询进行打分，提高性能</p><h4><span id="133-词项查询原理">1.3.3 词项查询原理</span></h4><p>词项查询大概有以下查询：</p><pre class=" language-lang-text"><code class="language-lang-text">1. Term Query：词项精确匹配2. Range Query：范围查询3. Exists Query: 是否存在判断查询4. Prefix Query：前缀查询5. Wildcard Query：通配符查询</code></pre><p>在 ES 中 ，词项查询不做分词处理，会直接将输入作为一个整体去倒排索引中查询</p><p>词项查询是对倒排索引的词项精确匹配，它不会对词的多样性进行处理，比如大小写转换之类的</p><h4><span id="134-全文查询原理">1.3.4 全文查询原理</span></h4><p>全文查询大概有以下查询：</p><pre class=" language-lang-text"><code class="language-lang-text">1. Match Query：匹配查询，拆分成多个独立的词进行查询然后汇总2. Match Phrase Query：语句查询3. Query String Query：通过在 URL 中使用 q 参数进行查询，相关说明参考上一篇文章 [Elasticsearch 入门学习](https://zhuanlan.zhihu.com/p/104215274)4. Match Phrase Prefix Query：和 Match Phrase Query 类似，只是在匹配时，容许对最后一个词的前缀进行匹配 5. Multi Match Query：同时对多个字段搜索匹配</code></pre><p>基于全文的查询，索引和搜索时都会进行分词，待查询字符串会先由分词器进行分词然后生成一个供查询的词项列表</p><p><strong>全文的查询会对分词项列表中每个分词进行底层查询，最后在上层进行结果的合并，并为每个查询到的文档生成一个算分</strong></p><p>拿一个例子看出是怎么整合的：</p><pre><code>dsl = {    &quot;size&quot;: 1,    &#39;query&#39;: {        &#39;bool&#39;: {            &quot;must&quot;: {                &quot;match&quot;: {&quot;content&quot;: &quot;我 靠&quot;}            }        }    }}hits = es.search(index=&quot;qa_data&quot;, body=dsl, explain=True)print(hits)   # 自己整理换行真是难受，早知道整个小工具用了{    &#39;took&#39;: 2,     &#39;timed_out&#39;: False,     &#39;_shards&#39;: {        &#39;total&#39;: 1,         &#39;successful&#39;: 1,         &#39;skipped&#39;: 0,         &#39;failed&#39;: 0    },    &#39;hits&#39;: {    &#39;total&#39;: {        &#39;value&#39;: 10000,         &#39;relation&#39;: &#39;gte&#39;    },     &#39;max_score&#39;: 9.72023,     &#39;hits&#39;: [{        &#39;_shard&#39;: &#39;[qa_data][0]&#39;,         &#39;_node&#39;: &#39;256T2YHuTJCxS4X2uW-hMA&#39;,         &#39;_index&#39;: &#39;qa_data&#39;,         &#39;_type&#39;: &#39;_doc&#39;,        &#39;_id&#39;: &#39;ROaF0XUBY0Yp--9b1MZE&#39;,         &#39;_score&#39;: 9.72023,        &#39;_source&#39;: {&#39;pid&#39;: &#39;qid_4791148485424909447&#39;,         &#39;content&#39;: &#39;我们若靠圣灵得生，就当靠圣灵行事，不随从肉体行事，不犯罪&#39;},        &#39;_explanation&#39;: {            &#39;value&#39;: 9.72023,             &#39;description&#39;: &#39;sum of:&#39;,             &#39;details&#39;: [{                &#39;value&#39;: 2.1879215,                 &#39;description&#39;: &#39;weight(content:我 in 236854) [PerFieldSimilarity], result of:&#39;,                &#39;details&#39;: [{                    &#39;value&#39;: 2.1879215, &#39;description&#39;: &#39;score(freq=1.0), computed as boost * idf * tf from:&#39;,                    &#39;details&#39;: [                        {&#39;value&#39;: 2.2, &#39;description&#39;: &#39;boost&#39;, &#39;details&#39;: []},                         {&#39;value&#39;: 2.1715496,&#39;description&#39;: &#39;idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:&#39;,                            &#39;details&#39;: [                                {&#39;value&#39;: 31505,&#39;description&#39;: &#39;n, number of documents containing term&#39;,&#39;details&#39;: []},                                {&#39;value&#39;: 276361,&#39;description&#39;: &#39;N, total number of documents with field&#39;,&#39;details&#39;: []}                            ]                        },                        {                        &#39;value&#39;: 0.4579724,                        &#39;description&#39;: &#39;tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:&#39;,                        &#39;details&#39;: [                             {&#39;value&#39;: 1.0, &#39;description&#39;: &#39;freq, occurrences of term within document&#39;,                              &#39;details&#39;: []},                             {&#39;value&#39;: 1.2, &#39;description&#39;: &#39;k1, term saturation parameter&#39;,                              &#39;details&#39;: []},                             {&#39;value&#39;: 0.75, &#39;description&#39;: &#39;b, length normalization parameter&#39;,                              &#39;details&#39;: []},                             {&#39;value&#39;: 25.0, &#39;description&#39;: &#39;dl, length of field&#39;, &#39;details&#39;: []},                             {&#39;value&#39;: 25.465807, &#39;description&#39;: &#39;avgdl, average length of field&#39;,                              &#39;details&#39;: []}                            ]                        }                    ]                }]            },            {            &#39;value&#39;: 7.532309,             &#39;description&#39;: &#39;weight(content:靠 in 236854) [PerFieldSimilarity], result of:&#39;,            &#39;details&#39;: [{                &#39;value&#39;: 7.532309, &#39;description&#39;: &#39;score(freq=2.0), computed as boost * idf * tf from:&#39;,                &#39;details&#39;: [                    {&#39;value&#39;: 2.2, &#39;description&#39;: &#39;boost&#39;, &#39;details&#39;: []},                     {&#39;value&#39;: 5.4498615,&#39;description&#39;: &#39;idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:&#39;,                    &#39;details&#39;: [                        {&#39;value&#39;: 1187,&#39;description&#39;: &#39;n, number of documents containing term&#39;,&#39;details&#39;: []},                        {&#39;value&#39;: 276361,&#39;description&#39;: &#39;N, total number of documents with field&#39;,&#39;details&#39;: []}]},                        {&#39;value&#39;: 0.6282319, &#39;description&#39;: &#39;tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:&#39;,                             &#39;details&#39;: [                             {&#39;value&#39;: 2.0, &#39;description&#39;: &#39;freq, occurrences of term within document&#39;,                              &#39;details&#39;: []},                             {&#39;value&#39;: 1.2, &#39;description&#39;: &#39;k1, term saturation parameter&#39;,                              &#39;details&#39;: []},                             {&#39;value&#39;: 0.75, &#39;description&#39;: &#39;b, length normalization parameter&#39;,                              &#39;details&#39;: []},                             {&#39;value&#39;: 25.0, &#39;description&#39;: &#39;dl, length of field&#39;, &#39;details&#39;: []},                             {&#39;value&#39;: 25.465807, &#39;description&#39;: &#39;avgdl, average length of field&#39;,                              &#39;details&#39;: []}                              ]                        }                ]}            ]}      ]}}]}}</code></pre><p>从上面的结果可以看出来，使用”我靠“来检索得到：“我们若靠圣灵得生，就当靠圣灵行事，不随从肉体行事，不犯罪”</p><p>主要是算出，“我”字的TF-IDF值与“靠”字TF-IDF值的加和，其中TF-IDF值是按照boost <em> idf </em> tf 来计算得到的。</p><h2><span id="2-python对接es检索应用">2. Python对接ES检索应用</span></h2><h3><span id="21-windows下环境搭建">2.1 windows下环境搭建</span></h3><h4><span id="211-安装java-se环境">2.1.1 安装Java SE环境</span></h4><p>安装jdk：<a href="https://www.oracle.com/java/technologies/javase-downloads.html" target="_blank" rel="noopener">https://www.oracle.com/java/technologies/javase-downloads.html</a></p><p>windows下配置<strong>JAVA_HOME</strong>环境变量（自行百度）</p><h4><span id="212-安装es">2.1.2 安装ES</span></h4><p>官方下载中心：<a href="https://www.elastic.co/cn/downloads/elasticsearch" target="_blank" rel="noopener">https://www.elastic.co/cn/downloads/elasticsearch</a></p><p>安装后打开bin目录下的elasticsearch.bat，浏览器输入<a href="http://localhost:9200后显示如下信息即为安装成功" target="_blank" rel="noopener">http://localhost:9200后显示如下信息即为安装成功</a></p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/image-20201119105040286.png" alt="image-20201119105040286"></p><h4><span id="213-安装head插件可选建议安装">2.1.3 安装head插件(可选，建议安装)</span></h4><ol><li><p>安装node.js：<a href="https://nodejs.org/en/" target="_blank" rel="noopener">https://nodejs.org/en/</a>   node -v可以检测是否成功</p></li><li><p>在elasticsearch-head-master目录下执行 npm install -g grunt-cli<br>grunt 是基于Node.js的项目构建工具，可以进行打包压缩、测试、执行等等的工作，head插件就是通过grunt启动。 grunt -version可以检测是否成功‘<br><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/image-20200718214302194.png" alt="image-20200718214302194"></p></li><li><p>下载并解压elasticsearch-head-master 到安装目录下，例如我的D:\software\elasticsearch-7.8.0-windows-x86_64\elasticsearch-7.8.0\elasticsearch-head-master<br>elasticsearch-head-master 的可用版本： <a href="https://codeload.github.com/mobz/elasticsearch-head/zip/master" target="_blank" rel="noopener">https://codeload.github.com/mobz/elasticsearch-head/zip/master</a> </p></li><li><p>配置 elasticsearch-7.8.0\config\elasticsearch.yml</p><pre><code>network.host: 0.0.0.0   # 设成0.0.0.0让任何人都可以访问(个人使用没事，线上服务别这样)http.port: 9200http.cors.enabled: true   # 解决elasticsearch-head 集群健康值: 未连接问题http.cors.allow-origin: &quot;*&quot;</code></pre></li><li><p>在elasticsearch-head-master目录下执行<strong>npm install</strong> 安装依赖（windows 下用git bash）</p></li><li><p>修改elasticsearch-head-master配置，修改服务器监听地址:<strong>Gruntfile.js</strong> </p></li></ol><pre><code>connect: {            server: {                options: {                    port: 9100,                    base: &#39;.&#39;,                    keepalive: true,                    hostname: &#39;*&#39;                }            }        }</code></pre><ol><li><p>继续在git bash,该目录下执行<strong>grunt server</strong> 命令，启动head服务</p></li><li><p>访问head管理页面，地址:<a href="http://localhost:9100/" target="_blank" rel="noopener">http://localhost:9100/</a> </p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/image-20200718210608007.png" alt="image-20200718210608007"></p></li></ol><p>上面的都是跟着reference某篇顺着下来的，一切顺利没遇到别的情况。</p><h4><span id="214-python依赖安装">2.1.4 python依赖安装</span></h4><p>pip install elasticsearch</p><p>启动服务—安装目录下的bin目录下，cmd中：</p><p>D:\software\elasticsearch-7.8.0-windows-x86_64\elasticsearch-7.8.0\bin</p><pre><code>启动   elasticsearch-service.bat start   停止   elasticsearch-service.bat stop</code></pre><p>或者单机该文件和关闭cmd来启动和停止</p><h4><span id="215-再次使用">2.1.5 再次使用</span></h4><pre><code>D:\software\elasticsearch-7.8.0-windows-x86_64\elasticsearch-7.8.0\elasticsearch-head-master**grunt server** 命令，启动head服务</code></pre><h3><span id="22-python-操作">2.2 python 操作</span></h3><pre><code>from elasticsearch import Elasticsearch</code></pre><h4><span id="221-index操作">2.2.1 index操作</span></h4><p>index是必须的，类似数据库名。</p><p><strong>创建一个Index</strong></p><pre class=" language-lang-python"><code class="language-lang-python">#  by default we connect to localhost:9200es = Elasticsearch()   # create an index in elasticsearch, ignore status code 400(index already exists)result = es.indices.create(index='songname', ignore=400)print(result)</code></pre><p>运行结果：</p><pre><code># 成功则为：{&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;news&#39;}# 由于index已经存在了，重新运行则为：{&#39;error&#39;: {&#39;root_cause&#39;: [{&#39;type&#39;: &#39;resource_already_exists_exception&#39;, &#39;reason&#39;: &#39;index [news/K_gOfdJHRb2_BenLKPQNug] already exists&#39;, &#39;index_uuid&#39;: &#39;K_gOfdJHRb2_BenLKPQNug&#39;, &#39;index&#39;: &#39;news&#39;}], &#39;type&#39;: &#39;resource_already_exists_exception&#39;, &#39;reason&#39;: &#39;index [news/K_gOfdJHRb2_BenLKPQNug] already exists&#39;, &#39;index_uuid&#39;: &#39;K_gOfdJHRb2_BenLKPQNug&#39;, &#39;index&#39;: &#39;news&#39;}, &#39;status&#39;: 400}</code></pre><p><strong>删除一个index</strong></p><pre><code>result = es.indices.delete(index=&#39;songname&#39;, ignore=[400, 404])</code></pre><p>可以通过在git bash 中运行‘ curl -X GET ‘<a href="http://localhost:9200/_cat/indices?v" target="_blank" rel="noopener">http://localhost:9200/_cat/indices?v</a>‘  ’ 查看当前index</p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/image-20200718211521012-1605667607033.png" alt="image-20200718211521012"></p><h4><span id="222-插入数据">2.2.2 插入数据</span></h4><p>这是插入数据or更新数据的命令，可以自定义id，也可以分配。</p><pre><code>es.index(index=&#39;music&#39;, body = {&#39;song_id&#39;:song_id,&#39;comment&#39;:comment }) es.index(index=&#39;music&#39;, id=123, body = {&#39;song_id&#39;:song_id,&#39;comment&#39;:comment })</code></pre><p>然后可以在head端看到已经导入的数据</p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/image-20200718231228824-1605667598445.png" alt="image-20200718231228824" style="zoom:50%;"></p><p><img src="/2020/05/30/yu-yan-gong-ju-ji-zhu-deng-wen-dang/elasticsearch/image-20200718231609075-1605667602227.png" alt="image-20200718231609075"></p><h4><span id="223-查询数据">2.2.3 查询数据</span></h4><p>直接无条件查询，可以搜索到所有结果</p><pre><code>result = es.search(index=&#39;news&#39;)print(result) # 会得到一个字典，里面包含各种信息，以及最终的结果</code></pre><p>我这里的例子是一个song_id对应一堆comment，我存入的数据的数据结构为：</p><pre><code>{    song_id : song_id,    comment : comment}</code></pre><p>通过更复杂一点的dsl 查询办法，限制song_id的对应，然后再通过lyric去检索comment</p><pre><code>dsl = {    &quot;size&quot;: 5,    &#39;query&#39;: {        &#39;bool&#39;: {            &#39;filter&#39;: [{&quot;term&quot;: {&quot;song_id&quot;: song_id}}],            &quot;must&quot;: {                   &quot;match&quot;: {&quot;comment&quot;: &#39;&#39;.join(lyric)}               }        }    }}result = es.search(index=&#39;music&#39;, body=dsl)for hit in result[&#39;hits&#39;][&#39;hits&#39;]:    sorted_comment.append(hit[&#39;_source&#39;][&#39;comment&#39;])print(sorted_comment)</code></pre><p>最终搞定一个简单的检索应用，也达到了我的要求。</p><p>因为数据结构很简单，所以并没有涉及太多复杂的内容，之后再用到再来补充。</p><h2><span id="reference">Reference</span></h2><p><a href="https://github.com/elastic/elasticsearch-py" target="_blank" rel="noopener">https://github.com/elastic/elasticsearch-py</a></p><p><a href="https://blog.csdn.net/wufaliang003/article/details/81368365" target="_blank" rel="noopener">https://blog.csdn.net/wufaliang003/article/details/81368365</a></p><p><a href="https://www.cnblogs.com/gangle/p/9328257.html" target="_blank" rel="noopener">https://www.cnblogs.com/gangle/p/9328257.html</a></p><p><a href="http://www.ruanyifeng.com/blog/2017/08/elasticsearch.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2017/08/elasticsearch.html</a></p><p><a href="https://www.zhihu.com/question/323811022" target="_blank" rel="noopener">https://www.zhihu.com/question/323811022</a></p><p><a href="https://www.cnblogs.com/sddai/p/11061412.html" target="_blank" rel="noopener">https://www.cnblogs.com/sddai/p/11061412.html</a></p><p><a href="https://zhuanlan.zhihu.com/p/46400723" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46400723</a></p><p><a href="https://zhuanlan.zhihu.com/p/104631505" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/104631505</a></p><p><a href="https://my.oschina.net/stanleysun/blog/1594220" target="_blank" rel="noopener">https://my.oschina.net/stanleysun/blog/1594220</a></p>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ElasticSearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本生成特异性</title>
      <link href="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/"/>
      <url>/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-问题背景">1 问题背景</span></h2><p>文本生成的安全响应问题，例如在对话领域，容易生成无意义的安全回答，例如”我不知道“。</p><p>倾向于返回高频但毫无意义响应的原因，一部分在于以往的方法针对一对一关系进行建模，忽略了可能的一对多的关系。</p><p>目前的方法汇总：</p><ol><li><p>找到一个更好的目标函数</p></li><li><p>改进排序规则（训练成本低）  90   70   91  92   93</p></li><li><p>引入潜在变量  94  82  95  96 97      很难解释，变量很难建模</p></li><li>引入主题 101   103</li></ol><h2><span id="2-评估">2 评估</span></h2><p>相关性：PPL、BLEU1、BLEU2</p><p>多样性：distinct-1、 distinct-2</p><p>其他：人工评估</p><p>论文[4]中使用了：distinct-1 &amp; distinct-2、BLEU</p><h1><span id="论文1中使用了distinct-1-amp-distinct-2-count-numbers-of-distinct-unigrams-and-bigrams-in-the-generated-responses-bleu-average-amp-extrema映射到向量空间中求余弦相似度参考论文2-人工评估">论文[1]中使用了：distinct-1 &amp; distinct-2 （count numbers of distinct unigrams and bigrams in the generated responses）、BLEU、Average &amp; Extrema（映射到向量空间中求余弦相似度，参考论文[2]）、人工评估。</span></h1><h2><span id="3-相关工作">3 相关工作</span></h2><h3><span id="31-目标函数改进">3.1 目标函数改进</span></h3><p>论文[4] 是较为早期的这方面的工作，它从目标函数入手。</p><p>一般的模型是以最大似然函数为目标函数，分数最高的回复通常是最常见的句子，更有意义的回复也会出现在N-best列表（beam search的结果）中，但是分数会相对低一些。此论文在模型中使用最大互信息作为目标函数。</p><p>最大似然函数，就是给定Source得到Target的概率最大；而加入了互信息，就相当于加入了source和target之间的相关性。</p><p>论文一共提出俩基于MMI的损失函数。</p><p>一个是：</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526190648554.png" alt="image-20200526190648554"></p><p>来源于以下互信息公式的改动：</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526190700851.png" alt="image-20200526190700851"></p><p>另一个是：</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526190731459.png" alt="image-20200526190731459"></p><p>其他的方法或多或少的都涉及目标函数的改进。</p><h3><span id="32-解码过程改进排序规则">3.2 解码过程改进排序规则</span></h3><h3><span id="33-引入潜在变量">3.3 引入潜在变量</span></h3><p>论文[1]是对话特异性的工作，避免生成无意义回答，例如”我不知道“。论文提出了一种新颖的受控反应产生机制，核心是输入了一个控制变量。</p><p>整体任务的输入除了utterence之外，还有一个范围是0-1的控制变量，这个控制变量通过高斯核层与单词的用法表示进行交互，指导模型生成不同特异性级别的响应。至于单词的用法表示，作者假设每个单词，除了与其含义相关的语义表示之外，在不同的响应目的下，还具有与使用偏好相关的另一个表示，将此表示形式称为单词的用法表示形式。</p><p>最后的概率由两部分组成，一部分是常规seq2seq的decoder得到的，一部分是包含了控制变量、单词用法表示、高斯核层的一个概率。（花里胡哨听得美妙）</p><p>原始数据中并没有这个控制变量，论文通过远程监督的方式去获取，并且描述了两种获取特异性控制变量的远距离标签的方法。两种方法分别是根据回复整体或者回复内词语的频率来打标签。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200525182333650.png" alt="image-20200525182333650"></p><p>论文[6] 提到以往的生成模型倾向于返回高频但毫无意义的响应，论文提出不同的source和不同的responce本身存在一对一的关系， 例如，考虑输入“您吃过饭了吗？”  （在汉语中，该词广泛用于问候），喜欢修辞问题的受访者可以回答“你呢？”。 相反，喜欢陈述性句子的被访者可以肯定地回答“是的，我有”。</p><p>论文提出了一种机制感知的响应机（MARM）的概率框架，假设存在一些潜在的响应机制，每种机制都可以为单个输入生成不同的响应。 在这种假设下，将不同的响应机制建模为潜在的嵌入，并开发了一种编码器-分离器-解码器（encoder-diverter-decoder ）框架，以端到端的方式训练其模块，其中diverter用于生成机制感知的上下文。</p><p>依赖于机制的响应生成这有助于将不合语法的输出与有意义但不频繁的响应区分开。对于输入x，有三种可能的输出，一种是概率大的常见响应，一种是概率小的错误语法响应，一种是有意义但不常见且概率也小的输出。diverter是为了将后两者区分开。然后相当于通过 latent responding factors 去建模多响应的机制。</p><p>说这么复杂，本质上就是对于一个输入x，挑选最可能的topL个机制，解码时每个机制得到K个回答，这L乘K个回答进行一个rerank。但是也有弊端，这种 latent  factors 很难去解释和确定factor的数量。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526205910691.png" alt="image-20200526205910691" style="zoom: 67%;"></p><p>论文[7] 提到seq2seq将不同的响应规律建模为一对一映射。</p><p>有前途的方法主要结合多种潜在机制来建立一对多关系，但是在训练过程中，如果没有准确选择与目标响应相对应的潜伏机制，这些方法就会对潜伏机制进行粗略的优化。此论文提出了一种多映射机制来更好地捕获一对多关系，其中多个映射模块被用作潜在机制来对从输入帖子到其不同响应的语义映射进行建模。 为了精确优化潜在机制，设计了后映射选择模块，根据目标响应选择对应的映射模块进行进一步的优化。 还介绍了辅助匹配损失，以帮助优化后映射选择。 </p><p>论文的贡献点在于提出了一种多重映射机制来捕获具有多个映射模块的一对多关系作为潜在机制，更具灵活性和可解释性。还提出了一种新颖的后部映射选择模块，以根据训练过程中的目标反应选择相应的映射模块，从而确保更准确地优化潜在机制。 还引入了辅助匹配损失，以促进后映射选择的优化。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200527140704469.png" alt="image-20200527140704469"></p><p>过程中，分别对post和responce编码，然后对post进行一个线性映射，映射到K个不同的responce表达中。然后从K个模块中选择响应的映射模块，引入了分类分布 π 来表示以目标响应为条件的响应，本质上就是一个m与y的内积+softmax。另外这个映射模块在测试过程中是随机挑选的。</p><p>实验过程中，发现映射挑选过程倾向于挑选相同的模块，模型陷入单个映射模块的局部最优。为了解决这个问题，加入了一个辅助目标，以改进responce部分的编码，即post和responce的相关性概率（内积）</p><p>论文 [9] 提出了一种对话生成模型，该模型直接捕获对对应输入的responce，从而减少了确定性对话模型的“无聊输出”问题。 </p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200527180529646.png" alt="image-20200527180529646" style="zoom:67%;"></p><p>论文[11] -2019 针对于对话生成多样性和相关性的联合优化。</p><p>论文中提出一种新颖的几何(?)的方法，利用了两种不同的模型，一个seq2seq产生预测的响应，一个ae产生潜在的响应，并且为了共享相同的潜在空间，使用了同一个解码器，并进行联合训练。论文强调了加入了正则化项的必要性，只共享解码器不一定会对齐两者的潜在空间，两个正则化项分别是：插值项Linterp，融合项Lfuse。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200608130132530.png" alt="image-20200608130132530"></p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200608130145180.png" alt="image-20200608130145180"></p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200608130032872.png" alt="image-20200608130032872"></p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601163800516.png" alt="image-20200601163800516"></p><p>论文[13] 建立短文本对话生成输入和输出的一对多映射。</p><p>模型的输入是x对应的集合y，对于每一个x，都设定有一个对应的隐变量z，并通过两个限制条件（合理地）将z设定为词典中的词。</p><p>两个模型交替训练，左边得到一些词，右边得到z的反馈。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601195532257.png" alt="image-20200601195532257" style="zoom:67%;"></p><h3><span id="34-引入主题或其他信息">3.4 引入主题或其他信息</span></h3><p>论文[3] 也是基于seq2seq的聊天机器人，为了生成有益的有趣的responce，论文将主题信息融入了seq2seq框架中，减少“我也是”，“我看到”或“我不知道”之类的琐碎响应。</p><p>输入的信息中通过其他语料预训练过的LDA提取出主题词，将输入信息和主题词分别编码后，通过attention机制，在解码过程中上下文向量和主题向量共同影响responce的生成，并修改loss，除原decoder之外还加入了主题相关的概率项。</p><p>综上，论文主要在于将主题词作为先验，并且改进了seq2seq。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200525231737567.png" alt="image-20200525231737567" style="zoom:67%;"></p><p>论文[8] 在对话生成的过程中，加入了一些Meta-Words，通过这些信息的辅助，显式建模一对多关系。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200527161032014.png" alt="image-20200527161032014" style="zoom:67%;"></p><p>具体而言，论文提出了一个goal tracking memory network: GTMN，在seq2seq的基础上，加入了状态存储模块和状态控制器。在编码过程中使用双向GRU，加入状态控制，使用meta-words初始化这个目标追踪网络，在解码过程中meta-words由状态控制器来更新，通过目前状态与目标的差异来控制解码。</p><p>损失方面，除了负对数可能性之外，还提出了最小化状态更新损失，该状态更新损失可以直接监督基于基本事实的存储网络的学习。 </p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200527162231630.png" alt="image-20200527162231630"></p><p>论文[14] 使用逐点互信息来选取单词作为关键字，再将这个关键字来解码得到responce。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601201056052.png" alt="image-20200601201056052" style="zoom:67%;"></p><p>论文[15] 集中于在学习过程中选择适当的知识，进而帮助对话的多样性。</p><p>训练过程的输入为，utterence、N个knowledge、response，输出为responce。测试过程中不输入responce。</p><p>将输入x、y、k进行编码后，计算给定x得到各个知识的先验概率，以及给定x、y得到各个k的后验概率，求两者的KL散度作为损失项之一。</p><p>然后将knowledge manager选定的某个k和x都输入decoder，得到y，这是一个loss。根据y和知识得到一个bow loss。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601222330314.png" alt="image-20200601222330314" style="zoom:67%;"></p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601222628334.png" alt="image-20200601222628334" style="zoom:67%;"></p><p>论文[16] 是一篇workshop。提出了较为简单的结构，用到了positive pointwise mutual information， 首先从source中识别关键词，并鼓励用关键词生成响应。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200602160320001.png" alt="image-20200602160320001"></p><p>论文[10] 的任务定义是，输入：sentence和style id，输出sentence</p><p>将sentence和id进行concat，然后投入decoder，每一步得到一个期望字符词向量，然后将得到的期望字符词向量分布于风格表示分布的互信息加入loss。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601152621511.png" alt="image-20200601152621511" style="zoom:67%;"></p><h3><span id="35-其他">3.5 其他</span></h3><p>论文[5] 提出一种方法，这种方法涉及数据提炼和模型训练之间的交替进行。删除上一轮训练的模型最常见的responce的数据，然后在剩余数据中重新训练模型。不同程度的数据提炼训练的模型表现出不同的特异性水平。</p><p>论文还训练了一个强化学习系统，来调整生成模型。下面是数据蒸馏的伪代码：</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526201502467.png" alt="image-20200526201502467" style="zoom:67%;"></p><p>实验效果看起来很有意思，随着数据蒸馏，在验证集上的损失和困惑度越来越大的同时，代表多样性的指标distinct-1和distinct-2增大不少。（不过也可以归结为迭代次数带来的，这方面并没有对比实验）</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526202512565.png" alt="image-20200526202512565"></p><p>论文[12] ，为对话生成提出了一个新的范式，即原型-编辑，它首先从预定义的索引中检索原型响应，然后根据原型上下文和当前上下文之间的差异来编辑原型响应。 我们的动机是，检索到的原型在语法上和信息量方面都提供了一个很好的生成起点，并且后期编辑过程进一步提高了原型的相关性和连贯性。 在实践中，我们设计了一个基于上下文的编辑模型，该模型建立在以编辑矢量为补充的编码器-解码器框架上。 我们首先通过考虑原型上下文和当前上下文之间的词汇差异来生成编辑矢量。 之后，将编辑矢量和原型响应表示馈送到解码器以生成新的响应。 </p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200601164849456.png" alt="image-20200601164849456" style="zoom:67%;"></p><h2><span id="4-模型层面">4 模型层面</span></h2><h3><span id="论文6-mechanism-aware-neural-machine-for-dialogue-response-generation-2017">论文[6] Mechanism-Aware Neural Machine for Dialogue Response Generation    2017</span></h3><p>论文[6] 提到以往的生成模型倾向于返回高频但毫无意义的响应，论文提出不同的source和不同的responce本身存在一对一的关系， 例如，考虑输入“您吃过饭了吗？”  （在汉语中，该词广泛用于问候），喜欢修辞问题的受访者可以回答“你呢？”。 相反，喜欢陈述性句子的被访者可以肯定地回答“是的，我有”。</p><p>论文提出了一种机制感知的响应机（MARM）的概率框架，假设存在一些潜在的语言机制，每种机制都可以为单个输入生成不同的响应。 在这种假设下，将不同的响应机制建模为潜在的嵌入，并开发了一种编码器-分离器-解码器（encoder-diverter-decoder ）框架，以端到端的方式训练其模块，其中diverter用于生成机制感知的上下文。</p><p>对encoder-decoder模型的中间产物context vector c进行分类，得到概率最大的机制 <img src="https://www.zhihu.com/equation?tex=m_%7Bi%7D" alt="[公式]"> ,然后将 <img src="https://www.zhihu.com/equation?tex=m_%7Bi%7D" alt="[公式]"> 与context vector 进行拼接作为decoder的输入，解码出生成的回复。</p><p><img src="https://www.zhihu.com/equation?tex=p%28y%7Cx%29%3D%5Csum_%7Bi%3D1%7D%5E%7BM%7D%7Bp%28y%2Cm_i%7Cx%29%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BM%7D%7Bp%28m_i%7Cx%29p%28y%7Cm_i%2Cx%29%7D" alt="[公式]"></p><p>（这段忘了从哪来的了）依赖于机制的响应生成这有助于将不合语法的输出与有意义但不频繁的响应区分开。对于输入x，有三种可能的输出，一种是概率大的常见响应，一种是概率小的错误语法响应，一种是有意义但不常见且概率也小的输出。diverter是为了将后两者区分开。然后相当于通过 latent responding factors 去建模多响应的机制。</p><p>在最后结果生成的过程中，就是对于一个输入x，挑选最可能的topL个机制，解码时每个机制得到K个回答，这L乘K个回答进行一个rerank，rerank的方式是：</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200612174004215.png" alt="image-20200612174004215"></p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200526205910691.png" alt="image-20200526205910691" style="zoom: 67%;"></p><h3><span id="论文17-elastic-responding-machine-for-dialog-generation-with-dynamically-mechanism-selecting-2018-aaai">论文[17] Elastic responding machine for dialog generation with dynamically mechanism selecting.  2018 AAAI</span></h3><p>论文[6]工作的延伸。</p><p>之前的<strong>encoder-diverter-decoder模型</strong>，对encoder-decoder模型的中间产物context vector c进行分类，得到概率最大的机制 <img src="https://www.zhihu.com/equation?tex=m_%7Bi%7D" alt="[公式]"> ,然后将 <img src="https://www.zhihu.com/equation?tex=m_%7Bi%7D" alt="[公式]"> 与context vector 进行拼接作为decoder的输入，解码出生成的回复。</p><p>本文作者在前文的基础上，又加入了Filter模块，作者的目的是从所有mechanism集合中选出一个子集 <img src="https://www.zhihu.com/equation?tex=S_x" alt="[公式]"> ，包含足够多的mechanism，能够生成多种style的response。这个子集 <img src="https://www.zhihu.com/equation?tex=S_x" alt="[公式]"> 需要满足两个条件：1）包含足够多的mechanism；2）子集的mechanism没有太高的重复性，没有冗余。选中了子集 <img src="https://www.zhihu.com/equation?tex=S_x" alt="[公式]"> 后，生成response <img src="https://www.zhihu.com/equation?tex=y" alt="[公式]"> 的概率为</p><p><img src="https://www.zhihu.com/equation?tex=p%28y%7Cx%29%3D%5Cfrac+%7B%5Csum_%7Bm+%5Cin+S_x%7D%5E%7B%7D%7Bp%28m%7Cx%29p%28y%7Cm%2Cx%29%7D%7D%7B%5Csum_%7Bm+%5Cin+S_x%7D%5E%7B%7D%7Bp%28m%7Cx%29%7D%7D" alt="[公式]"></p><p>使用强化学习选择子集，选出包含 <img src="https://www.zhihu.com/equation?tex=K" alt="[公式]">个mechanism之后，依次将 <img src="https://www.zhihu.com/equation?tex=c" alt="[公式]"> 和 <img src="https://www.zhihu.com/equation?tex=m_i%28m_i+%5Cin+S_x%29" alt="[公式]"> 连接在一起输入到decoder中生成各自mechanism对应的response。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200612173719071.png" alt="image-20200612173719071" style="zoom: 50%;"></p><h3><span id="论文7-generating-multiple-diverse-responses-with-multi-mapping-and-posterior-mapping-selection-2019">论文[7] Generating Multiple Diverse Responses with Multi-Mapping and Posterior Mapping Selection 2019</span></h3><p>论文[7] -2019 提到seq2seq将不同的响应规律建模为一对一映射。</p><p>有前途的方法主要结合多种潜在机制来建立一对多关系，但是在训练过程中，<strong>如果没有准确选择与目标响应相对应的潜伏机制，这些方法就会对潜伏机制进行粗略的优化</strong>。</p><p>为了获得更准确的建模，我们假设仅应选择与目标响应相对应的潜在机制进行优化。 例如，给定一个询问响应，我们应该仅优化对询问响应规律进行建模的潜在机制，而不是其他无关的规律。 尽管在某些方法中，对每个潜在机制的优化是由输入帖子中的权重决定的，但考虑到输入帖子和目标响应之间的语义差距，该权重并不代表相应的潜在机制的选择。</p><p><strong>此论文提出了一种多映射机制来更好地捕获一对多关系，其中多个映射模块被用作潜在机制来对从输入的post到其不同响应的语义映射进行建模。 为了精确优化潜在机制，设计了后映射选择模块，根据目标响应选择对应的映射模块进行进一步的优化。 还介绍了辅助匹配损失，以帮助优化后映射选择。</strong> </p><p>因此论文的贡献点在于提出了一种多重映射机制来捕获具有多个映射模块的一对多关系作为潜在机制，更具灵活性和可解释性。还提出了一种新颖的后部映射选择模块，以根据训练过程中的目标反应选择相应的映射模块，从而确保更准确地优化潜在机制。 还引入了辅助匹配损失，以促进后映射选择的优化。</p><p><img src="/2020/05/23/nlp-jin-jie/wen-ben-sheng-cheng-te-yi-xing/image-20200527140704469.png" alt="image-20200527140704469" style="zoom: 67%;"></p><p>过程中，分别对post和responce编码，然后对post进行一个线性映射，映射到K个不同的responce表达中。然后从K个模块中选择响应的映射模块，引入了分类分布 π 来表示以目标响应为条件的响应，本质上就是一个m与y的内积+softmax。另外这个映射模块在测试过程中是随机挑选的。</p><p>实验过程中，发现映射挑选过程倾向于挑选相同的模块，模型陷入单个映射模块的局部最优。为了解决这个问题，加入了一个辅助目标，以改进responce部分的编码，即post和responce的相关性概率（内积）</p><h3><span id> </span></h3><h2><span id="reference">Reference</span></h2><p><strong>[1] Learning to Control the Specificity in Neural Response Generation.   2018</strong></p><p>[2] Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dialogues.   In AAAI.  2017</p><p>[3] Topic Aware Neural Response Generation.   2018</p><p>[4] A Diversity-Promoting Objective Function for Neural Conversation Models.  2016</p><p>[5] Data Distillation for Controlling Specificity in Dialogue Generation   2017</p><p>[6] Mechanism-Aware Neural Machine for Dialogue Response Generation    2017</p><p>[7] Generating Multiple Diverse Responses with Multi-Mapping and Posterior Mapping Selection    2019</p><p>[8] Neural Response Generation with Meta-Words  2019</p><p>[9] Latent Variable Dialogue Models and their Diversity    2017</p><p>[10] Stylistic Chinese Poetry Generation via Unsupervised Style Disentanglement 2018</p><p>[11] Jointly Optimizing Diversity and Relevance in Neural Response Generation    2019</p><p>[12] Response Generation by Context-Aware Prototype Editing   AAAI 2019    数据集不支持模型不具有太多参考性</p><p>[13] Generating Multiple Diverse Responses for Short-Text Conversation   AAAI  2019</p><p>[14] Sequence to Backward and Forward Sequences: A Content-Introducing Approach to Generative Short-Text Conversation  2016</p><p>[15] Learning to Select Knowledge for Response Generation in Dialog Systems  2019  </p><p>[16] Relevant and Informative Response Generation using Pointwise Mutual Information  2019workshop</p><p>[17] Elastic responding machine for dialog generation with dynamically mechanism selecting.  2018 AAAI</p><p>[18] Get the point of my utterance! learning towards effective responses with multi-head attention mechanism.   2018IJCAI</p><h2><span id="end">End</span></h2><h1><span id="大大">大大</span></h1><script type="math/tex; mode=display">a_i</script>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本生成特异性 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>预训练模型具体工作survey</title>
      <link href="/2020/05/23/nlp-jin-jie/yu-xun-lian-mo-xing-ju-ti-gong-zuo-survey/"/>
      <url>/2020/05/23/nlp-jin-jie/yu-xun-lian-mo-xing-ju-ti-gong-zuo-survey/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-预训练模型综述">1 预训练模型综述</span></h2><h3><span id="11-历史进展">1.1 历史进展</span></h3><p><img src="/2020/05/23/nlp-jin-jie/yu-xun-lian-mo-xing-ju-ti-gong-zuo-survey/image-20201102103611033.png" alt="image-20201102103611033"></p><p><strong>第一代自然语言预训练模型：词向量模型</strong><br>I 典型代表：CBOW, Skip-gram, Glove, Fasttext<br>I 词向量表示是固定，不会随着上下文的改变而变化  </p><p><strong>第二代自然语言预训练模型：预训练语言模型</strong><br>I 典型代表：ELMo, BERT, GPT<br>I Pre-training-then-fine-tuning已经成为NLP研究新范式<br>I 将在pre-training阶段学习到的语言表示迁移到下游任务  </p><h3><span id="12-发展方向">1.2 发展方向</span></h3><ol><li><p><strong>大力出奇迹：大模型带来使用方式的变化</strong>  </p><p>预训练词向量：<br>I 将NLP带入神经网络时代<br>I 普通提高了NLP各项任务的水平<br>预训练语言模型（ELMo、BERT、GPT）：<br>I 采用预训练+微调模式，大大简化NLP模型设计<br>I 刷新各项NLP任务的性能<br>预训练语言模型（GPT-3）：<br>I 超大模型，无需微调，直接Zero-shot方式既可以完成各项NLP任务  </p></li><li><p>更小巧：压缩与加速<br>I 各种预训练模型被各大公司竞相提出<br>I 先做大阶段:“大算力+大模型+大数据+创意任务”探索能力边界<br>I 再做小阶段: 在各种下游任务上形成生产力（对话/阅读理解/搜索等）  </p></li><li><p>更优秀：功能更多、性能更高、训练更快<br>更好的模型结构、更难的训练任务、更多的功能  </p></li><li><p>更聪明：外部知识融入  </p></li><li><p>更能干：跨界出圈 （语音、图像等）</p></li></ol><h3><span id="13-训练任务">1.3 训练任务</span></h3><p>I Baseline: LM(GPT,ELMo), MLM(BERT), NSP(BERT)<br>I Whole Word Masking (BERT)、SpanBERT (Joshi et al. 2019)<br>I RTD (Replaced Token Prediction): Electra(Clark 2020)<br>I SOP (Sentence Order Prediction): ALBERT(Lan et al. 2020)<br>I DAE (Denoising Autoencoder (DAE): BART(Mike et al. 2019)<br>I Multi-task Learning: MT-DNN(Liu et al. 2019)<br>I Generator and Discriminator: Electra(Clark 2020)  </p><ul><li>LM(GPT,ELMo)</li><li>Masked Language Modeling (MLM)  (BERT)<ul><li>Sequence-to-Sequence MLM (Seq2Seq MLM)  </li><li>Enhanced Masked Language Modeling (E-MLM)  <ul><li>RoBERTa：动态mask而不是静态</li><li>UniLM：将mask任务拓展成三种LM任务，单向双向和seq2seqMLM</li><li>spanBERT：替代MLM为Random Contiguous Words Masking and Span Boundary Objective </li><li>structBERT：Span Order Recovery task   </li></ul></li></ul></li><li>Permuted Language Modeling (PLM)  <ul><li>取代MLM，对输入序列随机排列，置换序列中mask一些token去预测，根据的是这些token的原始位置和其他token</li></ul></li><li>Denoising Autoencoder (DAE)  </li><li>cc：破坏文本并恢复：mask/token delete/文本跨度mask预测缺失数量/句子打乱排列预测/随机选择一个token并旋转，预测序列开始位置</li><li>Contrastive Learning (CTL)  对比学习<ul><li>对比学习假设观察到的某些文本对在语义上比随机采样的文本更相似。</li><li>Deep InfoMax  预测正常ngram比打乱的概率大</li><li>Replaced Token Detection (RTD)  预测一个token是否被代替</li><li>Next Sentence Prediction (NSP)  </li><li>Sentence Order Prediction (SOP)  </li></ul></li></ul><h2><span id="2-具体工作">2 具体工作</span></h2><h3><span id="自回归型plm">自回归型PLM</span></h3><p><strong>ELMO</strong></p><p>双层双向的LSTM语言模型，特征集成应用到下游任务</p><p><strong>GPT / GPT2</strong></p><p>单向transformer，自回归语言模型。GPT2训练规模更大参数更大，模型结构微调。</p><p>自回归型PLM</p><h3><span id="自编码plm">自编码PLM</span></h3><p><strong>BERT</strong></p><p>Masked LM 获取上下文表征，NSP进行句子级建模。</p><p>缺点：生成任务效果差，无法进行文档级别的任务</p><h3><span id="引入外部知识">引入外部知识</span></h3><p><strong>ERNIE1.0（Baidu） 引入知识（实体）</strong></p><p>在预训练阶段，预先识别出实体，采用3中mask预测任务：随机subword mask，Phrase-Level Masking，Entity-Level Masking</p><p><strong>ERNIE（THU）引入知识（实体对齐-知识嵌入）</strong></p><p>在有实体输入的位置，将实体向量和文本表示通过<strong>非线性变换进行融合</strong>，以融合词汇、句法和知识信息。引入预测实体任务。</p><h3><span id="改进mask策略">改进Mask策略</span></h3><p><strong>Bert-wwm 整词mask</strong></p><p>原生BERT模型：按照subword维度进行mask，然后进行预测；局部的语言信号，缺乏全局建模的能力。</p><p>BERT WWM(Google)：按照whole word维度进行mask，然后进行预测；</p><p><strong>ERNIE1.0（Baidu） 引入知识（实体）</strong></p><p>在预训练阶段，预先识别出实体，采用3中mask预测任务：随机subword mask，Phrase-Level Masking，Entity-Level Masking</p><p><strong>SpanBERT</strong></p><p>采取随机mask一段空间长度，预测span中被mask的部分。</p><h3><span id="引入多任务学习">引入多任务学习</span></h3><p><strong>MTDNN（下游任务引入多任务学习）</strong></p><p>单句分类、句对相似度、句对分类、句对排序  多任务一同finetune</p><p><strong>ERNIE2.0 （Baidu）引入知识（实体）</strong></p><p>ERNIE 2.0 是在预训练引入多任务学习，构建多个层次的任务全面捕捉训练语料中的词法、结构、语义的潜在知识。词法层面、结构层面、语义层面。3大类任务的若干个子任务一起用于训练。</p><p>构建<strong>增量学习（</strong>后续可以不断引入更多的任务<strong>）</strong>模型，通过多任务学习<strong>持续更新预训练模型</strong>，这种<strong>连续交替</strong>的学习范式<strong>不会使模型忘记之前学到的语言知识</strong>。</p><h3><span id="精细调参">精细调参</span></h3><p><strong>RoBERTa(FaceBook)</strong></p><p>丢弃NSP，效果更好；动态改变mask策略，把数据复制10份，然后统一进行随机mask；</p><p>对学习率的峰值和warm-up更新步数作出调整；在更长的序列上训练： 不对序列进行截短，使用全长度序列；</p><p><img src="/2020/05/23/nlp-jin-jie/yu-xun-lian-mo-xing-ju-ti-gong-zuo-survey/1617252934206.png" alt="1617252934206"></p><h3><span id="大改进">大改进</span></h3><p><strong>XLNET</strong></p><p>针对自回归PLM无法对上下文进行表征。具体怎么做才能让这个模型：看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息呢？</p><p>引入排列语言模型：原LM的过程中引入打乱的序列，这样在预测当前token时可以看到一部分下文信息。</p><p><strong>ELECTRA</strong></p><p>把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过，<strong>序列标注任务</strong>。</p><p>随机替换太简单，那么就用一个MLM的G-BERT来对输入句子进行更改（MLM任务，输入的是mask的句子），然后去判断（序列标注）。</p><p>两者一同训练，但梯度不传递（离散的中间结果）</p><p><img src="/2020/05/23/nlp-jin-jie/yu-xun-lian-mo-xing-ju-ti-gong-zuo-survey/1618908926156.png" alt="1618908926156"></p><p><strong>Transformer-XL 文档级的建模</strong></p><p>如果序列长度超过固定长度，处理起来就比较麻烦。一种处理方式，就是将文本划分为多个segments。</p><p>这存在两个问题，1）因为segments之间独立训练，所以不同的token之间，最长的依赖关系，就取决于segment的长度；2）出于效率的考虑，在划分segments的时候，不考虑句子的自然边界，而是根据固定的长度来划分序列，导致分割出来的segments在语义上是不完整的。</p><p>在Trm的基础上，Trm-XL提出了一个改进，在对当前segment进行处理的时候，缓存并利用上一个segment中所有layer的隐向量序列，而且上一个segment的所有隐向量序列只参与前向计算，不再进行反向传播，这就是所谓的segment-level Recurrence。</p><h2><span id="reference">Reference</span></h2><p><a href="https://arxiv.org/abs/2003.08271" target="_blank" rel="noopener">https://arxiv.org/abs/2003.08271</a></p><p>pre-trained-langauge-models-research-advances-and-prospectives 刘群</p><p><a href="https://zhuanlan.zhihu.com/p/346964839" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/346964839</a></p><p><a href="https://zhuanlan.zhihu.com/p/84159401" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/84159401</a></p><p><a href="https://zhuanlan.zhihu.com/p/76912493" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/76912493</a></p><p><a href="https://zhuanlan.zhihu.com/p/71759544" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/71759544</a></p><p><a href="https://zhuanlan.zhihu.com/p/89763176" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/89763176</a></p><p><a href="https://zhuanlan.zhihu.com/p/70257427" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/70257427</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 预训练模型具体工作survey </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP中的联合建模与多任务学习</title>
      <link href="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/"/>
      <url>/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/</url>
      
        <content type="html"><![CDATA[<h1><span id="nlp中的联合建模与多任务学习">NLP中的联合建模与多任务学习</span></h1><p>[TOC]</p><h2><span id="1-介绍">1 介绍</span></h2><h3><span id="11-多任务场景分类">1.1 多任务场景分类</span></h3><p>自然语言处理有很多种任务，中文分词、依存分析、命名实体识别、关系抽取、对话生成等等。这些任务大多都可以通过一种端到端的单模型的方法来解决。</p><p>当然，有时候也会同时解决多个任务，例如一些相似或者相关的任务。</p><p>一些非常相似的NLP任务，比如：</p><ol><li><p>词性标注、句子分块、命名实体识别</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588065124953.png" alt="1588065124953" style="zoom:50%;"></p></li><li><p>Constituents and named entities ——成分划分和命名实体<br>一个是确定句子中各个词的语法成分，一个是确定句子中的各个词是否属于某个命名实体的一部分。</p></li></ol><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588064935807.png" alt="1588064935807" style="zoom:50%;"></p><p>例如一些任务是比较相关的，需要进行流水线工作的，比如：</p><ol><li><p>词性标注——分词+标注<br><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588065173127.png" alt="1588065173127" style="zoom:50%;"></p></li><li><p>实体关系抽取——命名实体识别 + 关系分类<br><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588065222089.png" alt="1588065222089" style="zoom: 33%;"></p></li><li><p>情感分析——命名实体识别 + 命名实体的情感分类</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588065257315.png" alt="1588065257315" style="zoom:33%;"></p></li></ol><p>当想要解决NLP领域的某种任务时，会发现有一些或相似或相关的其他任务，通过多个任务的联合建模，可以帮助提高目标任务的效果。</p><p>这些多任务的场景，根据已经了解的一部分工作，目前总结如下：</p><ol><li><p>比较相似的多个任务<br>例如，词性标注和中文分词。可以通过相似任务的促进，提高任务的效果。</p></li><li><p>具有Pipeline关系的多个任务<br>例如，实体识别和关系抽取。可以通过上游或者下游的任务互相促进。<br>Pipeline关系的几个子任务也可以看做是几个相似的任务</p></li><li><p>具有辅助关系的任务<br>例如自监督的辅助任务，可以帮助目标任务学习原任务不容易学到的特征等</p></li><li><p>具有对抗关系的或者相反的任务<br>例如，不同评估标准的中文分词任务。可以通过对抗训练帮助多任务学习</p></li><li><p>类似迁移学习的多任务<br>例如，辅助资源匮乏的任务，语言建模的预训练。</p></li></ol><h3><span id="12-联合建模方法分类">1.2 联合建模方法分类</span></h3><p>当面临着解决多种任务的时候，最一般的解决想法：</p><ul><li>看做几个独立的子任务。<br>他们单独训练，单独预测。<br>缺点：这样就浪费了他们的相关性，消费n倍的成本去解决n个问题，信息在任务之间没有交换，任务是互相隔离的，任务之间没有关系更没有相互促进。</li></ul><p>因此就有了联合建模的说法，联合建模就允许信息在任务间交换，让任务互相促进提升效果。</p><p>Joint models需要进行两个选择：1. 各个模型间是否共享参数、联合学习；2. 各个模型得出的结果是否互相影响。</p><p> <img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588068165849.png" alt="1588068165849" style="zoom: 50%;"></p><p>因此就出现了不同的组合：</p><ol><li>Joint Learning &amp; Joint Search</li><li>Separate Learning &amp; Joint Search</li><li>Joint Learning &amp; Separate Search</li></ol><p>至于Separate Learning &amp; Separate Search，很明显不属于Joint model的范畴。</p><h3><span id> </span></h3><p>接下来就是现有的各类任务的一些工作介绍了。</p><h2><span id="2-相似关系多任务学习">2 相似关系——多任务学习</span></h2><h3><span id="21-多任务学习方法分类">2.1 多任务学习方法分类</span></h3><p>多任务学习一般属于：Joint Learning &amp; Separate Search。</p><p>一般通过参数共享的方式互相促进，参数共享有两种形式：</p><ul><li>Hard参数共享：目前应用最为广泛的共享机制，通过在所有任务之间共享底部的隐藏层，同时保留几个特定任务的输出层来实现。适合处理有较强相关性的任务，但遇到弱相关任务时常常表现很差。<br><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588244058304.png" alt="1588244058304" style="zoom:50%;"></li><li>Soft参数共享：每个任务都有自己的参数，模型参数之间通过一定的限制（例如正则化）来鼓励参数相似化，每个任务的网络都可以访问其他任务对应网络中的信息，例如表示、梯度等。软共享机制非常灵活，不需要对任务相关性做任何假设，但是由于为每个任务分配一个网络，常常需要增加很多参数。</li></ul><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/8a64e11d217d4867b6d4b6b0a43a86f4_th.jpg" alt="img" style="zoom:50%;"></p><ul><li><p>分层共享：在网络的低层做较简单的任务，在高层做较困难的任务。分层共享比硬共享要更灵活，同时所需的参数又比软共享少，但是为多个任务设计高效的分层结构依赖专家经验。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/640.webp" alt="img" style="zoom: 50%;"></p></li><li><p>其他共享方式，例如论文[30]中的稀疏共享，为每个任务生成子网络，多任务子网络联合训练。</p></li></ul><h3><span id="22-相似任务的一些工作">2.2 相似任务的一些工作</span></h3><p>论文[8]不太容易找，附上链接：<a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" target="_blank" rel="noopener">http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf</a></p><p>模型解决各种任务，包括Tagging, Chunking and NER。</p><p>解决方案是多任务学习，其中lookup table参数共享、first linear layers参数共享。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588135252062.png" alt="1588135252062"></p><p>论文[10]的背景是：在之前有关深度多任务学习的工作中，所有任务监督都在同一（最外）层上。<br>论文提出了一种具有深层双向RNN的多任务学习体系结构，可以在不同的层进行不同的任务监督。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588140468580.png" alt="1588140468580" style="zoom:50%;"></p><p>论文[11] 是语义角色标注SRL和Parser的任务</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588141011657.png" alt="1588141011657" style="zoom: 33%;"></p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588141036358.png" alt="1588141036358" style="zoom:50%;"></p><p>总之，就是embedding层共享了参数而已</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588141273348.png" alt="1588141273348" style="zoom:33%;"></p><p>论文[19] 是机器翻译任务，他使用一个调度的多任务框架来帮助改善翻译质量：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588156350821.png" alt="1588156350821" style="zoom:50%;"></p><p>论文[21] 采用标准的多任务学习的方法，解决音译的问题。（采用了多种音译的数据）</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588157680439.png" alt="1588157680439" style="zoom:67%;"></p><h2><span id="3-pipeline关系的任务">3 Pipeline关系的任务</span></h2><p>当面临着解决pipeline任务的时候，最一般的解决想法：</p><ul><li>看做几个独立的子任务。他们单独训练，单独预测。用上游任务的结果作为下游任务的输入。<br>缺点：任务之间有误差的传递，任务越多，Error影响越大； 这样也浪费了他们的相关性，信息在任务之间没有交换，任务是互相隔离的，任务之间没有关系更没有相互促进。</li></ul><p>对pipeline上的这些子任务任务进行联合建模，就是为了力图减少Error propagation，提升效果。</p><p>Pipeline关系有三方面的解决方案，其中Joint Learning &amp; Separate Search 与相似任务的解决方式基本类似。</p><p>Joint Learning &amp; Joint Search 方面一般是训练单个模型然后联合解码；</p><p>Separate Learning &amp; Joint Search方面一般是训练两个独立的模型然后联合解码</p><h3><span id="21-joint-learning-amp-joint-search">2.1 Joint Learning &amp; Joint Search</span></h3><p>论文[2]中讨论了 Chinese part-of-speech tagging 任务中 One-at-a-time or all-at-once?  </p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588070679102.png" alt="1588070679102" style="zoom: 50%;"></p><p>中文词性标注为每个词语分配一个POS标签。 但是，由于中文句子中未划分单词，因此中文POS标签需要以中文分词为前提。 </p><p>可以严格在分词之后再进行词性标注（one-at-a-time），或者同时进行分词和POS标签这两种方法（all-at once）</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588070706470.png" alt="1588070706470" style="zoom:50%;"></p><p>论文发现，虽然基于字符的all-at once方法是最好的方法，但是基于字符的one-at-a-time方法却是一个值得折衷的方法，在准确性方面仅稍差一些，但训练和测试时间却更短。 （略有提升，差别不大）</p><p>论文[6]的任务是：词性标注。</p><p>以往的pipeline是：分词+pos。论文提出一个Joint Segmentation and Tagging Model。</p><p>模型的输入是：预定义了两个特征模板</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588081673363.png" alt="1588081673363"></p><p>然后通过一个decoding算法来得到最后的联合结果：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588081713596.png" alt="1588081713596"></p><p>论文 [7] 任务是：自动内容提取：Automatic Content Extraction。包括实体识别和关系抽取。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588084823613.png" alt="1588084823613"></p><p>解决方案也是：单模型+Decoding算法</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588084916587.png" alt="1588084916587" style="zoom:50%;"></p><p>论文[12]的任务是实体关系抽取</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588141447625.png" alt="1588141447625"></p><p>先抽取实体，再进行关系分类。实体抽取模型和关系分类模型分别是一个字序列LSTM和一个依赖树结构的LSTM。</p><p>其中word embedding 和实体抽取模型得到的label embedding都用于关系分类模型的输入中。关系分类模型和实体抽取模型的隐藏层共享训练参数。</p><p>还做了消融实验：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588142769366.png" alt="1588142769366"></p><p>论文[13] </p><p>任务也是实体关系抽取</p><p>维护一个表，使用Beamsearch的方式来得到一个全局最优的序列</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588149130931.png" alt="1588149130931" style="zoom:67%;"></p><p>这个全局序列，就是通过$h_T$得到的。</p><p>使用LSTM学习全局上下文表示。使用三种基本的LSTM结构：1)从左到右的单词LSTM；2)从右到左的单词LSTM；3)从左到右的实体边界标签LSTM</p><p>然后两个上层的模块采用不同的输入，得到序列：实体检测的特征表示。首先，从三个基本的LSTM中提取六个特征向量。关系分类的特征表示，与实体检测类似，12个特征。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588148967560.png" alt="1588148967560" style="zoom:67%;"></p><p>论文[20]是实体抽取+情感分类任务。</p><p>两种解决方案：pipeline(joint ) or collapsed。</p><p>论文的integrat的策略就不重点关注了。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588156636191.png" alt="1588156636191" style="zoom: 67%;"></p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588156732935.png" alt="1588156732935" style="zoom:80%;"></p><p>论文[38]是方面情感分析的工作</p><p>通常以流水线方式完成此任务，首先执行aspect term提取，然后对提取的方面项进行情感预测。</p><p>论文提出的模型美其名曰“交互式”多任务学习，本质就是不共享参数的部分连来连去的。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588937239627.png" alt="1588937239627"></p><h3><span id="22-separate-learning-amp-joint-search">2.2 Separate Learning &amp; Joint Search</span></h3><p>论文[4] 也是词性标注任务：分词 + 标注。</p><p>联合训练不被使用时因为当时训练成本昂贵。</p><p>论文使用了两个独立的CRF来训练，得到字符级别的分词的序列标注，和字符级别的词性序列标注。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588072393312.png" alt="1588072393312" style="zoom:50%;"></p><p>然后构造了一个概率的框架，找到各自的最适合的最终结果。具体怎么概率的，就不care了，领悟Separate Learning &amp; Joint Search比较重要！~</p><p>论文[5]任务——观点提取：观点实体提取+观点关系分类</p><p>Opinion expressions: OOpinion targets: T；Opinion holders: H</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588079600410.png" alt="1588079600410" style="zoom: 67%;"></p><p>解决方案：</p><ol><li><p>将意见实体识别任务表示为一个序列标记问题，利用CRF学习序列赋值的概率;</p></li><li><p>将关系抽取问题作为两个二元分类问题的组合，利用逻辑回归训练分类器</p></li><li><p>loss函数被定义为一个线性组合，用不同的预测参数 λ 来平衡这两个模型的损失</p><script type="math/tex; mode=display">Score= λ∙Score_{entity}  +(1-λ)∙Score_{relation}</script></li></ol><h2><span id="4-辅助关系的多任务学习">4 辅助关系的多任务学习</span></h2><p>通过引入其他任务来帮助主要任务的效果提升。其他任务的数据来源大多是自身，即源数据的其他任务标签或者一些自监督的方法。</p><p>论文[9] 的任务是：在序列标注任务中，只有部分相关标签。</p><p>为了让模型充分利用数据，加入了语言模型的任务——学习预测序列中的下一个词，不依赖任何注释。这样能让模型学习更多语言特征，提高性能。</p><p>预测前一个字和后一个字都放在一个非线性映射后的softmax中：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588139994792.png" alt="1588139994792"></p><p>论文[14] 通过键盘敲击的延时来帮助 syntactic chunking、CCG supertagging 组合范畴语法超标注等任务；</p><p>模型也是很简单的，共同享有的3层LSTM，再加上两个任务，这两个任务是随机选一个来train的。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588151763065.png" alt="1588151763065"></p><p>论文[31] 是对话生成的任务</p><p>对话模型与一个Auto-Encoder模型共享Decoder部分的参数。一方面是改善对话数据不够用的问题，一方面提高对话生成的效果。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588750766979.png" alt="1588750766979"></p><p>论文[32] 也是对话生成的任务。</p><p>提出了一个SPACEFUSION模型，共同优化多样性和相关性。该模型通过利用新颖的正则化项实质上融合了序列到序列模型和Auto-Encoder模型的潜在空间。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588750991632.png" alt="1588750991632" style="zoom:67%;"></p><p>论文[33] 也是对话任务。</p><p>主要任务是，对话生成，如下图右半部分。辅助任务是，通过context得到 Author profiles，下图左半部分。</p><p>Author profile是建模得来的用户特征，age, gender, education, and location。这四个任务可以视为四个多分类任务，其中每个分类器将上下文作为输入以预测文本序列所属的类别。</p><p>Encoder部分是共享参数的，这也是多任务所在，简单的硬参数共享。</p><p>Decoder部分，预测得到的用户特征是直接作为额外输入加入进去的，另外经过用户特征转化过的context embedding 是作为decoder的初始状态的。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588754659240.png" alt="1588754659240"></p><p>论文[34] 是神经机器翻译问题，神经机器翻译（NMT）在纯净的域内文本上实现了卓越的性能，但是当面对充满错别字，语法错误和其他杂音的文本时，性能会急剧下降。 </p><p>任务主要是针对法语到英语的翻译。其中英语都是纯净数据，法语有clean的有noisy的。</p><p>多任务学习模型包括三部分输入，noisy source sentence、clean source sentence、target translation。通过这种加入了脏数据的多任务学习，提高翻译的效果，相当于加了一些噪声吧。</p><p>论文中涉及了一些关于这些平行语料的挑选、生成、筛选等，有兴趣可以去看看。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588934422820.png" alt="1588934422820" style="zoom:67%;"></p><p>论文[36] 是语篇连贯评估任务。</p><p>论文提出了一个多任务训练的层次神经网络，利用两个任务之间的归纳迁移，学习预测文档级连贯性得分(在网络的顶层)和词级语法角色(在底层)。</p><p>单任务在于文档整体语篇连贯性的分数，看似是个pipeline然而重点不在这里。这里的多任务在于优化了一个底层的二级任务，word-level的标签：语法结构。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588938425536.png" alt="1588938425536"></p><h2><span id="5-对抗多任务学习">5 对抗多任务学习</span></h2><p>对抗任务一般是应对 具有不同输入的多任务学习。</p><p>通过对抗任务，使参数共享的部分包含更多公共信息，并减少特定任务的信息。</p><p>论文[25]提出了一种用于POS的<strong>跨语言</strong>迁移学习模型。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588160497355.png" alt="1588160497355" style="zoom:50%;"></p><p>蓝色模块共享所有语言的参数，红色模块具有不同语言的参数。 紫色圆圈表示目标标签，这些目标标签针对不同的语言使用不同的参数预测，其输入是公共BLSTM和私有BLSTM的输出总和。 用三个用红色框表示的目标训练模型。</p><p>对抗训练在于梯度反转那里，让蓝色部分的参数尽量与语言无关。</p><p>中文分词有许多不同的分词标准。 现有的大多数方法都专注于提高每个标准的性能。<br>论文[27]集成来自多个异构细分标准的共享知识，为CWS提出对抗性多标准学习。 8个标准。<strong>跨语料</strong></p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588167224626.png" alt="1588167224626" style="zoom:50%;"></p><p>灰色方框是私有的LSTM layer，黄色方框是参数共享的LSTM layer。</p><p>对抗训练使得黄色的LSTM与输入无关，对抗训练的目标函数是确定是哪个criteria任务。</p><p>论文[28] </p><p>用于情感分析的训练数据在多个领域中都很丰富，而在其他领域则很少。 </p><p>将每个领域的描述符向量与句子一起输入进模型。描述符向量用于对抗训练。</p><p>基础模型，所有domain共享一套参数：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588169006697.png" alt="1588169006697" style="zoom:50%;"></p><p>多任务学习模型，共享一部分参数：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588169083409.png" alt="1588169083409" style="zoom:50%;"></p><p>最终模型，右上角进行对抗训练，左上角进行特定领域的情感分析。<strong>跨领域</strong></p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588169112756.png" alt="1588169112756" style="zoom: 50%;"></p><p>论文[37] 联合建模了Recognizing Question Entailment (RQE)and medical Question Answering (QA) 两个任务。<strong>跨任务</strong></p><p>对抗训练是为了使共享表示包含更多公共信息并减少特定任务的信息的混合。</p><p>原话是：an adversarial training strategy is introduced to separate the private features of each task from the shared representations. </p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588765746423.png" alt="1588765746423"></p><p>论文[35] 是<strong>多方言</strong>的POS任务，论文的主体模型也是额外的很常见的对抗训练。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588770178094.png" alt="1588770178094" style="zoom: 67%;"></p><p>论文[39] 是标题生成的领域适应任务。<strong>跨领域</strong></p><p>源域标注数据多，目标域数据少。</p><p>训练流程：1. 源域的标记数据经过encoder、decoder进行反向传播；2. 源域、目标域的数据经过encoder、domain classifier进行反向传播。（这个图画的真是反人类）</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588770813763.png" alt="1588770813763" style="zoom:67%;"></p><h2><span id="6-迁移学习的多任务">6 迁移学习的多任务</span></h2><p>论文[15]是为了解决“predict RST discourse trees”任务，RST( Rhetorical Structure Theory)训练数据少，因此使用了相关任务or可选的views的数据。</p><p>这个任务主要是说文档内句子之间都是有关系的，比如a中有条件关系，b中有MANNER-MEANS关系。</p><p>a. [The gain on the sale couldn’t be estimated] [until the “tax treatment has been determined.”]<br>b. [On Friday, Datuk Daim added spice to an otherwise unremarkable address on Malaysia’s proposed budget for 1990] [by ordering the Kuala Lumpur Stock Exchange “to take appropriate action immediately” to cut its links with the Stock Exchange of Singapore.]</p><p>利用的数据和任务，整体模型：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588153248395.png" alt="1588153248395"></p><p>与之前类似，共享训练参数，上层更改task类型。</p><p>论文[16]是中文分词模型，对模块进行多种任务的预训练，提升效果。</p><p>模型是基于transition的方法，整体如下：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588154075604.png" alt="1588154075604" style="zoom:67%;"></p><p>使用一些其他任务来进行一个pre-training，保存了一部分模型的参数：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588154133639.png" alt="1588154133639" style="zoom:50%;"></p><p>论文[17] 提出了一种通用的无监督学习方法，以提高seq2seq的准确性。 论文使用两个语言模型的预训练权重初始化seq2seq的编码器和解码器的权重，然后使用标记的数据进行微调。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588154371671.png" alt="1588154371671" style="zoom:67%;"></p><p>两个语言模型在各自语言语料上独自训练。</p><p>论文[18]是著名的ELMO，通过预训练的方式来完成多种任务，近年也不少了，还有BERT等。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588154989344.png" alt="1588154989344"></p><p><img src="NLP中的联合建模与多任务学习//1588155032471.png" alt="1588155032471"></p><p>论文[22] 是依存分析任务，为了解决低资源问题，采用多任务学习的方式。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588158729565.png" alt="1588158729565"></p><p>使用软参数共享的方法，两个模型都有自己的参数，并不共享，只需要在模型中修改loss加入关于两模型参数差别的正则化项，以使得参数相似化。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588158793425.png" alt="1588158793425" style="zoom:67%;"></p><p>一定程度少减少数据缺失的问题：</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588158886740.png" alt="1588158886740" style="zoom:67%;"></p><p>论文[23] 任务是训一个新加坡语的parser。</p><p>通过英语的parser为基础，使用新加坡语料得到。</p><p>论文中也完成了新加坡语POS的任务，通过的是一样的思路。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588159546362.png" alt="1588159546362" style="zoom: 67%;"></p><p>论文[24] 任务是神经机器翻译，seq2seq框架在大型数据场景中已显示有效，但在资源匮乏的语言中却没有那么有效。 </p><p>论文提出一种迁移学习方法，关键思想是首先训练一个高资源语言对（父模型），然后将一些学习到的参数传递给低资源语言对（子模型）以初始化和约束训练。</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588160106566.png" alt="1588160106566" style="zoom:33%;"></p><p>论文[26]是用标准的多任务学习来实现迁移学习。</p><p>使用具有大量标注的原任务（Penn Treebank上的POS标记）来改善目标任务的可用注释较少的性能（POS标记用于 微博）</p><p><img src="/2020/05/09/nlp-jin-jie/nlp-zhong-de-lian-he-jian-mo-yu-duo-ren-wu-xue-xi/1588166543661.png" alt="1588166543661" style="zoom:67%;"></p><h2><span id="reference">Reference</span></h2><p>[1] Zhang Y. Joint models for NLP[C]//Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts. 2018.</p><p>[2] Ng, Hwee Tou, and Jin Kiat Low.”Chinese part-of-speech tagging: One-at-a-time or all-at-once? word-basedor character-based?.” <em>EMNLP</em>.2004.</p><p>[3] Finkel, Jenny Rose, and Christopher D. Manning. “Joint parsing and named entity recognition.” <em>Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics</em>. Association for Computational Linguistics, 2009.</p><p>[4] Shi, Yanxin, and Mengqiu Wang. “A Dual-layer CRFs Based Joint Decoding Method for Cascaded Segmentation and Labeling Tasks.” <em>IJcAI</em>. 2007.</p><p>[5] Yang B, Cardie C. Joint inference for fine-grained opinion extraction[C]//Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2013: 1640-1649.</p><p>[6] Zhang Y, Clark S. Joint word segmentation and POS tagging using a single perceptron[C]//Proceedings of ACL-08: HLT. 2008: 888-896.</p><p>[7] Li Q, Ji H. Incremental joint extraction of entity mentions and relations[C]//Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2014: 402-412.</p><p>[8] Collobert R, Weston J, Bottou L, et al. Natural language processing (almost) from scratch[J]. Journal of machine learning research, 2011, 12(Aug): 2493-2537.  不太容易找，附上链接：<a href="http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf" target="_blank" rel="noopener">http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf</a></p><p>[9] Rei, Marek. “Semi-supervised Multitask Learning for Sequence Labeling.”, In proceedings of ACL (2017).</p><p>[10] Søgaard A, Goldberg Y. Deep multi-task learning with low level tasks supervised at lower layers[C]//Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2016: 231-235.</p><p>[11] Shi P, Teng Z, Zhang Y. Exploiting mutual benefits between syntax and semantic roles using neural network[C]//Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 2016: 968-974.</p><p>[12] Miwa M, Bansal M. End-to-end relation extraction using lstms on sequences and tree structures[J]. arXiv preprint arXiv:1601.00770, 2016.</p><p>[13] Zhang M, Zhang Y, Fu G. End-to-end neural relation extraction with global optimization[C]//Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 2017: 1730-1740.</p><p>[14] Plank B. Keystroke dynamics as signal for shallow syntactic parsing[J]. arXiv preprint arXiv:1610.03321, 2016.</p><p>[15] Braud C, Plank B, Søgaard A. Multi-view and multi-task training of RST discourse parsers[C]. 2016.</p><p>[16] Yang J, Zhang Y, Dong F. Neural word segmentation with rich pretraining[J]. arXiv preprint arXiv:1704.08960, 2017.</p><p>[17] Ramachandran P, Liu P J, Le Q V. Unsupervised pretraining for sequence to sequence learning[J]. arXiv preprint arXiv:1611.02683, 2016.</p><p>[18] Peters M E, Neumann M, Iyyer M, et al. Deep contextualized word representations[J]. arXiv preprint arXiv:1802.05365, 2018.</p><p>[19] Kiperwasser E, Ballesteros M. Scheduled multi-task learning: From syntax to translation[J]. Transactions of the Association for Computational Linguistics, 2018, 6: 225-240.</p><p>[20] Zhang M, Zhang Y, Vo D T. Neural networks for open domain targeted sentiment[C]//Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 2015: 612-621.</p><p>[21] Kunchukuttan A, Khapra M, Singh G, et al. Leveraging orthographic similarity for multilingual neural transliteration[J]. Transactions of the Association for Computational Linguistics, 2018, 6: 303-316.</p><p>[22] Duong L, Cohn T, Bird S, et al. Low resource dependency parsing: Cross-lingual parameter sharing in a neural network parser[C]//Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2015: 845-850.</p><p>[23] Wang H, Zhang Y, Chan G Y L, et al. Universal dependencies parsing for colloquial singaporean english[J]. arXiv preprint arXiv:1705.06463, 2017.</p><p>[24] Zoph B, Yuret D, May J, et al. Transfer learning for low-resource neural machine translation[J]. arXiv preprint arXiv:1604.02201, 2016.</p><p>[25] Kim J K, Kim Y B, Sarikaya R, et al. Cross-lingual transfer learning for pos tagging without cross-lingual resources[C]//Proceedings of the 2017 conference on empirical methods in natural language processing. 2017: 2832-2838.</p><p>[26] Yang Z, Salakhutdinov R, Cohen W W. Transfer learning for sequence tagging with hierarchical recurrent networks[J]. arXiv preprint arXiv:1703.06345, 2017.</p><p>[27] Chen X, Shi Z, Qiu X, et al. Adversarial multi-criteria learning for chinese word segmentation[J]. arXiv preprint arXiv:1704.07556, 2017.</p><p>[28] Liu Q, Zhang Y, Liu J. Learning domain representation for multi-domain sentiment classification[C]//Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). 2018: 541-550.</p><p>[29] Ganin Y, Lempitsky V. Unsupervised domain adaptation by backpropagation[J]. arXiv preprint arXiv:1409.7495, 2014.</p><p>[30] Sun T, Shao Y, Li X, et al. Learning Sparse Sharing Architectures for Multiple Tasks[J]. arXiv preprint arXiv:1911.05034, 2019.</p><p>[31] Luan Y, Brockett C, Dolan B, et al. Multi-task learning for speaker-role adaptation in neural conversation models[J]. arXiv preprint arXiv:1710.07388, 2017.</p><p>[32] Gao X, Lee S, Zhang Y, et al. Jointly optimizing diversity and relevance in neural response generation[J]. arXiv preprint arXiv:1902.11205, 2019.</p><p>[33] Yang M, Huang W, Tu W, et al. Multitask Learning and Reinforcement Learning for Personalized Dialog Generation: An Empirical Study[J]. IEEE Transactions on Neural Networks and Learning Systems, 2020.</p><p>[34] Zhou H, Li X, Yao W, et al. DUT-NLP at MEDIQA 2019: An Adversarial Multi-Task Network to Jointly Model Recognizing Question Entailment and Question Answering[C]//Proceedings of the 18th BioNLP Workshop and Shared Task. 2019: 437-445.</p><p>[35] Zalmout N, Habash N. Adversarial Multitask Learning for Joint Multi-Feature and Multi-Dialect Morphological Modeling[J]. arXiv preprint arXiv:1910.12702, 2019.</p><p>[36] Chen F, Chen Y Y. Adversarial Domain Adaptation Using Artificial Titles for Abstractive Title Generation[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 2197-2203.</p><p>[37] Zhou S, Zeng X, Zhou Y, et al. Improving Robustness of Neural Machine Translation with Multi-task Learning[C]//Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1). 2019: 565-571.</p><p>[38] He R, Lee W S, Ng H T, et al. An interactive multi-task learning network for end-to-end aspect-based sentiment analysis[J]. arXiv preprint arXiv:1906.06906, 2019.</p><p>[39] Farag Y, Yannakoudakis H. Multi-Task Learning for Coherence Modeling[J]. arXiv preprint arXiv:1907.02427, 2019.</p><h1><span id="end">End</span></h1>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP中的联合建模与多任务学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Pre-trained Models for Natural Language Processing》</title>
      <link href="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/"/>
      <url>/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>语言表示学习及其研究进展。</p><p>如何使预训练模型的知识适应下游任务</p><p>预训练模型未来研究的一些潜在方向</p><h2><span id="1-论文概览">1 论文概览</span></h2><p>此篇论文主要有如下贡献：</p><ol><li>对NLP相关的预训练模型，进行了全面的介绍。<br>包括背景知识，模型结构，预训练的任务，一些扩展。</li><li>从四个角度对现有预训练模型进行系统分类<br>1）word表示的类型；  2）预训练模型的结构；  3）预训练任务类型；  4）特定类型的方案或输入的扩展</li><li>收集了大量资源。<br>包括开源实现，可视化工具，语料库和论文清单。</li><li>对预训练模型的整体讨论。<br>讨论并分析现有预训练模型的局限性，建议了可能的未来研究方向。</li></ol><h2><span id="2-背景知识">2 背景知识</span></h2><h3><span id="21-语言表示学习">2.1 语言表示学习</span></h3><p>一个好的语言表示应该表达通用的先验，它不是特定于任务的，但是对于解决各种任务可能很有用。因此，一个好的表示法应该捕获隐藏在文本数据中的隐含语言规则和常识知识，例如词汇含义，句法结构，语义角色，甚至是语用学。</p><p>分布式表示的核心思想是通过低维实值向量来描述一段文本的含义。向量的每个维度没有相应的意义，而整体则代表一个具体的概念。 </p><p>word embeddings（词嵌入）有两种：非上下文嵌入和上下文嵌入。 它们之间的区别在于，单词的嵌入是否会根据其出现的上下文而动态变化。</p><p><img src="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/1585818716729.png" alt="1585818716729" style="zoom:50%;"></p><p>非上下文嵌入有两个主要限制。<br>第一个问题是嵌入是静态的。 单词的嵌入与上下文无关，始终是固定的。 因此，这些非上下文嵌入无法建模多义词。 第二个问题是未登录词（out-of-vocabulary）问题。</p><h3><span id="22-为什么需要预训练">2.2 为什么需要预训练</span></h3><p>大力出奇迹需要巨大的超多参数的模型，需要更大的数据集来完全训练模型参数并防止过度拟合。</p><p>由于注释成本极其昂贵，因此对于大多数NLP任务而言，构建大规模的标记数据集是一项巨大的挑战。</p><p>相反，大规模的未标记语料库相对容易构建。 为了利用巨大的未标记文本数据，我们可以首先从它们中学习良好的表示形式，然后将这些表示形式用于其他任务。 最近的研究表明，借助从大型无注释语料库中的预训练模型提取的表示形式，可以在许多NLP任务上显着提高性能。</p><p>预训练的特长总结如下：</p><ol><li><p>可以学习通用的语言表示形式，并帮助完成下游任务；</p></li><li><p>更好的模型初始化，可以带来更好的泛化性能并加快收敛速度；</p></li><li><p>可以将预训练视为一种正则化，以避免对小数据过度拟合。</p></li></ol><h3><span id="23-预训练模型的发展变化">2.3 预训练模型的发展变化</span></h3><p>第一代预训练模型旨在学习良好的word embeddings.</p><p>下游任务不再需要这些预训练模型模型本身，因此对于计算效率来说它们通常很浅，例如Skip-Gram和GloVe。 </p><p>这些经过预训练的嵌入可以捕获单词的语义，但它们不受上下文限制，无法捕获更高层次的文本概念，例如句法结构，语义角色，回指等。</p><p>第二代预训练模型关注于学习上下文的word embeddings. 例如CoVe, ELMo, OpenAIGPT and BERT.</p><p>不同于第一代预训练模型，第二代预训练模型在下游任务中仍然需要这些模型结构本身来表示特定上下文中的单词。此外，不像早期的word2vec只有skip-gram和cbow两种简单的任务，各种预训练任务也被提出使预训练模型能更好的学习。</p><h2><span id="3-预训练模型的分类">3 预训练模型的分类</span></h2><p>不同的预训练模型，主要取决于上下文encoders、预训练的任务类型，当然还有其对应的目的</p><h3><span id="31-神经上下文encoder类型">3.1 神经上下文Encoder类型</span></h3><p>大多数神经上下文编码器可分为三类：卷积模型，序列模型和基于图的模型。</p><p>一种更直接的方法是使用全连接的图对每两个单词的关系进行建模，然后让模型自己学习结构。 通常，连接权重是通过self-attention机制动态计算的，该机制会隐式指示单词之间的联系，并且因此可以建模长距离的依赖。一种成功的实现：Transformer。</p><p>可惜transformer由于其结构复杂且庞大，需要巨大的语料来训练，否则容易过拟合。这也是为什么要预训练的原因。</p><p><img src="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/1585819538116.png" alt="1585819538116" style="zoom: 50%;"></p><h3><span id="32-预训练任务">3.2 预训练任务</span></h3><p>预训练任务可以概括为三类：监督学习，无监督学习和自监督学习。</p><p>其中监督学习就是从标注数据中学习从input到output的映射函数；无监督学习是在无标注数据中找到一些固有的知识，例如聚类、密度或潜在的表述等；自监督学习，是去预测输入的任意部分，accoding to输入的其他部分，例如masked语言模型。</p><p><img src="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/1587989122954.png" alt="1587989122954"></p><h4><span id="321-语言建模任务language-modeling-lm">3.2.1 语言建模任务(Language Modeling —— LM)</span></h4><p>语言建模任务是最常用的。</p><p>给定一个句子的前 t-1 个word，预测第 t 个word:</p><script type="math/tex; mode=display">p\left(x_{1: t}\right)=\prod_{t=1}^{T} p\left(x_{t} | x_{0: t-1}\right)</script><p>$x_0$一般是类似$“_BOS_”$这样的字符。</p><p>其中，条件概率$p\left(x_{t} | x_{0: t-1}\right)$是通过对上文进行建模，得到的一个词汇的分布。</p><p>单向语言模型的一个缺点是每个token的表示仅编码其左边的上文及其本身。更好的文本上下文表示应从两个方向对上下文信息进行编码。 一种改进的解决方案是双向语言模型，由两个单向LM组成。</p><h4><span id="322-掩码语言建模任务-masked-language-modelingmlm">3.2.2 掩码语言建模任务 (Masked Language Modeling——MLM)</span></h4><p>类似于完形填空（Cloze）任务，MLM首先从输入语句中用类似[MASK]的字符掩住一些token，然后训练模型来预测标记所掩盖的标记。 </p><p>这种方法将在训练阶段和fine-tuning阶段之间产生不匹配，因为在fine-tuning阶段不会出现[MASK]  token。 为了解决这个问题，模型BERT在80％的时间内使用特殊的[MASK] ，在10％的时间内使用随机token，在10％的时间内使用原始token进行屏蔽。（来自BERT）</p><p>将屏蔽的序列馈送到神经编码器，该编码器对应token的隐藏单元进一步馈入softmax分类器，以预测屏蔽的token。BERT中only predict the masked words rather than reconstructing the entire input。或者，可以使用seq2seq的结构，屏蔽序列输入进编码器，解码器以自回归的方式顺序生成masked token。</p><p>当然还有很多加强版的MLM任务。</p><h4><span id="323-排列语言建模-permuted-language-modeling-plm">3.2.3 排列语言建模 (Permuted Language Modeling —— PLM)</span></h4><p>PLM一定程度上也可以解决 使用了一些类似于[MASK]字符后，导致预训练阶段和fine-truning阶段的差异问题。</p><p>从所有可能的排列中随机抽取排列。 然后，将这个随机序列中的某些token选择为目标，然后训练模型以预测这些目标。 此排列不会影响序列的自然位置，而仅定义token预测的顺序。<br>比如给定句子（w1, w2, w3, w4） 和（1,2,3,4）的一个Permutation，比如说（2,4,3,1），以及要预测的词w3，则根据词w2,w4来预测w3。因为在Permutation中3的前面是2,4。即目标词的context为目标词在Permutation中之前的词。</p><h4><span id="324-降噪自编码器denoising-autoencoder-dae">3.2.4 降噪自编码器（Denoising Autoencoder —— DAE)</span></h4><p>借助于DAE的原理，通过对输入序列做一系列的操作来加入“噪声”，然后训练模型去除加入的噪声来恢复原始的输入序列</p><p> 有几种破坏文本的方法：</p><ol><li>屏蔽token：从输入中随机采样token，并将其替换为[MASK]元素；</li><li>删除token：从输入中随机删除token。与token屏蔽不同，该模型需要确定缺失输入的位置；</li><li>文本填充：扩充文本并替换为单个[MASK]标记；</li><li>打乱句子顺序</li><li>文档旋转。选择一个token，将token之后的放入开头。</li></ol><h4><span id="325-对照学习假设contrastive-learning-ctl">3.2.5 对照学习假设（Contrastive Learning —— CTL)</span></h4><p>一些观察到的文本对在语义上比随机取样的文本更为接近。</p><script type="math/tex; mode=display">\mathbb{E}_{x, y^{+}, y^{-}}\left[-\log \frac{\exp \left(s\left(x, y^{+}\right)\right)}{\exp \left(s\left(x, y^{+}\right)\right)+\exp \left(s\left(x, y^{-}\right)\right)}\right]</script><p>CTL背后的想法是“通过比较学习”。 与LM相比，CTL通常具有较少的计算复杂性，因此是PTM的理想替代训练标准。</p><h3><span id="33-预训练模型的分类">3.3 预训练模型的分类</span></h3><p>从以下几个方面对预训练模型进行分类：</p><ol><li>表示类型：根据用于下游任务的表示，可以将PTM分为非上下文模型和上下文模型。</li><li>网络结构：包括LSTM，Transformer encoder，Transformer decoder和完整的Transformer。  </li><li>预训练任务类型</li><li>扩展：针对各种场景而设计的预训练模型。包括加入丰富知识的PTM，多语言或特定语言的PTM，多模型PTM，特定领域的PTM和压缩的PTM。 </li></ol><p><img src="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/1587989390793.png" alt="1587989390793"></p><p><img src="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/1587989086246.png" alt="1587989086246"></p><h2><span id="4-预训练模型的拓展">4 预训练模型的拓展</span></h2><p>Knowledge-Enriched PTMs</p><p>Multilingual and Language-Specific PTMs</p><p>Multi-Modal PTMs</p><p>Domain-Specific and Task-Specific PTMs</p><p>Model Compression</p><ul><li><strong>模型剪枝</strong>，即删除不重要的参数</li><li><strong>权重量化</strong>，即使用少量的位去表示参数</li><li><strong>参数共享</strong>，即相同类型的层间进行参数共享</li><li><strong>知识蒸馏</strong>，用知识蒸馏的方式将老师网络中学到的知识转移到容量较小的学生网络中</li></ul><h2><span id="5-adapting-ptms-to-downstream-tasks">5 Adapting PTMs to Downstream Tasks</span></h2><p>尽管PTM可以从大型语料库中获取通用语言知识，但是如何有效地将其知识适应下游任务仍然是关键问题。</p><h3><span id="51-迁移学习">5.1 迁移学习</span></h3><p><img src="/2020/04/27/nlp-jin-jie/pre-trained-models-for-natural-language-processing/1587989804104.png" alt="1587989804104" style="zoom:50%;"></p><h3><span id="52-how-to-transfer">5.2 how to transfer?</span></h3><p>Choosing appropriate pre-training task, model architecture and corpus</p><p>Choosing appropriate layers</p><p>To tune or not to tune?</p><h3><span id="53-fine-tuning-strategies">5.3 Fine-Tuning Strategies</span></h3><h2><span id="reference">Reference</span></h2><p><a href="https://arxiv.org/abs/2003.08271" target="_blank" rel="noopener">https://arxiv.org/abs/2003.08271</a></p><p><a href="https://blog.csdn.net/Forlogen/article/details/105475933" target="_blank" rel="noopener">https://blog.csdn.net/Forlogen/article/details/105475933</a></p><p><a href="https://zhuanlan.zhihu.com/p/69989155" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/69989155</a></p><p><a href="https://www.cnblogs.com/shona/p/12546820.html" target="_blank" rel="noopener">https://www.cnblogs.com/shona/p/12546820.html</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Pre-trained Models for Natural Language Processing》 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker</title>
      <link href="/2020/04/23/gong-cheng-bu-shu-xiang-guan/docker/"/>
      <url>/2020/04/23/gong-cheng-bu-shu-xiang-guan/docker/</url>
      
        <content type="html"><![CDATA[<p>云笔记里还有一些后续补充：待搬运</p><h2><span id="1-docker-概念扫盲">1 Docker 概念扫盲</span></h2><p>Docker技术的三大核心概念，分别是：</p><ul><li><p>镜像（Image）   装有某软件的模板</p></li><li><p>容器（Container）  镜像创建的应用实例</p></li><li><p>仓库（Repository） 存放镜像文件的仓库</p></li></ul><p>从仓库（一般为DockerHub）下载（pull）一个镜像，Docker执行run方法得到一个容器，用户在容器里执行各种操作。Docker执行commit方法将一个容器转化为镜像。Docker利用login、push等命令将本地镜像推送（push）到仓库。其他机器或服务器上就可以使用该镜像去生成容器，进而运行相应的应用程序了。</p><h2><span id="2-docker-安装">2 Docker 安装</span></h2><h3><span id="21-docker-for-linux安装">2.1 Docker for linux安装</span></h3><p>使用安装脚本自动安装</p><pre><code>curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun</code></pre><p>启动Docker服务</p><pre><code>systemctl start dockersudo systemctl restart docker</code></pre><p>查看Docker版本和信息</p><pre><code>docker version docker info</code></pre><h3><span id="22-docker-for-windows安装">2.2 Docker for windows安装</span></h3><p>windows端这边使用了Docker Desktop for Windows，体验尚可。</p><p><img src="/2020/04/23/gong-cheng-bu-shu-xiang-guan/docker/image-20201015155753590.png" alt="image-20201015155753590"></p><h2><span id="3-docker-for-linux-配置">3 Docker for Linux 配置</span></h2><h3><span id="31-内网代理服务器的docker设置">3.1 内网代理服务器的docker设置</span></h3><p>服务器是内网，之前的想法是通过配置内网代理来进行各种docker的pull和push的操作。</p><p>流程为：</p><ol><li><p>内网代理配置，windows端可以使用CCproxy，注意：设置/高级/网络 处取消勾选’禁止局域网外部用户’</p><pre><code>vim ~/.bashrc添加如下：（10.61.3.208为可外网的主机IP）export http_proxy=http://10.61.3.208:808export https_proxy=http://10.61.3.208:808然后source  ~/.bashrc测试wget baidu.com</code></pre></li></ol><ol><li><p>设置docker代理</p><p>不设置的话会出现</p><pre><code>[dcy@localhost docker_about]$ docker pull nginxUsing default tag: latestError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</code></pre><p>设置方法</p><pre><code>sudo mkdir -p /etc/systemd/system/docker.service.dvim /etc/systemd/system/docker.service.d/http-proxy.conf添加：目前是这个：（）[Service]Environment=&quot;HTTP_PROXY=http://10.61.3.208:808/&quot; &quot;HTTPS_PROXY=https://10.61.3.208:80/&quot; 重启生效sudo systemctl daemon-reloadsudo systemctl restart dockersystemctl show --property=Environment docker</code></pre></li><li><p>镜像源<br>配置完还是报错</p><pre><code>[dcy@localhost docker_about]$ docker pull nginxUsing default tag: latestError response from daemon: Get https://registry-1.docker.io/v2/: proxyconnect tcp: net/http: TLS handshake timeout</code></pre><p>修改镜像源</p><pre><code>daocloud官网上有自动脚本curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io阿里云官网上也有</code></pre></li><li><p>继续配置</p><p>还是报错</p><pre><code>[dcy@localhost docker_about]$ docker pull nginxUsing default tag: latestlatest: Pulling from library/nginxImage docker.io/library/nginx:latest uses outdated schema1 manifest format. Please upgrade to a schema2 image for better future compatibility. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/bb79b6b2107f: Retrying in 16 seconds 111447d5894d: Retrying in 16 seconds a95689b8e6cb: Retrying in 16 seconds 1a0022e444c2: Waiting 32b7488a3833: Waiting</code></pre><p>修改</p><pre><code>vim /etc/resolv.confnameserver 8.8.8.8 #添加一条这个</code></pre><p>报的错不一样了</p><pre><code>[dcy@localhost docker_about]$ sudo docker pull nginxUsing default tag: latestlatest: Pulling from library/nginxbb79b6b2107f: Pulling fs layer 111447d5894d: Pulling fs layer a95689b8e6cb: Pulling fs layer 1a0022e444c2: Waiting 32b7488a3833: Waiting Get https://registry-1.docker.io/v2/: proxyconnect tcp: net/http: TLS handshake timeout没有error前缀了呢。。。</code></pre></li></ol><p>整体流程没错的，目测是服务器的原因吧。。。</p><h3><span id="32-内网harbor">3.2 内网harbor</span></h3><p>放弃挣扎了，使用harbor管理docker镜像（内网就可以访问，甚至都不需要代理了。</p><p>步骤：本地host，docker tar包，注册账号，login，pull</p><h2><span id="4-linux操作">4 Linux操作</span></h2><h3><span id="41-操作简记">4.1 操作简记</span></h3><pre><code>docker search centos  # 查看centos镜像是否存在docker pull centos     # 利用pull命令获取镜像docker images    # 查看当前系统中的images信息</code></pre><p>利用镜像启动容器后修改后再提交</p><pre><code>[root@xxx ~]# docker run -it centos:latest /bin/bash    # 启动一个容器[root@72f1a8a0e394 /]#    # 这里命令行形式变了，表示已经进入了一个新环境[root@72f1a8a0e394 /]# git --version    # 此时的容器中没有gitbash: git: command not found[root@72f1a8a0e394 /]# yum install git    # 利用yum安装git......[root@72f1a8a0e394 /]# git --version   # 此时的容器中已经装有git了git version 1.8.3.1exitdocker ps -a# 将容器转化为一个镜像，即执行commit操作，完成后可使用docker images查看：# -m 说明   -a 用户账号 容器id   目标镜像的用户名仓库名和tagdocker commit -m &quot;centosbal&quot; -a &quot;qixianhu&quot; 72f1a8a0e394 xianhu/centos:gitdocker images此时Docker引擎中就有了我们新建的镜像xianhu/centos:git，此镜像和原有的CentOS镜像区别在于多了个Git工具。此时我们利用新镜像创建的容器，本身就自带git了。[root@xxx ~]# docker run -it xianhu/centos:git /bin/bash[root@520afc596c51 /]# git --versiongit version 1.8.3.1[root@520afc596c51 /]# exit</code></pre><h3><span id="42-python部署的一个demo">4.2 python部署的一个demo</span></h3><p>Dockerfile </p><pre><code># 指定基础镜像FROM python:3.7# 指定镜像的维护者MAINTAINER jackfrued &quot;jackfrued@126.com&quot;# 设置工作目录WORKDIR /root# 从版本控制克隆代码RUN git clone https://gitee.com/jackfrued/apidemo.git# 设置工作目录WORKDIR /root/apidemo# 安装项目依赖项RUN pip install -r requirements.txt -i https://pypi.doubanio.com/simple/# 为启动脚本添加执行权限RUN chmod 755 start.sh# 容器启动时要执行的命令ENTRYPOINT [&quot;./start.sh&quot;]# 暴露端口EXPOSE 8000</code></pre><p>build</p><pre><code>docker build -t &quot;jackfrued/myapp&quot; .</code></pre><p>部署</p><pre><code>docker run -d -p 8000:8000 --name myapp jackfrued/myapp:latest</code></pre><p>效果</p><p><img src="/2020/04/23/gong-cheng-bu-shu-xiang-guan/docker/image-20201015161617898.png" alt="image-20201015161617898"></p><h2><span id="reference">Reference</span></h2><p><a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/centos-docker-install.html</a></p><p><a href="https://www.cnblogs.com/wangkun122/p/13036432.html" target="_blank" rel="noopener">https://www.cnblogs.com/wangkun122/p/13036432.html</a></p><p><a href="https://www.runoob.com/docker/centos-docker-install.html" target="_blank" rel="noopener">https://www.runoob.com/docker/centos-docker-install.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 工程部署相关 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本生成之VAE</title>
      <link href="/2020/04/15/shen-du-xue-xi-yu-nlp-ji-chu/vae/"/>
      <url>/2020/04/15/shen-du-xue-xi-yu-nlp-ji-chu/vae/</url>
      
        <content type="html"><![CDATA[<p>VAE的论文光看摘要就看的脑壳痛，网上很多关于VAE的资料都花里胡哨的，讲的特别乱。一是任务定义和模型的目的都没讲清楚就开始balabala了，二是只乱堆公式让人一头雾水，我来整理一下吧。（真是脑壳痛）</p><p>有部分词句是从文末的引用摘过来的，侵删~</p><h2><span id="1-自编码器-auto-encoder">1. 自编码器 (Auto-Encoder)</span></h2><p>想搞明白VAE是啥，得先知道Auto-Encoder是啥。</p><p>自编码器（Auto-Encoder），就是一种利用反向传播算法使得输出值等于输入值的神经网络。</p><p>它现将输入压缩成潜在空间表征，然后将这种表征重构为输出。Auto-Encoder的目的在于训练一个神经网络，用于信号降维，同时降维之后的信号能够很好地重建原信号。Auto-Encoder的核心价值就在于这种经编码器压缩后的潜在空间表征，以此为基础开展一些工作。</p><p>AE可以只能做到input=output，但是无法生成额外的类似input的其他数据。</p><p>VAE的流程与Auto-Encoder一样，都是重构文本。但目的不一样，VAE想要得到样本的概率分布，生成所有其他可能的样本。</p><h2><span id="2-vaeauto-encoding-variational-bayes">2. VAE(Auto-Encoding Variational Bayes)</span></h2><h3><span id="21-vae的目的与假设">2.1 VAE的目的与假设</span></h3><p>有一批数据样本$X:\{X_1,…,X_n\}$，我们想要得到 $p(X)$ 的概率分布，对$p(X)$进行采样从而生成所有样本。</p><p>但是$p(X)$很难直接得到，因此我们退而求其次，对于每一个$X_i$，我们假设有一个专属于$X_i$的分布$P(Z_i|X_i)$，如果能得到这个分布$P(Z_i|X_i)$，然后从中采样出来的$Z_i$还原为$X$，也能达到我们的目的。</p><p><img src="/2020/04/15/shen-du-xue-xi-yu-nlp-ji-chu/vae/2584918486.png" alt="事实上，vae是为每个样本构造专属的正态分布，然后采样来重构" style="zoom: 50%;"></p><p>对于这些分布$P(Z_i|X_i)$，我们假设其为正态分布，那么求得这个分布就是求得正态分布的均值和方差，采样就是从这个正态分布中随机采样。</p><p><img src="/2020/04/15/shen-du-xue-xi-yu-nlp-ji-chu/vae/v2-a3f264a40db57e010b7ebf0253198726_1440w.jpg" alt="img" style="zoom:50%;"></p><p>那么VAE就很明确了，第一部分是通过样本求得样本对应的正态分布，第二部分是通过正态分布中采样得到的$Z_i$，求得生成样本。</p><p>如果使用深度学习，那么我们需要拟合两个模型，第一部分是可以得到$P(Z|X_i)$的$f_1(X_i)$，目标函数是让求得的Z所属的分布接近一个正态分布；第二部分是$g(X_i’|Z)$，目标函数是生成的样本与真实样本之间的“距离”。</p><h3><span id="22-得到正态分布与隐含变量">2.2 得到正态分布与隐含变量</span></h3><ol><li><p>假设<script type="math/tex">p(Z|X_i)</script>是正态分布时，这时候对应每一个 $X_i$都配上了一个对应的正态分布。</p></li><li><p>构建两个神经网络去得到他们具体的分布，$μ_i=f_1(X_i)$，$log{σ_i^2}=f_2(X_i) $，求得均值和方差，即求得了要采样出$Z_k$的分布。</p></li><li><p>采样是不可导的，解决办法就是模拟采样的结果。采样一个服从$N(\mu,\sigma^2)$分布的$Z_i$ ，相当与从标准正态分布中采样的结果通过参数变换（均值和方差变换）得到。这就是原论文里提到的重参数技巧。</p><p><img src="/2020/04/15/shen-du-xue-xi-yu-nlp-ji-chu/vae/v2-924ea15ee91a6bcca5c63c870098e577_720w.png" alt="img"></p></li></ol><h3><span id="23-重构样本过程的学习">2.3 重构样本过程的学习</span></h3><ol><li>得到$Z_i$的采样结果后，再经过一个生成模型$X_i^{+}=g(Z_i)$，这个生成模型的主要损失函数是最小化$X_i’$与$X_i$的差别</li><li>最后将$Z_k$的分布$N(\mu,\sigma^2)$ 趋近标准正态分布，方法是在原损失函数中加入了额外的loss项，两个分布的KL散度。</li></ol><p>最终的损失函数：</p><p>​                                                        <script type="math/tex">\mathcal{L}(x, \hat{x})+\sum_{j} K L\left(p(z | x) \| N(0 , 1)\right)</script>        </p><p>在重构过程中，从正态分布采集样本（而不使用固定的样本）的策略是作为噪声，赋予生成结果一定的随机性。然后让$p(Z|X)$逼近标准正态分布，防止了噪声为0。</p><p>如果所有的$p(Z|X)$都接近正态分布的话，$p(Z)$也是接近正态分布的：</p><p>​                        <script type="math/tex">p(Z)=\sum_{X} p(Z | X) p(X)=\sum_{X} \mathcal{N}(0, I) p(X)=\mathcal{N}(0, I) \sum_{X} p(X)=\mathcal{N}(0, I)</script></p><p>这样就可以放心的通过标准正态分布的采样，去生成。</p><h2><span id="3-cvaeconditional-auto-encoding-variational-bayes">3. CVAE(Conditional Auto-Encoding Variational Bayes)</span></h2><h2><span id="reference">Reference</span></h2><p><a href="https://arxiv.org/pdf/1312.6114.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1312.6114.pdf</a></p><p><a href="https://www.bilibili.com/video/BV15E411w7Pz?from=search&amp;seid=10893615105081729770" target="_blank" rel="noopener">https://www.bilibili.com/video/BV15E411w7Pz?from=search&amp;seid=10893615105081729770</a></p><p><a href="https://zhuanlan.zhihu.com/p/34998569" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34998569</a></p><p><a href="https://www.sohu.com/a/287128531_455817" target="_blank" rel="noopener">https://www.sohu.com/a/287128531_455817</a></p><p><a href="https://zhuanlan.zhihu.com/p/62139750" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/62139750</a></p><p><a href="https://blog.csdn.net/weixin_40955254/article/details/82315909" target="_blank" rel="noopener">https://blog.csdn.net/weixin_40955254/article/details/82315909</a></p><p><a href="https://www.jeremyjordan.me/variational-autoencoders/" target="_blank" rel="noopener">https://www.jeremyjordan.me/variational-autoencoders/</a></p><p><a href="https://spaces.ac.cn/archives/5253" target="_blank" rel="noopener">https://spaces.ac.cn/archives/5253</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本生成之VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>个人生产力工具推荐</title>
      <link href="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/"/>
      <url>/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/</url>
      
        <content type="html"><![CDATA[<p>经常会搜集一些常用的工具，在工作学习的时候会带来一些便利，也经常安利给周围的朋友。趁着清明假期，整理一下常用的几个，不涉及具体的使用方法和破解方法，只介绍应用的场景和实用的功能，希望能给大家带来一些帮助吧。</p><p>目录如下，防止耽误大家时间：</p><p>[TOC]</p><h3><span id="1-美化">1. 美化</span></h3><p>在工作环境的舒适方面，我从来都是不遗余力的，首先尽可能有一个干净的桌子，放上电脑鼠标等东西，再加上个机械键盘和降噪耳机，就可以舒舒服服的开始工作了。</p><p>比如：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586166830210.png" alt="1586166830210" style="zoom: 33%;"></p><h4><span id="11-让人极其赏心悦目的壁纸软件wallpaper-engine">1.1 让人极其赏心悦目的壁纸软件——wallpaper engine</span></h4><p>这是个壁纸软件，steam上18块钱，里面的创意工坊里有大量的动态静态壁纸，颜值颇高，非常推荐。</p><p>集显占用不会太高，只是会占用一丢丢内存，轻薄笔记本完全可以带的动。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586166100829.png" alt="1586166100829" style="zoom: 33%;"></p><h4><span id="12-比startisback还好用的任务栏透明软件translucenttb">1.2 比StartIsBack还好用的任务栏透明软件——translucentTB</span></h4><p>一个让任务栏透明的工具，可以做到如下的效果：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586166264896.png" alt="1586166264896" style="zoom: 33%;"></p><p>还有个软件叫：StartIsBack。也可以做到如上效果，还可以让windows弹窗更换为不同风格，例如win7变win10。也很好用，但是破解较麻烦，而translucentTB在windows搜索栏直接搜索，跳转到windows的应用商城就可以下载，功能简单，无需破解，因此更为推荐。</p><h4><span id="13-各种电脑管家功能里唯一有用的硬件监控软件trafficmonitor">1.3 各种电脑管家功能里唯一有用的硬件监控软件——TrafficMonitor</span></h4><p>看到我的任务栏那里显示了 网速、cpu占用、内存占用的地方吗，那也是一个工具，比各大电脑管家内存占用小很多（当然我也没测试过），TrafficMonitor就可以做到这个，还可以通过设置进行不同的显示。</p><h4><span id="14-任务栏打上美滋滋的生活~">1.4 任务栏打上“美滋滋的生活~”？</span></h4><p>可能任务栏上的“美滋滋的生活~”大家也很感兴趣。</p><p>有两种方式在任务栏上添加自定义文字，一是修改时间那边的格式来添加，可以百度搜索“windows10 任务栏添加自定义文字”类似的词条，可以查到具体操作步骤。</p><p>还有一种方法是我这种效果，1、桌面新建文件夹，名字为你想显示 在任务栏的文字，如“美滋滋的生活~”；2、右键任务栏，工具栏，新建工具栏，选择你刚建的文件夹；3、删除刚建的文件夹。ok了。</p><h3><span id="2-日常使用工具">2. 日常使用工具</span></h3><h4><span id="21-简洁好用的日程软件滴答清单">2.1 简洁好用的日程软件——滴答清单</span></h4><p>一个日程提醒软件，最主要的是全平台可用且简洁好用，windows、android、ios。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586167881662.png" alt="1586167881662" style="zoom: 33%;"></p><h4><span id="22-不用说大家都懂的笔记软件有道云笔记">2.2 不用说大家都懂的笔记软件——有道云笔记</span></h4><p>日常的记录，大多通过有道云笔记来做的，也是全平台可用，很方便。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168034210.png" alt="1586168034210" style="zoom: 33%;"></p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168089603.png" alt="1586168089603" style="zoom: 33%;"></p><h4><span id="23-随手一记的便利贴sticky-notes">2.3 随手一记的便利贴——Sticky Notes</span></h4><p>电脑前的你，想随手记点东西，只需要在任务栏一点，就可以调出来这个小工具。他可以悬浮在桌面上，就像是书籍上的便利贴一样的东西。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168235990.png" alt="1586168235990"></p><h3><span id="3-代码-写作工具">3. 代码、写作工具</span></h3><h4><span id="31-最好用的markdown工具typora">3.1 最好用的Markdown工具——typora</span></h4><p>markdown真的是个好东西，尤其是有typora这种软件的加持，更是好用的不行。</p><p>它可以设置一番变成本地文本文档、word等工具的非常好的替代品：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168678508.png" alt="1586168678508" style="zoom:50%;"></p><p>在涉及到公式、各级标题的时候，格外的好用：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168731554.png" alt="1586168731554" style="zoom: 50%;"></p><p>markdown的编辑内容可以很好的移植到word、pdf、个人博客、微信公众号、知乎等地方，除了图片的存储等极个别几个点之外都很方便。具体的功能和使用就不详细介绍了。</p><h4><span id="32-不仅仅是代码或许可以用来备份同步文档的github-desktop">3.2 不仅仅是代码或许可以用来备份同步文档的Github Desktop</span></h4><p>github是程序员必备的工具之一，但是自从其开放了个人用户private权限后，完全可以将其当成云盘使用，也可以轻轻松松对文档尽心版本的更迭。其PC端也很简易好用，比命令行方式好入门很多，可以尝试其当做备份和文档更迭的工具，比如毕业论文、精心制作的ppt等等。</p><p>我的博客文件都是用github备份的23333</p><h4><span id="33-史上最好用的pdf软件adobe-acrobat-pro-dc">3.3 史上最好用的pdf软件——Adobe Acrobat pro DC</span></h4><p>这是目前我用过的最好用的pdf软件，秒杀国内一众 福昕阅读器类似的软件，只是需要破解or付费。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169010616.png" alt="1586169010616"></p><p>福昕阅读器上的收费项目，比如pdf页面的无水印拼接，页面增删等，这个软件都可以做到：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169083740.png" alt="1586169083740" style="zoom:67%;"></p><h4><span id="34-还在手打latex截图转公式的好帮手mathpix-snipping-tool">3.4  还在手打latex？截图转公式的好帮手Mathpix Snipping Tool</span></h4><p>可以将带有公式的图片（一般是去截图）转变为markdown or latex 的代码，识别率很高。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169235595.png" alt="1586169235595"></p><p>缺点就是每月只有50次免费使用的机会，使用频率高的用户需要付费，当然本没钱的学（白）生（嫖）党是不会告诉你多注册几个账号是完全可行且方便的。</p><h4><span id="35-打开文档找这仨notepad-editplus-atom">3.5 打开文档找这仨：Notepad++、Editplus、Atom</span></h4><p>本地文本文档打开很慢？现在谁还会用本地文本文档呢，一般的文档可以使用notepad++，完美打开各种文本文件，还可以标识代码：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169472038.png" alt="1586169472038"></p><p>如果是打开数据文件呢，动辄好几G的那种？使用editplus！</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169521998.png" alt="1586169521998"></p><p>如果是更大的呢，或者是更小众的文件格式呢？宇宙编辑器atom了解一下！</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169588587.png" alt="1586169588587"></p><h3><span id="4-论文工具">4. 论文工具</span></h3><p>这块关注一些论文管理和论文阅读的工具</p><h4><span id="41-界面简洁笔记好看的谷歌云-kami">4.1 界面简洁笔记好看的——谷歌云 + kami</span></h4><p>之前一段时间我使用谷歌云备份文件，kami来阅读，界面简洁赏心悦目。但是需要科学上网。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170038424.png" alt="1586170038424"></p><h4><span id="42-论文的管理同步还是最重要zotero坚果云papership">4.2 论文的管理同步还是最重要——Zotero+坚果云+Papership</span></h4><p>后来是使用zotero来进行论文的整理，在chrome中加载好了的论文可以一键加入到zotero，论文的本体文件备份到坚果云上，在ipados端使用papership来进行阅读。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170143504.png" alt="1586170143504"></p><p>ipad上是这样的，也可以保存到ipad的一些编辑软件比如notablity简单的做一些笔记：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170265638.png" alt="1586170265638"></p><p>具体的使用教程，我这里收藏的，不保证一直存在：<a href="https://zhuanlan.zhihu.com/p/28325366" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28325366</a> （侵删）</p><h4><span id="43-神一样的论文翻译软件copytranslator">4.3 神一样的论文翻译软件——Copytranslator</span></h4><p>可能阅读文章的各位有不少是和我一样的英语菜鸡，时不时需要大段翻译英文论文，google翻译是真的好用，然鹅论文里每行后面都有一个回车就很蠢。</p><p>我在刚开始是粘贴到word文档，然后用空字符全部替换掉回车，再粘贴到google翻译。现在想想真是好难。</p><p>copytranslator，可以自动去掉空格，可以自动粘贴，甚至可以自动复制，，可以自己选翻译的引擎。。</p><p>反正翻译论文小能手就是它了！~</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586172273280.png" alt="1586172273280"></p><h3><span id="5-chrome插件">5. chrome插件</span></h3><h4><span id="51-忍不住下载但是很少使用的webtime-tracker">5.1 忍不住下载但是很少使用的——Webtime Tracker</span></h4><p>放着玩，可以看你浏览器在哪个网站花的时间多。</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170505543.png" alt="1586170505543"></p><h4><span id="52-github小帮手sourcegraph">5.2 Github小帮手——Sourcegraph</span></h4><p>github在线阅读代码插件，比在原github页面跳转快的多。</p><p>例如打开github的一个代码库</p><p>你想仔细的阅读其中的几个代码文件，甚至想比较着来看，你就可以点击这个插件，然后出现一个这个：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170690302.png" alt="1586170690302"></p><p>点击后就跳转到插件所在的网站：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170739733.png" alt="1586170739733"></p><p>我相信看到这里你就会去添加插件了。</p><h4><span id="53-为拖延症晚期拔掉氧气管onetab">5.3 为拖延症晚期拔掉氧气管——OneTab</span></h4><p>因为你手头上的工作所以打开着很多网站，但是要先去干别的工作开别的网页，或者需要临时关闭电脑，或者要关闭一下浏览器去开别的？那这个插件很适合你</p><p>现在一堆的baidu.com等待你去看：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171115733.png" alt="1586171115733"></p><p>点击插件会全部关闭，等到下次打开的时候：</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171140126.png" alt="1586171140126"></p><p>是不是感觉进一步加剧了你的拖延症哈哈哈哈</p><h4><span id="54-不要登录真的相信我版页面双语对照翻译插件彩云小译">5.4 不要登录真的相信我版页面双语对照翻译插件——彩云小译</span></h4><p>chorme的页面翻译不知道你使用过没有呢？将页面内的文字全部翻译一下翻译成别的语言，之前的语言就没了。</p><p>而彩云小译是将页面保留，在下面或者旁边添加翻译，是不是感觉很贴心？</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171286930.png" alt="1586171286930"></p><p>补充一句，不要登录，不然使用次数会变的有限 /真哭笑不得</p><h4><span id="55-不知道好不好用反正下载就完事了广告拦截器adguard">5.5 不知道好不好用反正下载就完事了广告拦截器——ADGuard</span></h4><p>浏览器广告拦截器，不知道具体拦截了多少，不过周围人用的挺多的</p><p><img src="/2020/04/06/jing-yan-fen-xiang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171344275.png" alt="1586171344275"></p><p>本来还想推荐一些常用网站的，但是瞅了一眼可推荐的还挺多的，下次再说吧（拖延症又犯了吗）</p><p>大家有什么好用的软件，欢迎安利，可以补充到评论区~ </p><p>也非常感谢曾经安利过我工具的小伙伴，如果觉得有用，可以点个赞同~</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人生产力工具推荐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督学习</title>
      <link href="/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/"/>
      <url>/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-聚类">1 聚类</span></h2><ul><li>聚类定义：根据数据中样本与样本之间的距离或相似度，将样本划分为若干组／类／簇</li><li>划分的原则：类内样本距离小、类间样本距离大</li><li>聚类类型<ol><li>基于划分的聚类（无嵌套）</li><li>层次聚类（嵌套）</li></ol></li><li><p>聚类分析的“三要素”</p><ol><li>如何定义样本点之间的“远近”：使用相似性/距离函数</li><li>如何评价聚类出来的簇的质量</li><li>如何获得聚类的簇</li></ol></li><li><p>距离度量函数</p><ol><li><p>闵可夫斯基（Minkowski）距离：p为1时为曼哈顿距离，p为2时为欧氏距离</p><script type="math/tex; mode=display">\operatorname{dist}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\left(\sum_{d=1}^{D}\left|x_{i d}-x_{j d}\right|^{p}\right)^{1 / p}</script></li><li><p>余弦相似度(cosine similarity)</p><script type="math/tex; mode=display">s\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\frac{\sum_{d=1}^{D} x_{i d} x_{j d}}{\sqrt{\sum_{d=1}^{D} x_{i d}^{2}} \sqrt{\sum_{d=1}^{D} x_{j d}^{2}}}=\frac{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}{\left\|\mathbf{x}_{i}\right\|\left\|\mathbf{x}_{j}\right\|}</script></li><li><p>相关系数（Pearson系数）</p></li><li>杰卡德相似系数(Jaccard)</li></ol></li><li><p>聚类性能评价指标</p><ol><li>外部评价法(external criterion) ：聚类结果与参考结果有多相近</li><li>内部评价法(internal criterion)：聚类的本质特点（无参考结果）。簇内相似度越高，聚类质量越好；簇间相似度越低，聚类质量越好。</li></ol></li><li><p>经典聚类算法</p><ol><li>K均值聚类（K-means）</li><li>高斯混合模型和最大期望算法（Gaussian Mixture Models and Expectation- Maximization Algorithm）</li><li>层次聚类</li><li>基于密度聚类</li></ol></li></ul><h2><span id="2-k均值聚类k-means">2 K均值聚类（K-means）</span></h2><ul><li>算法流程</li></ul><p><img src="/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/1578206576314.png" alt="1578206576314" style="zoom:50%;"></p><ul><li>运行示意</li></ul><p><img src="/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/1578206607286.png" alt="1578206607286" style="zoom:50%;"></p><ul><li><p>一些细节</p><ul><li><p>如何划分节点：使用欧式距离进行距离度量，每个节点都划分到最近的那个质心的簇中</p></li><li><p>优化目标（损失函数）</p><script type="math/tex; mode=display">\quad J=\sum_{i=1}^{N} \sum_{k=1}^{K} r_{i k}\left\|\mathbf{x}_{i}-\boldsymbol{\mu}_{k}\right\|^{2}  \\ 从属度:r_{i,k} ∈ {0,1}  \\ u_k是类中心点</script></li></ul></li><li><p>K-Means的局限性<br>K-Means假定簇为球形且每个簇的概率相等。</p><p>当簇具有不同的尺寸、密度、簇非球形时，结果不理想；<br>离群点的影响</p></li></ul><h2><span id="3-em算法与高斯混合模型">3 EM算法与高斯混合模型</span></h2><h3><span id="31-em算法">3.1 EM算法</span></h3><ul><li><p>EM算法流程</p><ul><li>输入：观测变量数据Y，隐变量数据Z，联合分布$P(Y,Z | \theta )$，条件分布 $P(Y| Z , \theta )$；</li><li>输出：模型参数 $\theta$</li></ul><ol><li><p>选择模型参数的初始值$\theta _{0}$，开始迭代</p></li><li><p>E步，计算在模型参数固定为$\theta _{0}$的情况下的<strong>观测数据的概率</strong>：</p><script type="math/tex; mode=display">\begin{aligned}Q\left(\theta, \theta^{(i)}\right) &=E_{Z}\left[\log P(Y, Z | \theta) | Y, \theta^{(i)}\right] \\&=\sum_{Z} \log P(Y, Z | \theta) P\left(Z | Y, \theta^{(i)}\right)\end{aligned}</script></li><li><p>M步，当前观测数据概率下，通过极大似然估计来估计参数</p><script type="math/tex; mode=display">\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)</script></li><li><p>重复2、3，直到收敛</p></li></ol></li></ul><h3><span id="32-高斯混合模型">3.2 高斯混合模型</span></h3><h2><span id="4-层次聚类">4 层次聚类</span></h2><h2><span id="5-基于密度的聚类">5 基于密度的聚类</span></h2><ul><li>DBSCAN( density-based spatial clustering ofapplication with noise）</li></ul><h2><span id="reference">Reference</span></h2><pre><code>《模式识别与机器学习》  ——from UCAS</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL常用函数和评估指标</title>
      <link href="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/"/>
      <url>/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1><span id="1-激活函数">1 激活函数</span></h1><h3><span id="11-为什么需要激活函数">1.1 为什么需要激活函数</span></h3><p>如果不使用非线性激活函数，那么每一层输出都是上层输入的<strong>线性组合</strong></p><p>此时无论网络有多少层，其整体也将是线性的</p><h3><span id="12-常用的激活函数">1.2 常用的激活函数</span></h3><h4><span id="121-sigmoid">1.2.1 sigmoid</span></h4><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/00630Defly1g2x34jlnrrj306g0590st.jpg" alt="image" style="zoom: 67%;"></p><script type="math/tex; mode=display">\begin{aligned} a &=g(x)=\frac{1}{1+e^{-x}} \\ g(x)^{\prime} &=\frac{d}{d x} g(z)=\alpha(1-\alpha) \end{aligned}</script><p>缺点：</p><ol><li><p>梯度消失和梯度爆炸<br>如果我们初始化神经网络的权值为[0,1]之间的随机数，由反向传播算法的数学推导可以知道，梯度从后向前传播时，每传递一层梯度值都会下降为原来原来的0.25倍，如果神经网络层比较多是时，那么梯度会穿过多层之后变得接近于0，也就出现梯度消失问题，当权值初始化为 [1,+]期间内的值时，则会出现梯度爆炸问题；</p></li><li><p>非0均值<br>后果：会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。 产生的一个结果就是：x&gt;0, f=wTx+b那么对w求局部梯度则都为正，这样在反向传播的过程中w要么都往正方向更新，要么都往负方向更新，导致有一种捆绑的效果；</p></li></ol><h4><span id="122-tanh双曲正切">1.2.2 tanh(双曲正切)</span></h4><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/00630Defly1g2x355gdkij306k04q0sr.jpg" alt="image" style="zoom:67%;"></p><p>相当于sigmoid向下平移和伸缩变形</p><script type="math/tex; mode=display">\begin{array}{l}{a=g(x)=\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}} \\ {g(x)^{\prime}=\frac{d}{d z} g(x)=1-(\tanh (x))^{2}}\end{array}</script><p>解决 sigmoid 非0均值问题</p><p>弊端：梯度消失梯度爆炸、幂函数计算慢</p><h4><span id="123-relu">1.2.3 Relu</span></h4><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/00630Defly1g2x3f01a0gj306d04xmx7.jpg" alt="00630Defly1g2x3f01a0gj306d04xmx7"></p><ul><li>一定程度上缓解了梯度问题：其导数始终为一个常数</li><li><strong>计算速度非常快：</strong> 求导不涉及浮点运算，所以速度更快</li><li><strong>减缓过拟合：</strong> <code>ReLU</code> 在负半区的输出为 0，不会产生梯度/不会被训练，造成了网络的稀疏性——<strong>稀疏激活</strong>， 这有助于减少参数的相互依赖，缓解过拟合问题的发生</li></ul><p>解决梯度消失问题、计算速度快</p><p>弊端：dead relu problem 有些神经元参数永远不会更新</p><h4><span id="124-softmax">1.2.4 softmax</span></h4><p>softmax将所有的输入归一化，多用于多分类问题</p><script type="math/tex; mode=display">P(i) = \frac{e^{a_i}}{\sum_{k=1}^T e^{a_k}} \in [0,1]</script><p>softmax的损失：</p><script type="math/tex; mode=display">L = - \sum_{j=1}^T y_j \, log \, s_j</script><h3><span id="13-如何选择激活函数">1.3 如何选择激活函数</span></h3><ul><li>经验：如果是二分类0-1的问题：sigmoid，其余选Relu</li><li>一般隐层采用Relu， 有时也要试试 tanh</li></ul><h3><span id="14-常见问题">1.4 常见问题</span></h3><p>为何 tanh 比 sigmoid 收敛快？导数值域大</p><script type="math/tex; mode=display">tanh^{'}(x)=1-tanh(x)^{2}\in (0,1) \\sigmoid^{'}(x)=sigmoid(x)*(1-sigmoid(x))\in (0,\frac{1}{4}]</script><h1><span id="2-损失函数">2. 损失函数</span></h1><h2><span id="21-常见的损失函数">2.1 常见的损失函数</span></h2><h3><span id="0-1损失">0-1损失</span></h3><p>0-1损失是指预测值和目标值不相等为1， 否则为0:</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155055657.png" alt="image-20210703155055657"></p><h3><span id="绝对值损失">绝对值损失</span></h3><p>绝对值损失函数是计算预测值与目标值的差的绝对值：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155103130.png" alt="image-20210703155103130"></p><h3><span id="平方损失函数">平方损失函数</span></h3><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155108894.png" alt="image-20210703155108894"></p><h3><span id="log对数损失函数"><strong>log对数损失函数</strong></span></h3><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155120270.png" alt="image-20210703155120270"></p><h3><span id="hinge-损失函数">Hinge 损失函数</span></h3><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155130423.png" alt="image-20210703155130423"></p><h3><span id="交叉熵损失cross-entropy">交叉熵损失(Cross-entropy)</span></h3><p>交叉熵损失应该算作是log损失的一个特例</p><p>二分类的情况：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155428807.png" alt="image-20210703155428807"></p><pre><code> Y and P are lists of labels and estimations loss = -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P)) / len(Y)</code></pre><p>多分类的情况：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703155412898.png" alt="image-20210703155412898" style="zoom:67%;"></p><h2><span id="22-常问问题">2.2 常问问题</span></h2><p><strong>交叉熵函数与最大似然函数的联系和区别</strong></p><p>交叉熵函数可以由最大似然函数在<strong>伯努利分布</strong>的条件下推导出来</p><p>最小化交叉熵函数的本质就是对数似然函数的最大化。</p><p><strong>二分类为什么用交叉熵，而不用均方误差</strong></p><p>二分类大多用sigmoid，大部分值趋近于0或1，导致均方误差的结果小，更新慢</p><h1><span id="3-评估指标">3. 评估指标</span></h1><h2><span id="31-分类模型">3.1 分类模型</span></h2><h3><span id="二分类-多分类">二分类、多分类</span></h3><ul><li><p>二分类</p><ul><li>Accuracy</li><li>F1</li><li>AUC，单开一节说</li></ul></li><li><p>多分类</p><ul><li>micro-F1 <ul><li>将N分类看作N个二分类，计算所有分类的TP/FP/TN/FN，相加得到最终的TP/FP/TN/FN</li><li>根据最终的TP/FP/TN/FN，计算precision、recall、f1</li></ul></li><li>macro（n分类拆成n个二分类，求各个类别的f1平均值）<ul><li>将N分类看作N个二分类，计算所有分类的TP/FP/TN/FN，得到每个类别的P值/R值/F1值</li><li>取平均后得到的P值/R值/F1值，就是Macro的P值/R值/F1值</li></ul></li><li>weighted-F1<ul><li>将N分类看作N个二分类，计算所有分类的TP/FP/TN/FN，得到每个类别的P值/R值/F1值</li><li>计算每个类别的样本数量（按照真实值），作为权重</li><li>加权求和后得到的P值/R值/F1值，就是weighted的P值/R值/F1值</li></ul></li></ul></li></ul><h3><span id="auc">* AUC</span></h3><p>对于二分类任务，单个样本的正样本概率为P，划定阈值可得预测标签</p><p><strong>混淆矩阵</strong></p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703170254682.png" alt="image-20210703170254682" style="zoom:67%;"></p><p>True Positive Rate（真阳率）、False Positive（伪阳率）：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703170533222.png" alt="image-20210703170533222"></p><p><strong>ROC曲线</strong>：y = TP-rate和x = FP-rate的曲线</p><p>ROC曲线是通过<strong>遍历所有阈值</strong>来绘制整条曲线的，例如：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703172018860.png" alt="image-20210703172018860" style="zoom:50%;"></p><p>随着阈值的降低，更多的样本被预测为正样本，TP↑，FP↑，TN↓，FN↓。</p><p>当阈值越来越小，导致所有样本都预测为正样本时，TP-rate和FP-rate都为1.</p><p><strong>AUC</strong>同时考虑了分类器对于正例和负例的分类能力，在样本不平衡的情况下，依然能够对分类器作出合理的评价。</p><p>通俗的讲，AUC值是指模型将正样本排列在负样本之上的概率：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/v2-a8124ef5f06b689b6fe415e50a4f48c0_720w.jpg" alt="img"></p><p>相当于卡一个阈值，阈值右边全都预测为正，阈值左边全都预测为负，随着阈值的降低，RUC升高的越快，说明正样本排列的越靠右，效果就越好。</p><pre><code>labels = np.array([1,1,1,1,1,0,0,0,0,0])preds = np.array([0.9, 0.4, 0.7, 0.6, 0.5,  0.4, 0.3, 0.2, 0.6, 0.1])# 根据预测概率值排名后的样本真实值： [1,1,1,0,1,0,1,0,0,0]# AUC = 22/25 = 0.88</code></pre><p>优点：</p><ul><li>AUC不需要手动设定阈值，是一种整体上的衡量方法。</li><li>AUC对正负样本均衡并不敏感，在样本不均衡的情况下，也可以做出合理的评估。</li><li>AUC衡量的是一种排序能力，因此特别适合排序类业务；</li></ul><p>缺点：</p><ul><li>忽略了预测的概率值和模型的拟合程度；</li><li>AUC反应了太过笼统的信息。无法反应召回率、精确率等在实际业务中经常关心的指标；</li><li>它没有给出模型误差的空间分布信息，AUC只关注正负样本之间的排序，并不关心正样本内部，或者负样本内部的排序，这样我们也无法衡量样本对于好坏客户的好坏程度的刻画能力；</li></ul><p><strong>具体计算方法</strong></p><ol><li>计算ROC曲线下的面积。可以近似计算ROC曲线一个个小梯形的面积。比较直接的一种方法，但几乎不会用</li><li>从AUC统计意义去计算。<ol><li>所有的正负样本对中，正样本排在负样本前的pair对数 所占 全部正负样本对（M*N）的比例</li><li>随机的一个正样本排在随机的一个负样本前面的概率，即这个概率值。</li></ol></li></ol><pre><code># 不排序，直接对比分数对pair对计数def calcu_auc2(labels, preds):    pos_dix = [i for i in range(len(labels)) if labels[i] == 1]    neg_dix = [i for i in range(len(labels)) if labels[i] == 0]    auc = 0    for i in pos_dix:        for j in neg_dix:            if preds[i] &gt; preds[j]:                auc += 1            elif preds[i] == preds[j]:                auc += 0.5    return auc / (len(pos_dix) * len(neg_dix))# 先排序，再计算数量。没有考虑相等的预测值的排序带来的影响def calcu_auc3(labels, preds):    sorted_turple = sorted(zip(preds, labels), key=lambda x: x[0])    s = 0    total_true = sum(labels)    total_false = len(labels) - sum(labels)    false_cnt = 0    for i in sorted_turple:        if i[1]:            s += false_cnt        else:            false_cnt += 1    return s / (total_true * total_false)if __name__ == &#39;__main__&#39;:    labels = np.array([1,1,1,1,1,0,0,0,0,0])    preds = np.array([0.9, 0.4, 0.7, 0.6, 0.5,    0.4, 0.3, 0.2, 0.6, 0.1])    # 根据预测概率值排名后的样本真实值： [1,1,1,0,1,0,1,0,0,0]    print(&quot;AUC score of sklearn:&quot;,roc_auc_score(labels, preds))    print(&quot;AUC score of calcu_auc2:&quot;, calcu_auc2(labels, preds))    print(&quot;AUC score of calcu_auc3:&quot;, calcu_auc3(labels, preds))</code></pre><h3><span id="多标签分类">多标签分类</span></h3><p>和多分类一致的，只不过时有的样本被多次计算了（在计算N个类别时）</p><ul><li>micro-F1 </li><li>macro-F1 </li><li>weighted-F1 </li></ul><h3><span id="回归模型评估">回归模型评估</span></h3><div class="table-container"><table><thead><tr><th>指标</th><th>描述</th><th>metrics方法</th></tr></thead><tbody><tr><td>Mean Absolute Error(MAE)</td><td>平均绝对误差</td><td>from sklearn.metrics import mean_absolute_error</td></tr><tr><td>Mean Square Error(MSE)</td><td>均方误差</td><td>from sklearn.metrics import mean_squared_error</td></tr><tr><td>R-Squared</td><td>R决定系数</td><td>from sklearn.metrics import r2_score</td></tr></tbody></table></div><p>平均绝对误差：预测值与真实值之间平均差</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703163137975.png" alt="image-20210703163137975" style="zoom:50%;"></p><p>均方误差：预测值与真实值之间差的平方和 的平均值</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703163143234.png" alt="image-20210703163143234" style="zoom:50%;"></p><p>R决定系数：预测数据和原始数据的差的平方和 与 原始数据的离散程度的比值</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703163126494.png" alt="image-20210703163126494"></p><h2><span id="32-文本生成">3.2 文本生成</span></h2><h3><span id="准确度bleu">准确度：BLEU</span></h3><p>BLEU（Bilingual Evaluation Understudy），总体思想就是N-gram的准确率</p><p>给定标准译文reference，模型生成的句子是candidate（长度N）：那么candidate中有m个字符出现在reference，m/N就是bleu的1-gram的计算公式：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703162315709.png" alt="image-20210703162315709" style="zoom:67%;"></p><p>通俗解释：</p><p><img src="file://D:\OneDrive_workspace\OneDrive\Papers-and-Articles\NLP%E5%9F%BA%E7%A1%80\seq2seq\2019-8-29_16-7-11.png?lastModify=1625298914" alt="img"></p><p>举个例⼦，假设标签序列为A、B、C、D、E、F，预测序列为A、B、B、C、D，那么：</p><p>预测序列一元词组：A/B/C/D，都在标签序列里存在，所以P1=4/5，以此类推，p2 = 3/4, p3 = 1/3, p4 = 0。</p><p>短句子、残缺的准确句子的BLEU分数会偏高，因此引入长度惩罚因子</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703161849510.png" alt="image-20210703161849510" style="zoom:67%;"></p><p>Wn一般是均匀权重</p><h3><span id="准确度rouge">准确度：Rouge</span></h3><p>Rouge（Recall-Oriented Understudy for Gisting Evaluation）</p><p>可以看做是BLEU 的改进版，专注于<strong>召回率而非精度</strong>。</p><ul><li><p>ROUGE-N （将BLEU的精确率优化为召回率）<br><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703162427179.png" alt="image-20210703162427179" style="zoom:50%;"></p></li><li><p>ROUGE-L （将BLEU的n-gram优化为公共子序列）<br><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703162548636.png" alt="image-20210703162548636" style="zoom:67%;"></p></li></ul><p> C: a cat is on the table</p><p> S1: there is a cat on the table</p><p>例子的 ROUGE-1 和 ROUGE-2 分数如下：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703162453208.png" alt="image-20210703162453208"></p><h3><span id="多样性dist">多样性：Dist</span></h3><p>去重后的N-gram</p><p>与</p><p>全部的N-gram</p><p>的数量比例</p><h3><span id="流畅度困惑度">流畅度：困惑度</span></h3><p>用一个语言模型来评估句子的概率：</p><p><img src="/2019/12/24/shen-du-xue-xi-yu-nlp-ji-chu/dl-chang-yong-han-shu-he-ping-gu-zhi-biao/image-20210703162720057.png" alt="image-20210703162720057"></p><h3><span id="其他">其他</span></h3><ul><li><p>METEOR  需要wordnet</p></li><li><p>CIDEr   参考句与评测句的TFIDF向量余弦值</p></li><li><p>人工评测</p></li><li><ul><li>Coherence</li><li>Informativeness</li><li>Fluency</li></ul></li></ul><h1><span id="reference">Reference</span></h1><p><a href="https://zhuanlan.zhihu.com/p/58883095" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58883095</a></p><p><a href="https://zhuanlan.zhihu.com/p/223048748" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/223048748</a></p><p><a href="https://www.cnblogs.com/by-dream/p/7679284.html" target="_blank" rel="noopener">https://www.cnblogs.com/by-dream/p/7679284.html</a></p><p><a href="https://blog.csdn.net/shawroad88/article/details/105639148" target="_blank" rel="noopener">https://blog.csdn.net/shawroad88/article/details/105639148</a></p><p><a href="https://blog.csdn.net/sinat_16388393/article/details/91427631" target="_blank" rel="noopener">https://blog.csdn.net/sinat_16388393/article/details/91427631</a></p><p><a href="https://www.zhihu.com/question/39840928" target="_blank" rel="noopener">https://www.zhihu.com/question/39840928</a></p><p><a href="https://zhuanlan.zhihu.com/p/360765777" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/360765777</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL常用函数和评估指标 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP之文本表示</title>
      <link href="/2019/12/14/shen-du-xue-xi-yu-nlp-ji-chu/nlp-zhi-wen-ben-biao-shi/"/>
      <url>/2019/12/14/shen-du-xue-xi-yu-nlp-ji-chu/nlp-zhi-wen-ben-biao-shi/</url>
      
        <content type="html"><![CDATA[<h3><span id="1-词袋">1 词袋</span></h3><p>词袋模型(Bag-of-words model)是将一段文本（比如一个句子或是一个文档）用一个“装着这些词的袋子”来表示，这种表示方式不考虑文法以及词的顺序。<strong>「而在用词袋模型时，文档的向量表示直接将各词的词频向量表示加和」</strong>。</p><p>缺点：词与词之间是没有顺序关系</p><ul><li><p>图像领域（CNN系列模型）预训练的步骤：</p><ol><li>先用某个训练集合、在任务A上进行预训练，并保存参数；</li><li>采取相同的网络结构处理任务B，并在浅层结构中加载A任务学习好的参数，其它高层结构参数仍然随机初始化</li><li>之后我们用B任务的训练数据来训练网络，有两种做法：<ul><li>浅层加载的参数在训练B不更新，称为Frozen;</li><li>浅层加载的参数在训练B更新，称为Fine-Tuning</li></ul></li></ol></li><li><p>为什么有效：</p><ul><li>不同层级的CNN学到不同层级的特征，浅层特征的通用性更广，高层特征与任务更相关</li></ul></li></ul><h3><span id="2-word2vec-amp-glove">2 word2vec &amp; glove</span></h3><p>NLP领域的预训练最开始依靠的是word embedding，其中最知名的是word2vec，后来的glove在word2vec上进行改进。</p><p>word2vec就是训练了一个语言模型，其副产物（得到的参数矩阵）可以作为词的分布式表示</p><p>word2vec 的优点：考虑了词与词之间的顺序，引入了上下文的信息</p><p>glove的改进：损失函数不同，引入了词语的共现信息（构建了共现矩阵）</p><ul><li>word embedding的缺陷：多义词</li></ul><h3><span id="3-elmo">3 </span></h3><p>ELMO全称为：Embedding from Language Models。论文：《Deep contextualized word representation》</p><p><img src="/2019/12/14/shen-du-xue-xi-yu-nlp-ji-chu/nlp-zhi-wen-ben-biao-shi/640" alt="图片"></p><p>结构：两层双向LSTM，上下层的LSTM之间有residual connection ，加强了梯度的传播</p><p>训练：训练一个双向语言模型，前向语言模型和后向语言模型的概率对数和 为优化目标</p><p>下游：token embedding、双层LSTM的输出，三者分配权重，作为最后的embedding用于下游任务（实验证明每一层LSTM得到的embedding蕴含信息不一样，多种融合更丰富）</p><p>优点：基于的假设是词向量不应该是固定的，语言模型的作用可以利用上下文区分多义词。</p><p>缺点：LSTM特征抽取能力弱，双向特征融合的方式——拼接的方式融合特征能力较弱，参数固定</p><h3><span id="4-gpt">4 GPT：</span></h3><p>GPT是“Generative Pre-Training”的简称</p><p>两个大改进：特征抽取器使用了<strong>transformer</strong>；用于下游任务时，GPT<strong>参数可训练</strong></p><p>只使用上文预测下一个单词，抛开了下文（只是单向）</p><h3><span id="5-bert">5 BERT</span></h3><p>结构：Transformer的Encoder 12/24层，Hidden size 768/1024，自注意力的头12/16，总参数量100M/300M</p><p>预训练任务：</p><ol><li>MaskLM: 随机挖掉15%单词，预测被挖掉的单词。被挖掉的单词，80%被替换为MASK，10%替换为另一个单词，10%原地不动</li><li>句子关系预测，真正相连的句子和随机组合的两个句子</li></ol><p>BERT的输入：三个embedding：位置、单词、句子</p><p>优点：Transformer提取特征能力强，</p><p>缺点：参数太多，模型太大，少量数据训练容易过拟合。</p><h3><span id="references">References</span></h3><p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49271699</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP之文本表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF</title>
      <link href="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/tf-idf/"/>
      <url>/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/tf-idf/</url>
      
        <content type="html"><![CDATA[<hr><p>[TOC]</p><p>TF-IDF是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。</p><h2><span id="1-定义">1 定义</span></h2><p>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.</p><ul><li><p><strong>TF:</strong> Term Frequency, 表示词频。 一个给定的词在该文章中出现的次数。</p><script type="math/tex; mode=display">TF = \frac{\text{某个词在文章中的出现次数}}{\text{文章的总词数}}  \\</script></li><li><p><strong>IDF:</strong> Inverse Document Frequency, 表示逆文档频率。如果包含词条 t 的文档越少, IDF越大，则说明词条具有很好的类别区分能力。</p></li></ul><script type="math/tex; mode=display">IDF = log(\frac{语料库的文档总数}{包含该词的文档数+1})  \\</script><ul><li><strong>TF-IDF：</strong>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语<script type="math/tex; mode=display">\text{TF-IDF} = TF \times IDF</script></li></ul><h2><span id="2-例子">2 例子</span></h2><p>假设现在有一篇文章， 文章中包含 10000 个词组， 其中，”贵州” 出现100次，”的” 出现500次，那么我们可以计算得到这几个词的 TF(词频) 值：</p><script type="math/tex; mode=display">TF(贵州) = 100 / 10000 = 0.01 \\TF(的) = 500 / 10000 = 0.05</script><p>现在语料库中有 1000 篇文章， 其中，包含 “贵州” 的有 99 篇， 包含 “的” 的有 899 篇， 则它们的 IDF 值计算为：</p><script type="math/tex; mode=display">IDF(贵州) = log(1000 / (99+1)) = 1.000 \\IDF(的) = log(1000 / (899+1)) = 0.046</script><h2><span id="3-优缺点">3 优缺点</span></h2><ul><li>优点简单快速，而且容易理解。</li><li>缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。</li></ul><p>from</p><pre><code>https://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TF-IDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>seq2seq</title>
      <link href="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/"/>
      <url>/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/</url>
      
        <content type="html"><![CDATA[<h2><span id="1-seq2seq">1. seq2seq</span></h2><p>在⾃然语⾔处理的很多应⽤中，输⼊和输出都可以是不定⻓序列。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：</p><p>英语输⼊：“They”、“are”、“watching”、“.” </p><p>法语输出：“Ils”、“regardent”、“.” </p><p>当输⼊和输出都是不定⻓序列时，我们可以使⽤编码器—解码器（encoder-decoder）或者seq2seq模型。<strong>序列到序列模型，简称seq2seq模型。这两个模型本质上都⽤到了两个循环神经⽹络，分别叫做编码器和解码器。编码器⽤来分析输⼊序列，解码器⽤来⽣成输出序列。两 个循环神经网络是共同训练的。</strong></p><p>下图描述了使⽤编码器—解码器将上述英语句⼦翻译成法语句⼦的⼀种⽅法。在训练数据集中，我们可以在每个句⼦后附上特殊符号“\<eos>”（end of sequence）以表⽰序列的终⽌。编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号“\<eos>”。下图中使⽤了编码器在 最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的 编码信息和上个时间步的输出以及隐藏状态作为输⼊。我们希望解码器在各个时间步能正确依次 输出翻译后的法语单词、标点和特殊符号“\<eos>”。需要注意的是，解码器在最初时间步的输⼊ ⽤到了⼀个表⽰序列开始的特殊符号“<bos>”（beginning of sequence）。 </bos></eos></eos></eos></p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/2019-8-29_11-10-4.png" alt></p><h2><span id="2-编码器">2. 编码器</span></h2><p>编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量 c，并在该背景变量中编码输⼊序列信息。常⽤的编码器是循环神经⽹络。</p><p>让我们考虑批量⼤小为1的时序数据样本。假设输⼊序列是 x1, . . . , xT，例如 xi 是输⼊句⼦中的第 i 个词。在时间步 t，循环神经⽹络将输⼊ xt 的特征向量 xt 和上个时间步的隐藏状态 <img src="https://latex.codecogs.com/gif.latex?h_{t-1}" alt>变换为当前时间步的隐藏状态ht。我们可以⽤函数 f 表达循环神经⽹络隐藏层的变换：</p><p><img src="https://latex.codecogs.com/gif.latex?h_t=f(x_t,h_{t-1}" alt>)</p><p>接下来，编码器通过⾃定义函数 q 将各个时间步的隐藏状态变换为背景变量：</p><p><img src="https://latex.codecogs.com/gif.latex?c=q(h_1,...,h_T" alt>)</p><p>例如，当选择 <em>q</em>(<strong><em>h</em></strong>1<em>, . . . ,</em> <strong><em>h*</em></strong>T<em> ) = **</em>h<strong><em>*T</em> 时，背景变量是输⼊序列最终时间步的隐藏状态*</strong>h<em>**</em>T*。</p><p>以上描述的编码器是⼀个单向的循环神经⽹络，每个时间步的隐藏状态只取决于该时间步及之前的输⼊⼦序列。我们也可以使⽤双向循环神经⽹络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。</p><h2><span id="3-解码器">3. 解码器</span></h2><p>刚刚已经介绍，编码器输出的背景变量 c 编码了整个输⼊序列 x1, . . . , xT 的信息。给定训练样本中的输出序列 y1, y2, . . . , yT′ ，对每个时间步 t′（符号与输⼊序列或编码器的时间步 t 有区别），解码器输出 yt′ 的条件概率将基于之前的输出序列<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-39-17.png" alt>和背景变量 c，即：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-45-12.png" alt></p><p>为此，我们可以使⽤另⼀个循环神经⽹络作为解码器。在输出序列的时间步 t′，解码器将上⼀时间步的输出 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-46-6.png" alt>以及背景变量 c 作为输⼊，并将它们与上⼀时间步的隐藏状态  <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-46-54.png" alt>变换为当前时间步的隐藏状态st′。因此，我们可以⽤函数 g 表达解码器隐藏层的变换：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-47-36.png" alt></p><p>有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-49-45.png" alt>，例如，基于当XQ前时间步的解码器隐藏状态 st′、上⼀时间步的输出<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-46-54.png" alt>以及背景变量 c 来计算当前时间步输出 yt′ 的概率分布。</p><h2><span id="4-训练模型">4. 训练模型</span></h2><p>根据最⼤似然估计，我们可以最⼤化输出序列基于输⼊序列的条件概率：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-51-22.png" alt></p><p>并得到该输出序列的损失：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-52-25.png" alt></p><p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在上图所描述的模型预测中，我们需要将解码器在上⼀个时间步的输出作为当前时间步的输⼊。与此不同，在训练中我们也可以将标签序列（训练集的真实输出序列）在上⼀个时间步的标签作为解码器在当前时间步的输⼊。这叫作强制教学（teacher forcing）。</p><h2><span id="5-seq2seq模型预测">5. seq2seq模型预测</span></h2><p>以上介绍了如何训练输⼊和输出均为不定⻓序列的编码器—解码器。本节我们介绍如何使⽤编码器—解码器来预测不定⻓的序列。</p><p>在准备训练数据集时，我们通常会在样本的输⼊序列和输出序列后面分别附上⼀个特殊符号“\<eos>”表⽰序列的终⽌。我们在接下来的讨论中也将沿⽤上⼀节的全部数学符号。为了便于讨论，假设解码器的输出是⼀段⽂本序列。设输出⽂本词典Y（包含特殊符号“\<eos>”）的⼤小为|Y|，输出序列的最⼤⻓度为T′。所有可能的输出序列⼀共有 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-53-47.png" alt>种。这些输出序列中所有特殊符号“\<eos>”后⾯的⼦序列将被舍弃。</eos></eos></eos></p><h3><span id="51-贪婪搜索">5.1 贪婪搜索</span></h3><p>贪婪搜索（greedy search）。对于输出序列任⼀时间步t′，我们从|Y|个词中搜索出条件概率最⼤的词：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-55-26.png" alt></p><p>作为输出。⼀旦搜索出“\<eos>”符号，或者输出序列⻓度已经达到了最⼤⻓度T′，便完成输出。我们在描述解码器时提到，基于输⼊序列⽣成输出序列的条件概率是<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-56-6.png" alt>。我们将该条件概率最⼤的输出序列称为最优输出序列。而贪婪搜索的主要问题是不能保证得到最优输出序列。</eos></p><p>下⾯来看⼀个例⼦。假设输出词典⾥⾯有“A”“B”“C”和“\<eos>”这4个词。下图中每个时间步<br>下的4个数字分别代表了该时间步⽣成“A”“B”“C”和“\<eos>”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最⼤的词。因此，图10.9中将⽣成输出序列“A”“B”“C”“\<eos>”。该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048。</eos></eos></eos></p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/2019-8-29_13-44-5.png" alt></p><p>接下来，观察下面演⽰的例⼦。与上图中不同，在时间步2中选取了条件概率第⼆⼤的词“C”<br>。由于时间步3所基于的时间步1和2的输出⼦序列由上图中的“A”“B”变为了下图中的“A”“C”，下图中时间步3⽣成各个词的条件概率发⽣了变化。我们选取条件概率最⼤的词“B”。此时时间步4所基于的前3个时间步的输出⼦序列为“A”“C”“B”，与上图中的“A”“B”“C”不同。因此，下图中时间步4⽣成各个词的条件概率也与上图中的不同。我们发现，此时的输出序列“A”“C”“B”“\<eos>”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，⼤于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列“A”“B”“C”“\<eos>”并⾮最优输出序列。</eos></eos></p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/2019-8-29_13-47-56.png" alt></p><h3><span id="52-穷举搜索">5.2 穷举搜索</span></h3><p>如果⽬标是得到最优输出序列，我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列。</p><p>虽然穷举搜索可以得到最优输出序列，但它的计算开销<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-59-0.png" alt>很容易过⼤。例如，当|Y| =10000且T′ = 10时，我们将评估 <img src="https://latex.codecogs.com/gif.latex?10000^{10}=10^{40}" alt>个序列：这⼏乎不可能完成。而贪婪搜索的计算开销是 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-59-0.png" alt>，通常显著小于穷举搜索的计算开销。例如，当|Y| = 10000且T′ = 10时，我们只需评估<img src="https://latex.codecogs.com/gif.latex?10000*10=10^5" alt>个序列。</p><h3><span id="53-束搜索">5.3 束搜索</span></h3><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个束宽（beam size）超参数。我们将它设为 k。在时间步 1 时，选取当前时间步条件概率最⼤的 k 个词，分别组成 k 个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的 k 个候选输出序列，从 k |Y| 个可能的输出序列中选取条件概率最⼤的 k 个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“\<eos>”的序列，并将它们中所有特殊符号“\<eos>”后⾯的⼦序列舍弃，得到最终候选输出序列的集合。</eos></eos></p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/2019-8-29_14-0-5.png" alt></p><p>束宽为2，输出序列最⼤⻓度为3。候选输出序列有A、C、AB、CE、ABD和CED。我们将根据这6个序列得出最终候选输出序列的集合。在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/seq2seq/2019-8-29_16-5-41.png" alt></p><p>其中 L 为最终候选序列⻓度，α ⼀般可选为0.75。分⺟上的 Lα 是为了惩罚较⻓序列在以上分数中较多的对数相加项。分析可知，束搜索的计算开销为<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_16-6-24.png" alt>。这介于贪婪搜索和穷举搜索的计算开销之间。此外，贪婪搜索可看作是束宽为 1 的束搜索。束搜索通过灵活的束宽 k 来权衡计算开销和搜索质量。</p><h2><span id="6-代码实现">6. 代码实现</span></h2><p><a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.5%20seq2seq/seq2seq.ipynb" target="_blank" rel="noopener">TensorFlow  seq2seq的基本实现</a></p><h2><span id="reference">Reference</span></h2><p><a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seq2seq </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2vec</title>
      <link href="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/"/>
      <url>/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-one-hot-vector">1 One-hot vector</span></h2><p>将每个单词用N维的长向量表示，N是一个很大的数字。</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/v2-4e2374fcb759404e1c16f56662093c9d_b.jpg" alt="img"></p><p>缺点是：</p><ol><li>每个单词都是单独的，无法计算单词之间的相似性，因为他们的乘积为0</li><li>并且维度较高，计算成本较大</li></ol><h2><span id="2-基于svd奇异值分解的词向量表示方法">2 基于SVD奇异值分解的词向量表示方法</span></h2><p>通过包含若干句话的语料库：</p><ol><li>I enjoy flying. </li><li>I like NLP. </li><li>I like deep learning</li></ol><p>我们可以得到一个共现矩阵，矩阵的各个数值代表词前后所跟其他词汇的频率统计：</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/v2-c1864b26d102c8c63bf6bac88e769250_b.jpg" alt="img"></p><p>我们可以把这个矩阵进行处理（降维），得到维度较小的词向量矩阵。</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/v2-1e6a5951b66d4f78ae26fef9b8b7e5ca_b.jpg" alt="img"></p><p>最终得到的矩阵USV，U就是我们想要得到的词向量。</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/v2-6eb166d15516ea5bf488c357e88a0dda_b.jpg" alt="img"></p><p>它达到了降维的目的，有效的编码语义和语法信息，U的V个行向量就分别是各个单词的词向量了。</p><p>奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，是特征分解（方阵分解）在任意矩阵上的推广，最大目的是数据的降维。</p><p>但是它依然有很多的问题：</p><ol><li>矩阵维度变化频繁（语料库的词数改变）</li><li>矩阵过于稀疏</li><li>矩阵维度依然很大</li><li>计算SVD的效率问题</li></ol><h2><span id="3-word2vec">3 Word2vec</span></h2><p>通过神经网络，把ont-hot转换成低维的能体现相似性词向量的方法。</p><p>有两种算法，原理都差不太多：</p><ol><li>Skip-grams (SG) ：通过当前词预测上下文</li><li>Continuous Bag of Words (CBOW)：通过上下文预测当前词</li></ol><p>基于局部滑动窗口计算的。该方法利用了局部的上下文特征</p><h3><span id="31-skip-grams-sg">3.1 Skip-grams (SG)</span></h3><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/1575882450737.png" alt="1575882450737" style="zoom: 50%;"></p><ol><li><strong>输入层：</strong>输入当前词的one-hot 向量$x_{k}$。词表大小为V，则ont-hot向量的维度为V</li><li><strong>输入层到隐藏层：</strong>第一个目标是训练出V行N列的矩阵W。 N是要训练出来的词向量的长度。这个矩阵就代表着最终训练得到的中心词的词向量矩阵。通过W与$x_k$的相乘，这个词的ont-hot转化为向量。</li><li><strong>隐藏层：</strong>Skip-gram的隐藏层不做任何处理，输入即输出：$h_i$</li><li><strong>隐藏层到输出层：</strong>第二个目标是训练出V行N列的矩阵W’。 通过隐层的输出与W’的点积，得到进行下一步</li><li><strong>输出层：</strong>通过隐层的输出$h_i$与W’的点积，得到一个V*1的向量，每一行的值代表着输入的中心词与矩阵W’中各个词的相似度。通过softmax将这个值转化成概率，损失函数则是真实概率与输出概率的差值，当概率越接近，得到的W和W’结果就越好。</li></ol><p>得到的W和W’就是我们将one-hot转化成的新的词向量矩阵。</p><h3><span id="32-cbow">3.2 CBOW</span></h3><p><strong>与skip-gram相反，CBOW</strong>通过上下文信息预测中心词wt。</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/1575883141332.png" alt="1575883141332" style="zoom: 50%;"></p><ol><li><strong>输入层：</strong>输入若干上下文词的one-hot 向量$x_{1k},x_{2k},…,x_{Ck}$。词表大小为V，则ont-hot向量的维度为V</li><li><strong>输入层到隐藏层：</strong>第一个目标是训练出V行N列的矩阵W。 N是要训练出来的词向量的长度。这个矩阵就代表着最终训练得到的中心词的词向量矩阵。通过W与$x_k$的相乘，这些词的ont-hot转化为向量。</li><li><strong>隐藏层：</strong>CBOW的隐藏层是词袋模型，得到$h_{i}$</li><li><strong>隐藏层到输出层：</strong>第二个目标是训练出V行N列的矩阵W’。 通过隐层的输出与W’的点积，得到进行下一步</li><li><strong>输出层：</strong>通过隐层的输出$h_i$与W’的点积，得到一个V*1的向量，每一行的值代表着输入的中心词与矩阵W’中各个词的相似度。通过softmax将这个值转化成概率，损失函数则是真实概率与输出概率的差值，当概率越接近，得到的W和W’结果就越好。</li></ol><p>word2Vec 模型训练带来了两个词嵌入矩阵 ， 分别代表中心词和上下文矩阵 。 上下文词嵌入会在训练后被丢弃，但保留了中心词词嵌入矩阵。</p><h3><span id="33-hierarchical-softmax">3.3 Hierarchical Softmax</span></h3><p>Skip-gram和cbow在输出层到隐藏层每次分别更新1行、C行的权重，但在隐藏层到输出层要更新所有权重，效率不行。</p><p>层次Softmax直接丢掉了第二个矩阵W’，直接将隐藏层的输出$h_{i}$直接处理：</p><p>根据词表构建一个哈夫曼树，之前要对V个词计算相似度，现在只需要最多计算$log(V)$次线就可以得到</p><p><img src="/2019/12/08/shen-du-xue-xi-yu-nlp-ji-chu/word2vec/hs.png" alt="hs"></p><h3><span id="34-negative-sampling">3.4 Negative Sampling</span></h3><p>在 Word2Vec 中， 对于输出层来说，我每一个输出节点都要预测词表中所有词在当前位置的概率，在动辄几万甚至几十万大的词表中，用softmax 计算真的十分困难。 </p><p>但我们的目的不在于训练一个精准的语言模型，而只是为了训练得到语言模型的副产物-词向量，那么我们可不可以把输出压缩呢，将几万的输出压缩到几十程度，这计算量是成几何倍数的下降。</p><p>负采样的思路很简单，<strong>不直接让模型从整个词表中找最可能的词，而是直接给定这个词（正例）和几个随机采样的噪声词（负例），然后模型能够从这几个词中找到正确的词，就算达到目的了。</strong></p><p>那么如何对负例进行采样呢？作者直接使用<strong>基于词频的权重分布</strong>来获得概率分布进行抽样：</p><script type="math/tex; mode=display">weight(w) = \frac{count(w)^{0.75}}{\sum_u count(w)^{0.75}}</script><p>然后选取权重最大的一些词语作为负采样的样本</p><h2><span id="reference">reference</span></h2><p><a href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf" target="_blank" rel="noopener">http://cs224d.stanford.edu/lecture_notes/notes1.pdf</a></p><p><a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Word2vec </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率图模型-CRF</title>
      <link href="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/"/>
      <url>/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-马尔可夫随机场mrf">1 马尔可夫随机场MRF</span></h2><p>马尔可夫随机场是典型的马尔可夫网，即一个无向图模型。</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image001.png" alt="img"></p><p>马尔可夫随机场表示的随机变量之间具有马尔可夫性</p><ol><li>成对马尔可夫性：给定Yo的条件下，Yu和Yv条件独立</li></ol><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image006.png" alt="https://images2015.cnblogs.com/blog/1008922/201705/1008922-20170528162455422-1812780274.png" style="zoom:33%;"></p><ol><li>局部马尔可夫性</li></ol><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image008.png" alt="https://images2015.cnblogs.com/blog/1008922/201705/1008922-20170528160105610-150035802.png" style="zoom: 50%;"></p><ol><li>全局马尔可夫性</li></ol><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image010.png" alt="https://images2015.cnblogs.com/blog/1008922/201705/1008922-20170528162514578-985158227.png" style="zoom: 50%;"></p><h2><span id="2-条件随机场crf">2 条件随机场CRF</span></h2><p>条件随机场是给定随机变量X，输出随机变量Y的马尔可夫随机场，一种判别式无向图模型。</p><p>CRF的特点是假设输出随机变量构成马尔可夫随机场。</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/1575706608437.png" alt="1575706608437" style="zoom:50%;"></p><h2><span id="3-条件随机场的参数化形式">3 条件随机场的参数化形式</span></h2><p>设P(Y|X)为线性链条件随机场，则在随机变量X取值为x的条件下，随机变量Y取值为y的条件概率具有如下形式：</p><script type="math/tex; mode=display">\begin{aligned} P(y | x) &=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \\ Z(x) &=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \end{aligned}</script><p>条件随机场定义条件概率的方式与隐马尔可夫模型类似，但是需要定义特征函数$f_k$</p><h2><span id="4-hmm与crf的区别">4 HMM与CRF的区别</span></h2><p> 一个是生成模型，一个是判别模型</p><p>用于标注问题的生成模型 and 由输入对输出进行预测的判别模型</p><p>HMM是一个特殊的CRF，CRF是HMM的一般化的形式</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率图模型-CRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率图模型</title>
      <link href="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/"/>
      <url>/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-概率图模型">1 概率图模型</span></h2><p>机器学习最重要的任务，是根据一些己观察到的证据（例如训练样本）来对感兴趣的未知变量（例如类别标记）进行估计和推测。</p><p>概率图模型（probabilistic graphical model）是一类用图来表达变量相关关系的概率模型。</p><p>它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，结点之间的边表示变量间的概率相关关系，即”变量关系图”。</p><ul><li>节点表示随机变量/状态，边表示概率关系<ul><li>有向概率图模型或贝叶斯网络：因果关系</li><li>无向图模型或马尔科夫随机场：关联关系</li></ul></li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578149027920.png" alt="1578149027920" style="zoom: 50%;"></p><p>第一类是使用有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网(Bayesiannetwork)</p><p>第二类是使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网(Markovnetwork)</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578149164966.png" alt="1578149164966" style="zoom:50%;"></p><h2><span id="2-有向概率图模型贝叶斯网">2 有向概率图模型(贝叶斯网)</span></h2><ul><li>有向概率图模型的例子<br>隐马尔科夫模型、卡尔曼滤波、因子分析、概率主成分分析、独立成分分析、混合高斯、转换成分分析、概率专家系统、Sigmoid 信念网络、层次化混合专家</li></ul><h3><span id="21-有向概率图模型贝叶斯网">2.1 有向概率图模型(贝叶斯网)</span></h3><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153210482.png" alt="1578153210482" style="zoom: 67%;"></p><ul><li><p>概率分布</p><script type="math/tex; mode=display">p\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}\right)=p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{1}\right) p\left(x_{4} | x_{2}\right) p\left(x_{5} | x_{3}\right) p\left(x_{6} | x_{2}, x_{5}\right)</script></li><li><p>贝叶斯网的表示<br>贝叶斯网使用一系列变量间的“局部”关系“紧凑”地表示联合概率分布</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153439973.png" alt="1578153439973" style="zoom: 50%;"></p></li><li><p>条件独立：$X_{4} \perp\left\{X_{1}, X_{3}\right\} | X_{2}$</p><script type="math/tex; mode=display">p\left(x_{4} | x_{1}, x_{2}, x_{3}\right)=p\left(x_{4} | x_{2}\right)</script><p> 其他的条件独立陈述：给定X2和X3，X1和X6独立</p></li></ul><h3><span id="22-三种经典有向图条件独立">2.2 三种经典有向图，条件独立</span></h3><ul><li>三种经典有向图，注意其中的条件独立证明</li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153674350.png" alt="1578153674350"></p><ul><li>通过贝叶斯球算法进行条件独立检验<ul><li>定义几个术语，描述贝叶斯球在一个节点上的动作：<ul><li><strong>通过（pass through）</strong>：从当前节点的父节点方向过来的球，可以访问当前节点的任意子节点。（父-&gt;子）从当前节点的子节点方向过来的球，可以访问当前节点的任意父节点。（子-&gt; 父）</li><li><strong>反弹（bounce back）</strong>：从当前节点的父节点方向过来的球，可以访问当前节点的任意父节点。（父-&gt; 父）从当前节点的子节点方向过来的球，可以访问当前节点的任意子节点。（子-&gt; 子）</li><li><strong>截止（block）</strong>：当前节点阻止贝叶斯球继续运动。</li></ul></li><li>贝叶斯球算法(规则)：<ul><li>假设在贝叶斯网络中，有一个按一定规则运动的球。已知中间节点（或节点集合）Z，如果球不能由节点X出发到达节点Y（或者由Y到X），则称X和Y关于Z独立。</li></ul></li><li>规则<ul><li>未知节点：总能使贝叶斯球通过，同时还可以反弹从其子节点方向来的球。（父-&gt; 子）|（子-&gt; 父/子）</li><li>已知节点：反弹从其父节点方向过来的球，截止从其子节点方向过来的球。（父-&gt; 父）|（子-&gt;“截止”）</li></ul></li><li></li></ul></li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153956080.png" alt="1578153956080" style="zoom: 50%;"></p><h2><span id="3-无向概率图模型马尔科夫随机场">3 无向概率图模型(马尔科夫随机场)</span></h2><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578154032413.png" alt="1578154032413" style="zoom:50%;"></p><ul><li>无向概率图模型的概率分布<ul><li>有向图：利用“局部” 参数（条件概率）去表示联合概率</li><li>无向图：放弃条件概率，失去局部概率表示，保持独立地任意地选择这些函数的能力，保证所有重要的联合表示可以表示为局部函数的积</li></ul></li><li>局部函数的定义域<ul><li>团（全连通无向图）、极大团（局部最大的团）、势函数（非负实值函数，一种无约束形式）</li></ul></li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578154236713.png" alt="1578154236713" style="zoom:50%;"></p><h2><span id="4-生成式模型与称判别式模型">4 生成式模型与称判别式模型</span></h2><p>生成模型是<strong>对联合概率分布$P(X,Y,Z)$进行建模</strong>，在给定观测集合X的情况下，<strong>计算边缘分布</strong>$P(X)$来<strong>得到对变量的推断</strong>。</p><script type="math/tex; mode=display">P(Y | X) = P(X, Y) / P(X) = \sum_{Z}{P(X,Y,Z)} / \sum_{Y, Z}{P(X,Y,Z)}</script><p>判别式模型直接<strong>对条件概率分布$P(Y,Z | X)$进行建模。</strong></p><script type="math/tex; mode=display">P(Y | X) = \sum_{Z}{P(Y,Z | X)}</script><p>判别式模型（DiscriminativeModel）是直接学习决策函数f(x)或者直接求得条件概率分布p(y|x;θ)。常见的判别式模型有线性回归模型、支持向量机SVM等。</p><p><strong>对条件分布进行建模。</strong></p><h2><span id="5-自然语言处理中概率图模型的演变">5 自然语言处理中概率图模型的演变</span></h2><p>横向：由点到线（序列结构）、到面（图结构）。</p><p>纵向：在一定条件下生成式模型（genmodel）转变为判别式模型（discriminativemodel）</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/clip_image002.jpg" alt="img"></p><h2><span id="6-典型的概率图模型">6 典型的概率图模型</span></h2><p>HMM、CRF等见其他文章。</p><h2><span id="reference">Reference</span></h2><pre><code>《模式识别与机器学习》课件 —— from UCAS《统计学习方法》 李航</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率图模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention</title>
      <link href="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/attention/"/>
      <url>/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/attention/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-basic-attention">1 Basic Attention</span></h2><h3><span id="11-attention-定义">1.1 Attention 定义</span></h3><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/attention/006gOeiSly1g0tf397umyj30kn08uq34.jpg" alt="006gOeiSly1g0tf397umyj30kn08uq34"></p><p>对于Attention机制的整个计算过程，可以总结为以下三个过程：</p><ul><li><strong>socre 函数：</strong> 根据 Query 与 Key 计算两者之间的相似性或相关性， 即 socre 的计算。</li><li><strong>注意力权重计算：</strong>通过一个softmax来对值进行归一化处理获得注意力权重值， 即$a_{i,j}$ 的计算。</li><li><strong>加权求和生成注意力值：</strong>通过注意力权重值对value进行加权求和， 即 $c_i$ 的计算。</li></ul><script type="math/tex; mode=display">\alpha_{i,j} = \frac{e^{score(Query, Key(j))}}{\sum_{k=1}^t e^{score(Query, Key(k))}} \\c_i= \sum_{i=1}^n = \alpha_{i,j} h_j</script><h3><span id="12-score函数的选择">1.2 Score函数的选择</span></h3><p>常见的方式主要有以下三种：</p><ul><li>求点积：学习快，适合向量再同一空间中，如 Transformer 。</li></ul><script type="math/tex; mode=display">score(Query, Key(j)) = Query \cdot Key(j)</script><ul><li>Cosine 相似性</li></ul><script type="math/tex; mode=display">score(Query, Key(j)) = \frac{Query \cdot Key(j)}{||Query|| \cdot ||Key(j)||}</script><ul><li>MLP网络</li></ul><script type="math/tex; mode=display">score(Query, Key(j)) = MLP(Query,  Key(j)) \\general: score(Query, Key(j)) = Query \, W \, Key(j) \\concat: score(Query, key(j)) = W \, [Query;Key(j) ]</script><p>一般情况下，采用MLP网络更加灵活一些，且可以适当的扩展层以及改变网络结构，这对于一些任务来说是很有帮助的。</p><h3><span id="13-self-attention">1.3 Self-Attention</span></h3><p>Self-Attention 的本质就是<strong>自己注意自己</strong>， 粗暴点来说，就是，<strong>Q，K，V是一样的</strong>，即：</p><script type="math/tex; mode=display">Attention \, value = Attention(W_QX,W_KX,W_VX)</script><p>它的内部含义是对序列本身做 Attention，来获得序列内部的联系。 </p><p>整个计算过程：</p><p>输入X，转化为嵌入向量W</p><p>根据嵌入向量得到Q、K、V</p><h2><span id="2-seq2seqattention">2 Seq2Seq+Attention</span></h2><ul><li>传统的Seq2Seq<script type="math/tex; mode=display">\begin{array}{c}{s_{i}=f\left(y_{i-1}, s_{i-1}, c\right)} \\ {p\left(y_{i} | y_{1}, y_{2} \ldots y_{i-1}\right)=g\left(y_{i-1}, s_{i}, c\right)}\end{array}</script>$s_{i-1}$是decoder中的上一个隐状态；$y_{i-1}$是decoder中上一个预测得到的输出，在下一步进行输入；c是encoder得到的编码信息</li></ul><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/attention/11525720-15378fee9fe62013.webp" alt="11525720-15378fee9fe62013" style="zoom:50%;"></p><ul><li>加入Attention的Seq2Seq：<strong>原公式中encoder的编码信息c变成了$c_{i}$</strong><script type="math/tex; mode=display">{p\left(y_{i} | y_{1}, y_{2}, \ldots, y_{i-1}\right)=g\left(y_{i-1}, s_{i-1}, c_{i}\right)}</script>$$</li><li>其中，对encoder得到的编码信息做了处理：语境权重加入到encoder的隐藏向量得到编码信息<script type="math/tex; mode=display"></script>c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j}<br>$$</li></ul><script type="math/tex; mode=display">- 语境权重是decoder中的上一个隐状态与所有的encoder中的隐藏向量计算得到（例如内积）</script><script type="math/tex; mode=display">\begin{aligned} \alpha_{i j}=& \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)} \\ e_{i j}=& a\left(s_{i-1}, h_{j}\right) \end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN and RNNs</title>
      <link href="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/"/>
      <url>/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-cnn">1 CNN</span></h2><p>卷积神经网络：是一种专门用来处理具有类似网格结构数据的神经网络。</p><h3><span id="11-卷积神经网络结构">1.1 卷积神经网络结构</span></h3><div class="table-container"><table><thead><tr><th>CNN层次结构</th><th>作用</th></tr></thead><tbody><tr><td>输入层</td><td>卷积网络的<strong>原始输入</strong>，可以是原始或预处理后的像素矩阵</td></tr><tr><td>卷积层</td><td>参数共享、局部连接，利用平移不变性<strong>从全局特征图提取局部特征</strong></td></tr><tr><td>激活层</td><td>将卷积层的输出结果进行<strong>非线性映射</strong>，首选 Relu</td></tr><tr><td>池，化层</td><td><strong>进一步筛选特征</strong>，可以有效<strong>减少</strong>后续网络层次所需的<strong>参数量</strong></td></tr><tr><td>全连接层</td><td>用于把该层之前提取到的<strong>特征综合起来</strong>。</td></tr></tbody></table></div><h3><span id="12-卷积层的内容">1.2 卷积层的内容</span></h3><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/1.gif" alt="1"></p><p>原理上其实是对两个矩阵进行<strong>点乘求和</strong>的数学操作</p><p><strong>参数</strong>：</p><ul><li><p>width：卷积核的尺寸       </p></li><li><p>stride：步长</p></li><li><p>padding：填充</p></li><li><p>channel：通道数，包括输入和输出通道数</p></li><li><p><strong>单一维度尺寸计算公式</strong>：</p><script type="math/tex; mode=display">\text {output}=\left\lfloor\frac{(\text {input}-\text {width}+2 * \text {pad})}{\text {stride}}\right\rfloor+ 1</script></li></ul><p>对文本做卷积的例子：</p><pre><code>class CNN(nn.Module):    def __init__(self, num_keynel, kernel_width, embedd_size):        super(CNN, self).__init__()        self.conv = nn.Conv2d(in_channels=1, out_channels=num_keynel, kernel_size=[kernel_width, embedd_size], stride=1 ,padding=[1, 0])    def forward(self, input):        return self.conv(input)if __name__ == &#39;__main__&#39;:    x = torch.rand(3, 20, 200) # [batch_size, seq_length, embedd_size]    model = CNN(num_keynel=50, kernel_width=3, embedd_size=200)    y = model(x.unsqueeze(1))  # [batch_size, 50, seq_length, 1]    y = F.tanh(y.squeeze(3)) # [batch_size, 50, seq_length]    y = F.avg_pool1d(y, kernel_size=20, padding=0, stride=1) # [batch_size, 50, 1]    y = y.squeeze(2) # [batch_size, 50]</code></pre><h3><span id="13-常见问题">1.3 常见问题</span></h3><ul><li>卷积和池化的区别</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">卷积层</th><th style="text-align:center">池化层</th></tr></thead><tbody><tr><td style="text-align:center">结构</td><td style="text-align:center">零填充时输出维度不变，而通道数改变</td><td style="text-align:center">输出维度会降低，通道数不变</td></tr><tr><td style="text-align:center">稳定性</td><td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td><td style="text-align:center">感受域内的细微变化不影响输出结果</td></tr><tr><td style="text-align:center">作用</td><td style="text-align:center">感受域内提取局部关联特征</td><td style="text-align:center">感受域内提取泛化特征，降低维度</td></tr><tr><td style="text-align:center">参数量</td><td style="text-align:center">与卷积核尺寸、卷积核个数相关</td><td style="text-align:center">不引入额外参数</td></tr></tbody></table></div><ul><li><p>为什么需要padding</p><ol><li>防止图像变小</li><li>防止边缘像素点关注过少</li></ol></li><li><p>为什么卷积核都是奇数尺寸</p><ol><li>保证像素点中心位置，避免位置信息偏移</li><li>填充边缘时能保证两边都能填充，原矩阵依然对称</li></ol></li><li><p>CNN的特性有哪些作用</p><ol><li>稀疏交互：卷积核尺度远小于输入的尺度。每个输出神经网络仅与前一层神经元存在连接权重</li></ol><blockquote><ul><li>提高模型的统计效率：原本一幅图像只能提供少量特征，现在每一块像素区域都可以提供一部分特征</li><li>使得参数大量减少，优化的时间复杂度也会减小几个数量级，过拟合情况也得到改善。 </li><li>稀疏交互的意义在于，<strong>先从局部的特征入手，再将局部特征组合起来形成更复杂和抽象的特征</strong>。</li></ul></blockquote><ol><li>参数共享：参数共享指的是<strong>同一个模型的不同模块中使用相同的参数</strong>。参数共享的意义在于使得卷积层具有<strong>平移等特性</strong>。</li></ol><blockquote><ul><li>权重共享一定程度上能增强参数之间的联系，获得更好的<strong>共性特征</strong>。</li><li>很大程度上降低了网络的参数，<strong>节省计算量和计算所需内存</strong>。</li><li>权重共享能起到<strong>很好正则的作用</strong>。正则化的目的是为了降低模型复杂度，防止过拟合，而权重共享则正好降低了模型的参数和复杂度。</li></ul></blockquote><ol><li>平移不变性：（局部）平移不变性是一个很有用的性质，尤其是当我们关心某个特征<strong>是否出现</strong>而不关心它出现的具体位置时。平移不变性是由于参数共享 和池化 所带来的。</li></ol></li><li><p>CNN有什么不足</p><ol><li>信息损失问题</li><li>忽略了位置信息</li></ol></li><li>1乘1卷积核的作用<ol><li>特征图大小不变，但是通道数可以改变，减小参数量</li><li>增加非线性，后面有激活函数</li><li>可以用于实现跨通道的信息交互，例如这一层有1乘1卷积核和3乘3卷积核同时存在</li></ol></li></ul><h2><span id="2-rnn">2 RNN</span></h2><p>循环神经网络（Recurrent Neural Network）是一种递归神经网络（Recursive Neural Network）。</p><p>其以序列数据为输入，在序列的演进方向进行递归且所有节点按链式连接的。<strong>将序列输入翻译成定长向量</strong></p><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/clipboard-1575710987473.png" alt="clipboard-1575710987473"></p><p>R、O决定网络类型，普通的RNN中：</p><script type="math/tex; mode=display">s_i=tanh(W_xx_i+W_ss_{i-1}+b)\\y_i=W_ys_i+b</script><h2><span id="3-lstm">3 LSTM</span></h2><ul><li><p>LSTM解决的问题：</p><ol><li>梯度消失，使得 RNN 很难有效地训练 。</li><li>RNN 难以捕捉到长距离依赖信息。</li></ol></li><li><p>LSTM的构成：输入门 i、遗忘门 f、输出门 o</p></li></ul><script type="math/tex; mode=display">\begin{aligned} c_{j} &=f \odot c_{j-1}+i \odot z \\ h_{j} &=o \odot \tanh \left(c_{j}\right) \\ i &=\sigma\left(x_{j} W^{x i}+h_{j-1} W^{h i}+b_i\right) \\ f &=\sigma\left(x_{j} W^{x f}+h_{j-1} W^{h f}+b_f\right) \\ o &=\sigma\left(x_{j} W^{x o}+h_{j-1} W^{h o}+b_o\right) \\ z &=\tanh \left(x_{j} W^{x z}+h_{j-1} W^{x z}+b_z\right)  \end{aligned}</script><ul><li><ul><li><strong>时刻 j 的状态</strong>：两个向量组成：<strong>记忆状态$c_j$ 和 隐藏状态$h_j$</strong></li><li><strong>记忆状态$c_j$ ：</strong>由经过遗忘门的上一个记忆状态$c_{j-1}$ 和经过输入门的更新候选项$z$</li><li><strong>更新候选项$z$：</strong>由当前输入$x_j$和前一个状态 $h_{j-1}$ 加一个tanh决定</li><li><strong>输入门 i、遗忘门 f、输出门 o：</strong>门的值由当前输入$x_j$和前一个状态 $h_{j-1}$ 加一个sigmoid决定</li></ul></li><li>LSTM的hidden和cell存储的信息是什么？<br>hidden存储到当前时间步的所有信息，cell存储将来的时间步中可能需要的特定信息。</li></ul><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/clipboard.png" alt="clipboard"></p><h2><span id="4-lstm如何解决rnn梯度消失问题">4 LSTM如何解决RNN梯度消失问题</span></h2><h3><span id="41-rnn为什么会出现梯度消失问题">4.1 RNN为什么会出现梯度消失问题</span></h3><p>RNN单元有三个w参数矩阵，分别控制输入$x_i$，上一个状态$S_{i-1}$，以及输出$y_i$</p><script type="math/tex; mode=display">S_i=tanh(W_xx_i+W_ss_{i-1}+b)\\O_i=W_ys_i+b</script><ul><li>前向传播过程（假设序列长度为3：）</li></ul><script type="math/tex; mode=display">\begin{array}{l}{S_{1}=W_{x} X_{1}+W_{s} S_{0}+b_{1}} \\ {S_{2}=W_{x} X_{2}+W_{s} S_{1}+b_{1}} \\ {S_{3}=W_{x} X_{3}+W_{s} S_{2}+b_{1} \\  O_{3}=W_{o} S_{3}+b_{2}}\end{array}</script><ul><li>反向传播（假设损失为$L_3$）</li></ul><script type="math/tex; mode=display">\begin{array}{l}{\frac{\partial L_{3}}{\partial W_{0}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial W_{o}}} \\ {\frac{\partial L_{3}}{\partial W_{x}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{x}}} \\ {\frac{\partial L_{3}}{\partial W_{s}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{s}}}\end{array}</script><ul><li>任意时刻对$w_s,w_x$的偏导为：</li></ul><script type="math/tex; mode=display">\frac{\partial L_{t}}{\partial W_{x}}=\sum_{k=0}^{t} \frac{\partial L_{t}}{\partial O_{t}} \frac{\partial O_{t}}{\partial S_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}\right) \frac{\partial S_{k}}{\partial W_{x}}</script><ul><li>由于tanh的累乘极限为0，则序列较长时梯度消失出现</li></ul><h4><span id="42-lstm是怎么解决的">4.2 LSTM是怎么解决的</span></h4><p>LSTM与RNN本质是相同的，其公式为：</p><script type="math/tex; mode=display">\begin{aligned} c_{j} &=f \odot c_{j-1}+i \odot z \\ h_{j} &=o \odot \tanh \left(c_{j}\right) \\ i &=\sigma\left(x_{j} W^{x i}+h_{j-1} W^{h i}+b_i\right) \\ f &=\sigma\left(x_{j} W^{x f}+h_{j-1} W^{h f}+b_f\right) \\ o &=\sigma\left(x_{j} W^{x o}+h_{j-1} W^{h o}+b_o\right) \\ z &=\tanh \left(x_{j} W^{x z}+h_{j-1} W^{x z}+b_z\right)  \end{aligned}</script><p>RNN，LSTM可以抽象为下图：</p><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/1470684-20190512212754183-21935751.png" alt="img"></p><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/1470684-20190512212754481-298118564.png" alt="img"></p><p>根据其思想，其本质上是：</p><script type="math/tex; mode=display">s_j=tanh(f \odot s_{j-1} + i \odot x)</script><p>其反向传播公式也是如同RNN的形式，限制其梯度消失的因素中：</p><script type="math/tex; mode=display">\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}  = \prod_{j=k+1}^{t} tanh'\sigma(y)</script><p>LSTM与RNN区别就在于tanh变成了<script type="math/tex">tanh'\sigma(x)</script>，他的图像如下，大部分的数值要么0要么1：</p><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/cnn-rnn/1576391115887.png" alt="1576391115887" style="zoom:50%;"></p><p>则反向传播的结果中，相当于一部分保留一部分消失。</p><p><strong>即解决的本质上是：RNN中导致梯度消失的项替换为0或1，使多项式一部分消失一部分保留</strong></p><h2><span id="5-gru">5 GRU</span></h2><p>GRU是一种LSTM的替代方案。</p><p>GRU 也基于门机制，但是总体上使用了更少的门并且网络不再额外给出记忆状态</p><script type="math/tex; mode=display">\begin{aligned} s_{j}=R_{\mathrm{GRU}}\left(s_{j-1}, x_{j}\right) &=(1-z) \odot x_{j-1}+z \odot \tilde{s_{j}} \\ z &=\sigma\left(x_{j} W^{xz}+s_{j-1} W^{s z}\right) \\ r &=\sigma\left(x_{j} W^{xr}+s_{j-1} W^{s r}\right) \\ \tilde{s_{j}} &=\tanh \left(x_{j} W^{x_{j}}+\left(r \odot s_{j-1}\right) W^{xg}\right) \\ y_{j}=O_{\mathrm{GRU}}\left(s_{j}\right) &=s_{j} \end{aligned}</script><ul><li><ul><li>更新门z：由前一时刻的状态和输入决定</li><li>重置门r：由前一时刻的状态和输入决定</li></ul></li><li><ul><li>s_j~ ：由重置门r、输入、前一个状态决定</li><li>当前状态s_j：由更新门、输入、s_j~决定</li></ul></li></ul><p>LSTM 中的输入与遗忘门对应于 GRU 的更新门，重置门直接作用于前面的隐藏状态。</p><p>因为目前为止并没有一个很普适一致的观点，来说明到底那一个更好，哪一个在哪方面更加适用。</p><p>GRU，简单有效 ，效率更高</p><p>LSTM，灵活，复杂导致过拟合风险高</p><h2><span id="reference">Reference</span></h2><p>《统计学习方法》</p><p>《深度学习》 （花书）</p><p><a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a><br><a href="https://github.com/htfhxx/NLPer-Interview" target="_blank" rel="noopener">https://github.com/htfhxx/NLPer-Interview</a><br><a href="https://www.cnblogs.com/jins-note/p/10853788.html" target="_blank" rel="noopener">https://www.cnblogs.com/jins-note/p/10853788.html</a><br><a href="https://zhuanlan.zhihu.com/p/44163528" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44163528</a></p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=nn%20conv2d#torch.nn.Conv2d" target="_blank" rel="noopener">https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html?highlight=nn%20conv2d#torch.nn.Conv2d</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN and RNNs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL优化方法</title>
      <link href="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/dl-you-hua-fang-fa/"/>
      <url>/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/dl-you-hua-fang-fa/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-防止欠拟合-过拟合的方法">1 防止欠拟合、过拟合的方法</span></h2><ul><li><strong>欠拟合：</strong>模型没有充分学习到数据中的特征信息，无法很好的拟合数据，在训练集和测试集上效果都不好。</li><li><strong>过拟合：</strong>模型过于复杂，将噪声数据的特征也学习到模型中了，泛化能力下降，导致模型的训练误差远小于测试误差。</li></ul><p><img src="/2019/12/07/shen-du-xue-xi-yu-nlp-ji-chu/dl-you-hua-fang-fa/2019-8-18_21-7-0.png" alt="2019-8-18_21-7-0"></p><h3><span id="11-降低欠拟合的风险">1.1 降低欠拟合的风险</span></h3><ol><li>添加新特征<br>避免特征不足or特征与样本标签的相关性不强时无法拟合</li><li>增加模型复杂度<br>提高模型的学习能力</li><li>减少正则化系数</li></ol><h3><span id="12-降低过拟合风险">1.2 降低过拟合风险</span></h3><ol><li><p>从数据入手，获得更多的训练数据</p></li><li><p>降低模型复杂度</p></li><li><p>正则化<br>在模型损失函数中添加惩罚项，使学出的模型参数值较小，避免权值过大带来的过拟合风险。</p></li><li><p>集成学习方法（Dropout、交叉检验等）<br>降低单一模型的过拟合风险</p></li><li><p>训练的调优：Early stoping</p><p>当验证集的误差连续 <code>n</code>次递增时停止训练。</p></li></ol><h2><span id="2-深度学习技巧">2. 深度学习技巧</span></h2><h3><span id="21-dropout">2.1 Dropout</span></h3><ul><li>Dropout的思想是：在每次训练过程中随机地忽略一些神经元。</li><li><p>为什么Dropout能解决过拟合<br>整个dropout过程就相当于： 对很多个不同的神经网络取平均，相当于Bagging的一种近似</p></li><li><p>dropout是怎么实现的</p><ul><li>在训练时，每个神经单元以概率pp被保留(Dropout丢弃率为1−p)；</li><li>在预测阶段（测试阶段），每个神经单元都是存在的，权重参数w要乘以p，输出是：pw</li><li>为了保持同样的输出期望值并使下一层也得到同样的结果</li></ul><pre><code>import numpy as npp = 0.5  # 神经元激活概率def train_step(X):    &quot;&quot;&quot; X contains the data &quot;&quot;&quot;    # 三层神经网络前向传播为例    H1 = np.maximum(0, np.dot(W1, X) + b1)    U1 = np.random.rand(*H1.shape) &lt; p   # first dropout mask    H1 *= U1 # drop!    H2 = np.maximum(0, np.dot(W2, H1) + b2)    U2 = np.random.rand(*H2.shape) &lt; p # second dropout mask    H2 *= U2 # drop!    out = np.dot(W3, H2) + b3def predict(X):    # ensembled forward pass    H1 = np.maximum(0, np.dot(W1, X) + b1) * p    # NOTE: scale the activations    H2 = np.maximum(0, np.dot(W2, H1) + b2) * p   # NOTE: scale the activations    out = np.dot(W3, H2) + b3</code></pre></li></ul><h3><span id="22-正则化">2.2 正则化</span></h3><h4><span id="l1正则化">L1正则化</span></h4><p>L1正则化项：向量的1范数</p><script type="math/tex; mode=display">正则化项： \Omega(\theta) = ||w||_1 =  \sum_i |w_i| \\目标函数： \tilde{J}(w;X,y) = \alpha ||w||_1  + J(w;X,y)  \\梯度： \nabla_w \tilde{J}(w;X,y) = \alpha sign(w) + \nabla_w J(w;X,y) \\</script><p>L1 正则化使得权重值可能被减少到0。 因此，L1对于压缩模型很有用。</p><h4><span id="l2正则化">L2正则化</span></h4><p>L2正则化项：向量的2范数（元素平方和相加再开放</p><script type="math/tex; mode=display">正则化项： \Omega(\theta) = \frac{1}{2} ||w||_2^2  = \frac{1}{2}w^Tw \\目标函数： \tilde{J}(w;X,y) = \frac{\alpha}{2}w^Tw  + J(w;X,y)  \\梯度： \nabla_w \tilde{J}(w;X,y) = \alpha w + \nabla_w J(w;X,y) \\梯度更新 ： w \leftarrow (1- \epsilon \alpha) w - \epsilon \nabla_w J(w;X,y)</script><p>L2正则化又称权重衰减。因为其导致权重<strong>趋向于0</strong>（但不全是0）。</p><h4><span id="关于正则化的一些问题">关于正则化的一些问题</span></h4><ul><li><p>为什么只对权重做惩罚，不包含偏置:<br>精确拟合偏置所需的数据通常比拟合权重少得多，因此偏置拟合程度较好。<br>正则化偏置参数可能会导致明显的欠拟合。</p></li><li><p><strong>L1正则化与L2正则化的异同</strong><br>相同点：限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。<br>不同点：</p><ol><li>L1 正则化可以产生<strong>稀疏权值矩阵</strong>，可以用于特征选择；L2 趋向于生成参数值很小的矩阵</li><li>L1 适用于特征之间有关联的情况； L2 适用于特征之间没有关联的情况</li></ol></li><li><p>为什么L1正则化可以产生稀疏解，而L2不会？</p><p>产生稀疏解，其实就是目标函数求导后 w=0时得到目标函数的极小值</p><p>形式化一下分别加了L1和L2正则化的目标函数：</p><script type="math/tex; mode=display">\begin{gathered}J_{L 1}(w)=L(w)+\lambda|w| \\J_{L 2}(w)=L(w)+\lambda w^{2}\end{gathered}</script><p>假设在w=0处，其L(w)的导数为d0，那么L2正则化的导数在w=0处和以前一样，没啥变化，不会导致稀疏解</p></li></ul><script type="math/tex; mode=display">\begin{gathered}\left.\frac{\partial J_{L 2}(w)}{\partial w}\right|_{w=0}=d_{0}+2 \times \lambda \times w=d_{0} \end{gathered}</script><p>​        但是L1正则化的导数不一样，在w=0的两边取极限，发现两边的导数可能异号，导数先小于0后大于0，所以w=0处出现极小值，导致稀疏解：</p><script type="math/tex; mode=display">\begin{gathered}\left.\frac{\partial J_{L 1}(w)}{\partial w}\right|_{w=0^{-}}=d_{0}-\lambda \\\left.\frac{\partial J_{L 1}(w)}{\partial w}\right|_{w=0^{+}}=d_{0}+\lambda\end{gathered}</script><ul><li>为什么L1和L2正则化可以防止过拟合<br>L1 &amp; L2 正则化会使模型偏好于更小的权值，更小的权值意味着更低的模型复杂度，有助于提高模型的泛化能力。</li></ul><h2><span id="3-梯度下降方法">3 梯度下降方法</span></h2><h3><span id="21-基础梯度下降法">2.1 基础梯度下降法</span></h3><ul><li><p>标准梯度下降</p><script type="math/tex; mode=display">\theta = \theta + \eta \cdot  \nabla_\theta J(\theta)</script></li><li><p>mini-batch 梯度下降</p><script type="math/tex; mode=display">\theta = \theta + \eta \cdot  \nabla_\theta J(\theta; x^{(i:i+n)}; y^{(i:i+n)})</script></li><li><p>随机梯度下降</p><script type="math/tex; mode=display">\theta = \theta + \eta \cdot  \nabla_\theta J(\theta; x^{(i)}; y^{(i)}) \,\,\, \eta 为学习率</script></li><li><p>优缺点：</p><ol><li>标准梯度下降需要计算所有样本梯度，更新速度慢；梯度方向一致且更新次数少，容易陷入局部最小值</li><li>随机梯度下降是mini-batch的特殊，不能达到最优解，不容易陷入局部最小</li></ol></li></ul><h3><span id="22-动量梯度下降法">2.2 动量梯度下降法</span></h3><ul><li><p>梯度的指数加权平均值$v_t$:</p><script type="math/tex; mode=display">v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)  \\ \theta = \theta - v_t \\\gamma: \text{加权系数，常取值为0.9}</script></li><li><p>使用指数加权平均值来更新梯度：本质是通过增加动量来减少随机，增加梯度的稳定性。</p></li><li><p>其中的指数加权平均值本质上考虑了前几次的梯度，不至于在”下坡“过程中因为遇到暂时的”上坡“而改变方向</p></li></ul><h3><span id="23-自适应学习率优化算法">2.3  自适应学习率优化算法</span></h3><ul><li>Adagrad</li><li>Adadelta</li><li>RMSprop</li><li>Adam</li></ul><p>ps：看到这里脑壳痛，先放放</p><h2><span id="4-归一化方法">4 归一化方法</span></h2><h3><span id="41-ics问题">4.1 ICS问题</span></h3><p>独立同分布（independent and identically distributed），独立同分布的数据可以简化常规机器学习模型的训练、提高模型的能力。</p><p>内部协变量偏移（Internel Covariate Shift, ICS）问题</p><p>深度神经网络的训练过程中，每一层的参数更新会导致上层的输入数据分布发生变化，上层参数需要不断适应新的输入数据分布，会降低学习速度。</p><h3><span id="42-batch-normalization">4.2 Batch Normalization</span></h3><p>动机：针对单个神经元，利用网络训练时一个 mini-batch 的数据来计算该神经元 $x_i$ 的均值和方差，进行归一化。</p><p>假设一个 batch 为 m 个输入 $B = \{x_{1}, \cdots, x_m\}$ , Batch Normalization在这 m 个数据之间做归一化， 并学习参数 $\gamma , \beta$：</p><script type="math/tex; mode=display">\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_i    \\\sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2  \\\hat{x}_i  \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\y_i \leftarrow \gamma \hat{x}_i + \beta \equiv BN_{\gamma, \beta}{(x_i)}</script><p>对Batch Normalization的优点进行总结，大概有以下几个方面：</p><ul><li>让模型更稳定，可以采用更高的学习率，提高模型训练的速度，并且可以有效避免梯度的消失和爆炸</li><li>起到正则化的作用，类似dropout的功能</li><li>不用太在意参数的初始化</li></ul><p>Batch Normalization 在推断时如何设置方差和期望：</p><ul><li>不用验证样本的均值和方差，而是利用训练集统计的均值和方差，统计方式一般包括移动平均和全局平均。</li></ul><p>有哪些问题</p><ul><li>依赖 batch_size</li><li>无法处理序列化数据</li><li>推断时无法使用</li></ul><h3><span id="43-layer-normalization">4.3 Layer Normalization</span></h3><p>不同于 BN， 其在层内进行 Normalization。即直接对隐层单元的输出做 Normalization。</p><script type="math/tex; mode=display">u^l = \frac{1}{H} \sum_{i=1}^H a_i^l \\\sigma^l = \sqrt{\frac{1}{H}\sum_{i=1}^H(a_i^l - u^l)^2} \\\hat{a}_i^l  \leftarrow \frac{a_i^l - \mu^l}{\sqrt{\sigma_l^2 + \epsilon}} \\y_i^l \leftarrow \gamma \, \hat{a}_i^l + \beta \equiv LN_{\gamma, \beta}{(a_i^l)}</script><p><strong>与Batch Normalization 区别</strong></p><p>Batch 顾名思义是对一个batch进行操作。假设我们有 10行 3列 的数据，即我们的batchsize = 10，每一行数据有三个特征，假设这三个特征是【身高、体重、年龄】。那么BN是针对每一列（特征）进行缩放，例如算出【身高】的均值与方差，再对身高这一列的10个数据进行缩放。体重和年龄同理。这是一种“列缩放”。</p><p>而layer方向相反，它针对的是每一行进行缩放。即只看一笔数据，算出这笔所有特征的均值与方差再缩放。这是一种“行缩放”。</p><p><strong>BN是取不同样本的同一个通道的特征做归一化；LN取的是同一个样本的不同通道做归一化</strong></p><pre><code>一个batch的数据：[[1,2],[3,4],[5,6]]Batch Normalization:    index 1: 1+3+5 = 9    index 2： 2+4+6 = 12    [[1/9,2/12],[3/9,4/12],[5/9,6/12]]Layer Normalization:    [[1/3,2/3],[3/7,4/7],[5/11,6/11]]</code></pre><h2><span id="reference">reference：</span></h2><pre><code>《统计学习方法》https://github.com/NLP-LOVE/ML-NLPhttps://github.com/km1994/NLP-Interview-Notes/https://zhuanlan.zhihu.com/p/54530247https://zhuanlan.zhihu.com/p/74516930https://blog.csdn.net/b876144622/article/details/81276818https://www.zhihu.com/question/37096933https://www.cnblogs.com/zingp/p/11631913.htmlhttps://www.cnblogs.com/mengxiangtiankongfenwailan/p/9989065.html</code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归与逻辑回归</title>
      <link href="/2019/12/05/ji-qi-xue-xi/xian-xing-hui-gui-yu-luo-ji-hui-gui/"/>
      <url>/2019/12/05/ji-qi-xue-xi/xian-xing-hui-gui-yu-luo-ji-hui-gui/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1><span id="1-线性回归">1 线性回归</span></h1><h2><span id="11-简介">1.1 简介</span></h2><p>简单来说，线性回归算法就是<strong>找到一条直线（一元线性回归）或一个超平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。</strong></p><script type="math/tex; mode=display">y = w_0x_0 + \cdots  + w_px_p + b = wx+b</script><h2><span id="12-如何计算">1.2 如何计算</span></h2><ul><li><p>损失函数为：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \qquad  \\</script></li><li><p>利用<strong>梯度下降法</strong>找到最小值点，也就是最小误差，最后把 w 和 b 给求出来</p></li></ul><h2><span id="13-过拟合如何解决">1.3 过拟合如何解决</span></h2><ul><li><p>使用岭回归-Ridge（加入L2正则项）</p><script type="math/tex; mode=display">\hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i w_i^2</script></li><li><p>使用Lasso回归（加入L1正则项）</p><script type="math/tex; mode=display">\hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i |w_i|</script></li><li><p>使用场景</p><ul><li>只要数据线性相关，用LinearRegression拟合的不是很好，<strong>需要正则化</strong>，可以考虑使用岭回归</li><li>如果输入特征的维度很高，而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。</li><li><strong>L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0</strong>，从而增强模型的泛化能力 。</li></ul></li></ul><h1><span id="2-逻辑回归">2 逻辑回归</span></h1><h2><span id="21-简介">2.1 简介</span></h2><p>logistic回归用于解决的是分类问题，<strong>其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。</strong></p><p>也就是说，logistic 回归不是对所有数据点进行拟合，而是要对<strong>数据之间的分界线</strong>进行拟合。</p><ul><li>表达式是：</li></ul><script type="math/tex; mode=display">h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}</script><ul><li>其中，sigmoid函数的参数就是线性回归的结果</li></ul><p><img src="/2019/12/05/ji-qi-xue-xi/xian-xing-hui-gui-yu-luo-ji-hui-gui/00630Defly1g4pvk2ctatj30cw0b63yq.jpg" alt="image" style="zoom: 50%;"></p><h2><span id="22-如何求解">2.2 如何求解</span></h2><ul><li><p>线性回归的拟合函数本质上是对 <strong>输出变量 y 的拟合</strong>， 而逻辑回归的拟合函数是对 <strong>label 为1的样本的概率的拟合</strong>。</p></li><li><p>线性回归其参数计算方式为<strong>最小二乘法</strong>， 逻辑回归其参数更新方式为<strong>极大似然估计</strong>。</p></li><li><p>逻辑回归满足伯努利分布（0-1分布）：</p><script type="math/tex; mode=display">P(Y=1|x; \theta) = h_{\theta}(x) \\P(Y=0|x; \theta)  = 1 - h_{\theta}(x) \\p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}</script></li><li><p>逻辑回归的极大似然函数是：</p><script type="math/tex; mode=display">\begin{align}L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x(i);\theta) \\ &= \prod_{i=1}^m  (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}\\\end{align}</script></li><li><p>对数似然函数为：</p><script type="math/tex; mode=display">\begin{align}L(\theta) &= log L(\theta ) \\&= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))\end{align}</script></li></ul><ul><li><p>求解梯度：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L(\theta)}{\partial \theta_j} &= (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} \\&= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} \\&= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j \\&= [y - h_{\theta} (x)]x_j \\\end{align}</script></li><li><p>优化参数：</p><script type="math/tex; mode=display">\begin{align}\theta_j &= \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} \\&= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}   \end{align}</script></li></ul><h2><span id="23-如何实现多分类">2.3 如何实现多分类</span></h2><ul><li><strong>方式1：</strong> softmax。修改逻辑回归的损失函数，将sigmoid改为softmax函数构造模型从而解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。</li><li><strong>方式2：</strong> 根据每个类别都建立一个二分类器。本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。</li></ul><p>若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。</p><h2><span id="24-逻辑回归特征的离散化">2.4 逻辑回归特征的离散化</span></h2><p>很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型</p><p>LR 为何要对特征进行离散化</p><ul><li><strong>非线性。</strong> 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； </li><li><strong>速度快。</strong> 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展</li><li><strong>鲁棒性。</strong> 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li><li><strong>方便交叉与特征组合</strong>： 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。</li><li><strong>稳定性：</strong> 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</li><li><strong>简化模型：</strong> 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li></ul><h1><span id="reference">Reference</span></h1><pre><code>《统计学习方法》《机器学习》——西瓜书https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归与逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法题易用写法汇总</title>
      <link href="/2019/12/04/yu-yan-gong-ju-ji-zhu-deng-wen-dang/suan-fa-ti-yi-yong-xie-fa-hui-zong/"/>
      <url>/2019/12/04/yu-yan-gong-ju-ji-zhu-deng-wen-dang/suan-fa-ti-yi-yong-xie-fa-hui-zong/</url>
      
        <content type="html"><![CDATA[<h2><span id="二分查找">二分查找</span></h2><p>leetcode704：<a href="https://leetcode.com/problems/sqrtx/submissions/" target="_blank" rel="noopener">https://leetcode.com/problems/sqrtx/submissions/</a></p><pre><code>class Solution {public:    int search(vector&lt;int&gt;&amp; nums, int target) {        if(nums.empty()==true)            return -1;        int left=0,right=nums.size()-1;        while(left&lt;right){    //不再需要考虑退出循环时返回left还是right            int middle=left+(right-left)/2;   //得到的始终是左中位数；left+(right-left+1)/2是右中位数   //(left+right)/2; 会有整型溢出问题                     // if(nums[middle]==target)  //只夹逼，不判断，效率且适用性高            //     return middle;            if(nums[middle]&lt;target)  //注意如果是从这里收缩就不要带等于号（不收缩掉middle）                left=middle+1;            else   //只需要判断在左边还是右边就好了                   right=middle;  //因为取的是左中位数，所以要从左边界收缩（例如只有两个数字[1,2]，target=3，从右边界收缩会在1处死循环）        }        if(nums[left]==target) //因为临界条件中left==right时没有判断，因此在这里需要判断            return left;        return -1;    }};</code></pre><p>判断是否左收缩还是右收缩，可以分析只有两个数字的极限情况<br>例如leetcode69中，求sqrt(x)的整数解，需要右收缩：只有两个数字时：[2,3]要取2的时候右收缩<br>收缩过程要收缩掉middle<br><a href="https://leetcode.com/problems/sqrtx/submissions/" target="_blank" rel="noopener">https://leetcode.com/problems/sqrtx/submissions/</a></p><h2><span id="reference">Reference</span></h2><pre><code>https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247486644&amp;idx=1&amp;sn=a4c5e9aad51a42fceeb13543b80c22a3&amp;chksm=fa0e6335cd79ea23a15452bf3c4195f99c6a70fc10273870dda2f4f0db46543e429189affd8d&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1574858020671&amp;sharer_shareid=59332ea7c33ee752808701f0287171ae#rd</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法题易用写法汇总 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/2019/12/02/ji-qi-xue-xi/jue-ce-shu/"/>
      <url>/2019/12/02/ji-qi-xue-xi/jue-ce-shu/</url>
      
        <content type="html"><![CDATA[<h1><span id="1-决策树">1. 决策树</span></h1><p>[TOC]</p><h2><span id="11-简介">1.1 简介</span></h2><p>决策树是一个分而治之的递归过程。 </p><ol><li>构建根节点，初始化特征集合和数据集合；</li><li>选择最优特征，并划分子集，更新数据集合和特征集合；</li><li>继续划分直到所有训练数据基本划分正确。</li></ol><p>决策树学习的关键问题：<strong>特征选择、决策树的生成和决策树的修剪。</strong></p><h2><span id="12-特征选择">1.2 特征选择</span></h2><p><strong>选择最优划分属性</strong>是决策树的关键。</p><p>几种常见的划分属性的策略：ID3、C4.5、CART。</p><h3><span id="121-id3-c45">1.2.1 ID3、C4.5</span></h3><ul><li>数据集的<strong>信息熵</strong>：不确定性、用于度量样本的集合纯度，其中p是类别 $i$ 占总样本的比例:</li></ul><script type="math/tex; mode=display">H(D)=-\sum_{i=1}^{n} p_{i} \log p_{i}, \\ p_i = D_i / D</script><ul><li>针对某个特征 A，对于数据集 D 的<strong>条件熵</strong> $H(D|A)$:</li></ul><script type="math/tex; mode=display">\begin{aligned}H(D \mid A) &=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right) \end{aligned}</script><ul><li>信息增益：<strong>数据集D的信息熵$D(D)$</strong>与<strong>特征A下的数据集D的条件熵</strong>之差：</li></ul><script type="math/tex; mode=display">g(D, A)=H(D)-H(D | A)</script><ul><li><p>信息增益的算法<br><img src="/2019/12/02/ji-qi-xue-xi/jue-ce-shu/image-20210710164845451.png" alt="image-20210710164845451"></p><ul><li><p>输入：数据集D和特征A<br>输出：特征A对数据集D的信息增益$g(D, A)$</p><ol><li>计算数据集D的经验熵H(D)： </li></ol><script type="math/tex; mode=display">H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}</script><ol><li>计算特征A对数据集D的经验条件熵H(D|A)：</li></ol><script type="math/tex; mode=display">H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D |} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}</script><ol><li>计算信息增益： $g(D, A)=H(D)-H(D | A)$</li></ol></li></ul></li><li><p>信息增益比：</p></li></ul><script type="math/tex; mode=display">g_{R}(D, A)=\frac{g(D, A)}{H(D)}</script><ul><li><p>优缺点：</p><ul><li>信息增益对<strong>取值数目较多的属性</strong>有偏好(分支多，划分后的信息增益大)</li><li>增益比对取值数目较少的属性有所偏好</li><li>于是：c4.5先从侯选属性中找出信息增益高于平均水平的，再从中选择增益率最高的</li></ul></li><li><p>为什么信息增益偏向属性取值多的分支？</p><ul><li>当特征取值较多时， 根据此特征划分得到的子集纯度提升的更多（对比取值较少的特征）</li></ul></li></ul><ul><li>其他问题：<ul><li>生成树的过程中：如果特征$A_g$的信息增益（比）小于阈值，则置为单节点的树</li><li>终止条件：数据集为空或者特征集为空的情况返回树</li></ul></li></ul><h3><span id="122-cartclassification-and-regression-tree">1.2.2 CART（Classification And Regression Tree)</span></h3><p>将信息熵换为基尼指数</p><ul><li><p>基尼指数：表示样本集合的不纯度，数值越小纯度越高</p><script type="math/tex; mode=display">\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}</script></li><li><p>给定样本D的基尼指数</p><script type="math/tex; mode=display">\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}</script></li><li><p>如果根据特征 $A$ 被分割成 $D_1$ 和 $D_2$ 两部分， 那么在特征 A 的条件下， 集合 D 的基尼系数定义为：</p><script type="math/tex; mode=display">\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)</script></li><li><p>选择最小的$\operatorname{Gini}(D, A_i)$来确定特征的选择</p></li><li><p>优缺点：减少了大量的对数运算</p></li></ul><h2><span id="13-决策树的剪枝处理">1.3 决策树的剪枝处理</span></h2><p>剪枝的动机：防止决策树过拟合，泛化能力差</p><p>剪枝的分类：预剪枝和后剪枝</p><p>预剪枝：在节点划分前确定是否停止剪枝：</p><ol><li>节点内数据样本数量低</li><li>划分后精度变低</li></ol><p>后剪枝：尽可能多的划分，然后在已经生成的决策树上进行剪枝</p><ol><li>从叶子节点自底向上递归，回缩到对应的父节点上，计算精度的变化，如果剪枝后精度上升，就可以将子树替代为节点</li></ol><h2><span id="14-面试常问">1.4 面试常问</span></h2><ul><li><p>递归的终止条件是什么呢？</p><ul><li>树的最大深度</li><li>直到每个叶子节点都<strong>只有一种类型</strong>时停止，（这种方式很容易过拟合） </li><li>当叶子节点的<strong>样本个数</strong>小于一定的阈值</li><li>节点的<strong>信息增益</strong>（或基尼系数）小于预定阈值    <strong>推荐</strong></li><li>没有更多特征</li></ul></li><li><p>回归树？</p><ul><li>使用<strong>标准差</strong>来衡量子集中元素的纯度。</li></ul></li><li><p>为什么信息增益偏向属性取值多的分支？</p><ul><li>当特征取值较多时， 根据此特征划分得到的子集纯度提升的更多（对比取值较少的特征）</li></ul></li></ul><h2><span id="reference">Reference：</span></h2><pre><code>《统计学习方法》《机器学习》——西瓜书https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interviewhttps://www.bilibili.com/video/BV1Ca4y1t7DS?p=2&amp;spm_id_from=pageDriver</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM</title>
      <link href="/2019/11/28/ji-qi-xue-xi/svm-de-jian-ji-xiang-zheng-li/"/>
      <url>/2019/11/28/ji-qi-xue-xi/svm-de-jian-ji-xiang-zheng-li/</url>
      
        <content type="html"><![CDATA[<h1><span id="svm">SVM</span></h1><p>[TOC]</p><h2><span id="1-整体概览">1 整体概览</span></h2><ul><li>什么是SVM<br>SVM 是一种二分类模型。它的基本思想是在特征空间中寻找间隔最大的分离超平面使数据二分类</li></ul><ul><li>线性可分支持向量机——硬间隔支持向量机</li><li>线性支持向量机——软间隔支持向量机</li><li>非线性支持向量机——核技巧</li></ul><h2><span id="2-线性可分支持向量机">2 线性可分支持向量机</span></h2><h3><span id="21-函数间隔与几何间隔">2.1 函数间隔与几何间隔</span></h3><ul><li>分离超平面：$w^{<em>} \cdot x + b^{</em>}=0$</li><li>点到超平面的距离：正的一侧：$\gamma_{i}=\frac{1}{|w|}(w \cdot x_i +b) $      负的一侧：$\gamma_{i}=-\frac{1}{|w|}(w \cdot x_i +b) $      </li><li>由于$w \cdot x+b$与类标记y的符号是否一致能代表分类正确，定义：<ul><li>超平面对于<strong>样本点</strong>$(x_i,y_i)$的<strong>函数间隔</strong>：$\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)$</li><li>超平面对于<strong>数据集</strong>的函数间隔：$\hat{\gamma}=\min  \hat{\gamma}_{i}$</li><li>超平面对于<strong>样本点</strong>$(x_i,y_i)$的<strong>几何间隔</strong>：$\gamma_{i}=y_i\frac{1}{|w|}\left(w \cdot x_{i}+b\right)$</li><li>超平面对于<strong>数据集</strong>的几何间隔：$\gamma=min{ \gamma_{i}}$</li></ul></li></ul><h3><span id="22-间隔最大化">2.2 间隔最大化</span></h3><ul><li><p>最大间隔分离超平面可以表示为约束优化问题：</p><ul><li>最大间隔求分离超平面</li></ul><script type="math/tex; mode=display">\max _{w, b}\gamma</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N</script><ul><li><p>将几何间隔改为函数间隔：</p><script type="math/tex; mode=display">\max _{w, b} \frac{\hat{\gamma}}{\|w\|}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N</script></li><li><p><strong>将函数间隔固定为1，最大化改为最小化（得到凸二次规划问题）</strong></p><script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2}</script></li></ul></li></ul><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right)\geqslant 1, \quad i=1,2, \cdots, N</script><h3><span id="23-学习的对偶算法">2.3 学习的对偶算法</span></h3><ul><li><p>拉格朗日对偶问题</p><ul><li><p>转化为最小约束的问题：</p><script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1, \quad i=1,2, \cdots, N</script></li><li><p><strong>拉格朗日乘子法（带约束的问题转为无约束的问题）</strong>，这也是<strong>损失函数</strong><br>在这里，对于系数$\alpha$，非支持向量的系数为0，支持向量的系数大于0</p></li><li><script type="math/tex; mode=display">L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}</script></li></ul></li></ul><ul><li><p>对偶问题求解 —— 求$\min_{w,b} L(w,b,a)$  - 原始问题的对偶问题极大极小问题，后面两部分始终小于0</p><ul><li><p>对w、b求偏导：</p><script type="math/tex; mode=display">\begin{array}{l}{\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0} \\ {\nabla_{b} L(w, b, \alpha)=\sum_{i=1}^{N} \alpha_{i} y_{i}=0}\end{array}</script><p>得到：</p><script type="math/tex; mode=display">\begin{array}{l}{w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}} \\ {\sum_{i=1}^{N} \alpha_{i} y_{i}=0}\end{array}</script></li><li><p>将其带入拉格朗日函数</p><script type="math/tex; mode=display">\begin{aligned} L(w, b, \alpha) &=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\ &=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \end{aligned}</script></li></ul></li><li><p>对偶问题求解 —— 求$\min_{w,b} L(w,b,a)$对a的极大</p><ul><li><p><strong>对偶问题为</strong></p><script type="math/tex; mode=display">\begin{array}{ll}{\max _{\alpha}} & {-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script></li><li><p><strong>极大转换成极小</strong></p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{\alpha}} & {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script></li><li><p>假设有解：$\mathrm{a}^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \ldots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$，则最优化的解为：</p><script type="math/tex; mode=display">\begin{array}{c}{w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}} \\ {b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)}\end{array}</script></li><li><p>并可以证明满足KKT的条件。</p></li><li><p>补充：极小问题满足二次规划问题，通过代入数据集后求解a从而求得w和b</p></li></ul></li></ul><h2><span id="3-线性不可分支持向量机与软间隔">3 线性(不可分)支持向量机与软间隔</span></h2><h3><span id="31-引入软间隔后的问题">3.1 引入软间隔后的问题</span></h3><ul><li><p>引入松弛变量后的约束</p><script type="math/tex; mode=display">y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}</script></li><li><p>引入松弛变量后的目标函数</p><script type="math/tex; mode=display">\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}</script></li><li><p>线性不可分的学习问题变为如下凸二次规划问题：</p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{w, b, \xi}} & {\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}} \\ {\text { s.t. }} & {y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N} \\ {} & {\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script></li></ul><h3><span id="32-对偶问题与求解">3.2 对偶问题与求解</span></h3><ul><li><p>拉格朗日函数为</p><script type="math/tex; mode=display">L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}</script></li><li><p>对偶问题求解 —— 求$\min_{w,b,\xi} L(w, b, \xi, \alpha, \mu)$</p><ul><li><p>对各变量的偏导数</p><script type="math/tex; mode=display">\begin{array}{l}{\nabla_{w} L(w, b, \xi, \alpha, \mu)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0} \\ {\nabla_{b} L(w, b, \xi, \alpha, \mu)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {\nabla_{\xi_{i}} L(w, b, \xi, \alpha, \mu)=C-\alpha_{i}-\mu_{i}=0}\end{array}</script></li><li><p>偏导数为0的结果</p><script type="math/tex; mode=display">\begin{array}{c}{w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}} \\ {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {C-\alpha_{i}-\mu_{i}=0}\end{array}</script></li></ul></li><li><p>对偶问题求解 —— 对$\min_{w,b,\xi} L(w, b, \xi, \alpha, \mu)$求a的极大</p></li></ul><script type="math/tex; mode=display">\begin{array}{ll}{\max _{\alpha}} & {-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {C-\alpha_{i}-\mu_{i}=0} \\ {} & {\alpha_{i} \geqslant 0} \\ {} & {\mu_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script><ul><li>利用等式约束消去u，最终的结果</li></ul><script type="math/tex; mode=display">\begin{array}{c}{w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}} \\ {b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)}\end{array}</script><ul><li>并可以证明满足KKT的条件。</li></ul><h2><span id="4-非线性支持向量机与核函数">4 非线性支持向量机与核函数</span></h2><h3><span id="41-核函数">4.1 核函数</span></h3><ul><li>非线性带来高维转换（高维空间更容易线性可分）</li><li>对偶表示带来内积</li><li>核函数的定义</li></ul><script type="math/tex; mode=display">\phi(x): \mathcal{X} \rightarrow \mathcal{H}</script><script type="math/tex; mode=display">K(x, z)=\phi(x) \cdot \phi(z)</script><h3><span id="42-正定核">4.2 正定核</span></h3><ul><li>定义映射并构成向量空间S</li><li>在S上定义内积，构成内积空间</li><li>将S完备化构成希尔伯特空间</li></ul><h3><span id="43-常用的核函数">4.3 常用的核函数</span></h3><ul><li>多项式核函数</li><li>高斯核函数</li><li>sigmoid核函数</li></ul><h2><span id="小结">小结</span></h2><p>　SVM算法的主要优点有：</p><ol><li>解决高维特征的分类问题和回归问题很有效，在特征维度大于样本数时依然有很好的效果。</li><li>仅仅使用一部分支持向量来做超平面的决策，无需依赖全部数据。</li><li>有大量的核函数可以使用，从而可以很灵活的来解决各种非线性的分类回归问题。</li><li>样本量不是海量数据的时候，分类准确率高，泛化能力强。</li></ol><p>　</p><p>SVM算法的主要缺点有：</p><ol><li>如果特征维度远远大于样本数，则SVM表现一般。</li><li>SVM在样本量非常大，核函数映射维度非常高时，计算量过大，不太适合使用。</li><li>非线性问题的核函数的选择没有通用标准，难以选择一个合适的核函数。</li><li>SVM对缺失数据敏感。</li></ol><h2><span id="reference">reference：</span></h2><pre><code>《统计学习方法》《机器学习》——西瓜书Bilibili白板推导https://www.cnblogs.com/pinard/p/6113120.html</code></pre>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习/概率图模型-HMM</title>
      <link href="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/"/>
      <url>/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-马尔可夫模型">1 马尔可夫模型</span></h2><h3><span id="11-概念导入">1.1 概念导入</span></h3><p>在某段时间内，交通信号灯的颜色变化序列是：红色 - 黄色 - 绿色 - 红色。</p><p>在某个星期天气的变化状态序列：晴朗 - 多云 - 雨天。</p><p>像交通信号灯一样，某一个状态只由前一个状态决定，这就是一个一阶马尔可夫模型。而像天气这样，天气状态间的转移仅依赖于前n天天气的状态，即状态间的转移仅依赖于前n个状态的过程。这个过程就称为<strong>n阶马尔科夫模型</strong>。</p><p>不通俗的讲，马尔可夫模型（Markovmodel）描述了一类重要的随机过程，随机过程又称随机函数，是随时间而随机变化的过程。</p><h3><span id="12-马尔可夫模型定义">1.2 马尔可夫模型定义</span></h3><p>存在一类重要的随机过程：如果一个系统有N个状态$S_1$,$S_2$,$S_3$..$S_N$ 随着时间的推移，该系统从某一状态转移到另一状态。如果用$q_t$ 表示系统在时间t的状态变量，那么 t 时刻的状态取值为$S_j$(1&lt;=j&lt;=N)的概率取决于前t-1 个时刻(1, 2, …, t-1)的状态，该概率为：</p><script type="math/tex; mode=display">p\left(q_{t}=S_{j} | q_{t-1}=S_{i}, q_{t-2}=S_{k}, \cdots\right)</script><ol><li><strong>假设一：</strong>如果在特定情况下，系统在时间t 的状态只与其在时间t-1 的状态相关，则该系统构成一个<strong>离散的一阶马尔可夫链</strong>：</li></ol><script type="math/tex; mode=display">p\left(q_{t}=S_{j} | q_{t-1}=S_{i}, q_{t-2}=S_{k}, \cdots\right)=p\left(q_{t}=S_{j} | q_{t-1}=S_{i}\right)</script><ol><li><strong>假设二：</strong>如果只考虑独立于时间t的随机过程，状态与时间无关，那么<script type="math/tex; mode=display">p\left(q_{t}=S_{j} | q_{t-1}=S_{i}\right)=a_{i j}, \quad 1 \leq i, j \leq N</script>即：t时刻状态的概率取决于前t-1 个时刻(1, 2, …, t-1)的状态,且状态的转换与时间无关，则<strong>该随机过程</strong>就是<strong>马尔可夫模型</strong>。</li></ol><h2><span id="2-隐马尔可夫模型">2 隐马尔可夫模型</span></h2><h3><span id="21-概念导入">2.1 概念导入</span></h3><p>在马尔可夫模型中，每个状态代表了一个可观察的事件，所以，马尔可夫模型有时又称作可视马尔可夫模型（visibleMarkovmodel，VMM），这在某种程度上限制了模型的适应性。</p><p>对于盲人来说也许不能够直接获取到天气的观察情况，但是他可以通过触摸树叶通过树叶的干燥程度判断天气的状态。于是天气就是一个隐藏的状态，树叶的干燥程度是一个可观察的状态，于是我们就有了两组状态，一个是不可观察、隐藏的状态（天气），一个是可观察的状态（树叶），我们希望设计一种算法，在不能够直接观察天气的情况下，通过树叶和马尔可夫假设来预测天气。</p><p>以此为例，一个一阶的马尔可夫过程描述：  </p><p> <img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/clip_image004-1573140263848.jpg" alt="clip_image004" style="zoom:67%;"></p><p>在隐马尔可夫模型（HMM）中，我们<strong>不知道模型具体的状态序列</strong>，<strong>只知道状态转移的概率</strong>，即模型的状态转换过程是不可观察的。</p><p>因此，该模型是一个<strong>双重随机过程</strong>，包括<strong>模型的状态转换</strong>和<strong>特定状态下可观察事件的随机</strong>。</p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1.png" alt="1" style="zoom: 50%;"></p><h3><span id="22-hmm的组成">2.2 HMM的组成</span></h3><p>例如，N个袋子，每个袋子中有M种不同颜色的球。选择一个袋子，取出一个球，得到球的颜色。</p><ol><li>状态数为N(袋子的数量)</li><li>每个状态可能的符号数M(不同颜色球的数目)</li><li>状态转移概率矩阵A＝$a_{ij}$(从一只袋子(状态Si) 转向另一只袋子(状态Sj ) 取球的概率)</li><li>从状态Sj 观察到某一特定符号vk 的概率分布矩阵为：$B=\betaj(k)$  (从第j个袋子中取出第k种颜色的球的概率)</li><li>初始状态的概率分布为：$\pi=\pi_{i}$</li></ol><p><strong>一般将一个隐马尔可夫模型记为：$λ=[π, A,B]$</strong></p><p>需要确定以下三方面内容（三要素）：</p><ol><li>初始状态概率π：模型在初始时刻各状态出现的概率，通常记为$π=(π_1,π_2,…,π_N)$，$π_i$表示模型的初始状态为$S_i$的概率.</li><li>状态转移概率A：模型在各个状态间转换的概率，通常记为矩阵A[$a_{ij}$]，其中$a_{ij}$表示在任意时刻t，若状态为Si，则在下一时刻状态为Sj的概率.</li><li>输出观测概率B：模型根据当前状态获得各个观测值的概率通常记为矩阵<br>B=[$(\beta{ij})$]。其中，$\beta{ij}$表示在任意时刻t，若状态为$S_j$，则观测值$O_j$被获取的概率.</li></ol><p>相对于马尔可夫模型，隐马尔可夫只是多了一个各状态的观测概率</p><p>给定隐马尔可夫模型  $λ=[A, B, π]$，它按如下过程产生观测序列   ${X_1,X_2，…,X_n}$:</p><p>(1)  设置t=1，并根据初始状态概率π选择初始状态$Y_1$;</p><p>(2)  根据状态值和输出观测概率B选择观测变量取值$X_t$ ;</p><p>(3)  根据状态值和状态转移矩阵A转移模型状态，即确定$Y_{t+1}$;</p><h2><span id="3-三个问题">3 三个问题</span></h2><p>一旦一个系统可以作为HMM被描述，就可以用来解决三个基本问题。</p><p><strong>1.</strong>  <strong>评估（Evaluation）：概率计算问题</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，求某个观察序列的概率$p(x| \lambda)$。</p><p>例如：给定一个天气的隐马尔可夫模型，包括第一天的天气概率分布，天气转移概率矩阵，特定天气下树叶的湿度概率分布。<strong>求第一天湿度为1，第二天湿度为2，第三天湿度为3的概率。</strong></p><p><strong>2.</strong>  <strong>解码 or 推断（ Decoding）：预测问题</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，以及某个观察序列，求得天气的序列y。</p><p>例如：给定一个天气的隐马尔可夫模型，包括第一天的天气概率分布，天气转移概率矩阵，特定天气下树叶的湿度概率分布。并且已知第一天湿度为1，第二天湿度为2，第三天湿度为3。<strong>求得这三天的天气情况</strong>。</p><p> 即：发现“最优”状态序列能够“最好地解释”观察序列</p><p><strong>3.</strong>  <strong>学习（Learning）</strong></p><p>给定一个观察序列，得到一个隐马尔可夫模型$\lambda=[π, A,B]$。</p><p>例如：已知第一天湿度为1，第二天湿度为2，第三天湿度为3。<strong>求得一个天气的隐马尔可夫模型</strong>，包括第一天的天气，天气转移概率矩阵，特定天气下树叶的湿度概率分布。</p><h3><span id="31-前向算法">3.1 前向算法</span></h3><p><strong>对于评估问题（Evaluation）</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，求某个观察序列的概率。</p><p>例如：给定一个天气的隐马尔可夫模型，包括第一天的天气概率分布，天气转移概率矩阵，特定天气下树叶的湿度概率分布。<strong>求第一天湿度为1，第二天湿度为2，第三天湿度为3的概率。</strong></p><p><strong>思路一：找到所有状态序列，得到各状态概率，得到每种状态概率对应的观察概率，求和。</strong> </p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/clip_image006.png" alt="clip_image006" style="zoom:67%;"></p><p>即：找到每一个可能的隐藏状态，并且将这些隐藏状态下的观察序列概率相加。</p><p>对于上面那个（天气）例子，将有3^3 = 27种不同的天气序列可能性，因此，观察序列的概率是：Pr(dry,damp,soggy | HMM) = Pr(dry,damp,soggy | sunny,sunny,sunny) + Pr(dry,damp,soggy | sunny,sunny ,cloudy) + Pr(dry,damp,soggy | sunny,sunny ,rainy) + . . . . Pr(dry,damp,soggy | rainy,rainy ,rainy)</p><p>用这种方式计算观察序列概率极为昂贵，特别对于大的模型或较长的序列，因此我们可以利用这些概率的时间不变性来减少问题的复杂度。</p><p><strong>思路二：采用动态规划——前向算法</strong></p><p> 基本思想：定义前向变量$α_t(i)$  ：t时刻状态为$S_i$且观察状态为$O_t$ 的概率</p><script type="math/tex; mode=display">\alpha_{t}(i)=p\left(O_{1} O_{2} \cdots \underline{O_{t}}, q_{t}=S_{i} | \lambda\right)</script><p> 如果可以高效地计算$α_t(i)$，就可以高效地求得$p(O|\lambda)$。</p><p> <img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571903774343.png" alt="1571903774343" style="zoom:50%;"></p><p>蓝色部分是：从t时刻的各个状态$S_i$得到t时刻的观察状态$O_t$的概率</p><p>黑色部分是：从t时刻的各个状态$S_i$得到t+1时刻的各个状态</p><p> 即：</p><ol><li>初始化：$\alpha_{1}(i)=\pi_{i} \beta_{i}\left(O_{1}\right), \quad 1 \leq i \leq N$</li><li>循环计算：$\alpha_{t+1}(j)=\left[\sum_{i=1}^{N} \alpha_{t}(i) a_{i j}\right] \times \beta{j}\left(O_{t+1}\right), \quad 1 \leq t \leq T-1$</li><li>结束，输出：$p(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$</li></ol><h3><span id="32-后向算法">3.2 后向算法</span></h3><p>定义后向变量$\beta_t(i)$是在给定了模型 $\lambda=[π, A,B]$和假定在时间t 状态为$S_i$的条件下，模型输出<br>观察序列$O_{t+1} O_{t+2} \cdots {O_{T}}$ 的概率：</p><script type="math/tex; mode=display">\beta_{t}(i)=p\left(O_{t+1} O_{t+2} \cdots O_{T} | q_{t}=S_{i}, \mu\right)</script><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571904725005.png" alt="1571904725005" style="zoom:50%;"></p><p>蓝色部分是：从t+1时刻的各个状态$S_i$得到t+1时刻的观察状态$O_t$的概率</p><p>黑色部分是：从t+1时刻的各个状态$S_i$得到t时刻的各个状态</p><p> 即：</p><ol><li>初始化：$\beta_{T}(i)=1, \quad 1 \leq i \leq N$</li><li>循环计算：$\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} \beta{j}\left(O_{t+1}\right) \times \beta_{t+1}(j), \quad T-1 \geq t \geq 1, \quad 1 \leq i \leq N$</li><li>结束，输出：$p(O | \lambda)=\sum_{i=1}^{N} \beta_{1}(i) \times \pi_{i} \times \beta{i}\left(O_{1}\right)$</li></ol><h3><span id="33-viterbi-搜索算法">3.3 Viterbi 搜索算法</span></h3><p><strong>对于解码问题（ Decoding）</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，以及某个观察序列，求得天气的序列。</p><p>例如，给定一个天气的隐马尔可夫模型，包括第一天的天气，天气转移概率矩阵，特定天气下树叶的湿度概率分布。并且已知第一天湿度为1，第二天湿度为2，第三天湿度为3。求得这三天的天气情况。</p><p> 即：发现“最优”状态序列能够“最好地解释”观察序列</p><h4><span id="331-如何理解最优的状态序列">3.3.1 如何理解“最优”的状态序列？</span></h4><p>解释(1)：</p><p>状态序列中的每个状态都单独地具有概率，对于每个时刻$t(1 \leq t \leq T)$, 寻找$q_t$ 使得$\gamma_{t}(i)=p\left(q_{t}=S_{i} | O, \lambda\right)$最大。——<strong>近似算法</strong></p><p>问题：每一个状态单独最优不一定使整体的状态序列最优，两个最优的状态之间的转移概率可能为0</p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905343398.png" alt="1571905343398" style="zoom:50%;"></p><p>解释(2)：在给定模型$\lambda$和观察序列O的条件下求概率最大的状态序列<strong>——Viterbi 算法: 动态搜索最优状态序</strong></p><script type="math/tex; mode=display">\widehat{Q}=\underset{Q}{\arg \max } p(Q | O, \mu)</script><h4><span id="332-viterbi-搜索算法过程">3.3.2 Viterbi 搜索算法过程</span></h4><p>搜索过程大概如下：</p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905575248.png" alt="1571905575248" style="zoom:50%;"></p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905516366.png" alt="1571905516366" style="zoom:50%;"></p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905527009.png" alt="1571905527009" style="zoom:50%;"></p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905543642.png" alt="1571905543642" style="zoom:50%;"></p><p>每个时刻都求得每个状态的概率，并求得最大概率对应的上一时刻的状态。</p><p><strong>算法描述</strong></p><ol><li><p>初始化：$\delta_{1}(i)=\pi_{i} \beta_{i}\left(O_{1}\right), \quad 1 \leq i \leq N$</p><p>概率最大的路径变量：$\psi_{1}(i)=0$</p></li><li><p>递推计算：$t$ 是计算的顺序数（计算第 t 个），$j$  是计算过程中的第 j 个状态</p><p>$\delta_{t}(j)=\max _{1 \leq i \leq N}\left[\delta_{t-1}(i) \cdot a_{i j}\right] \cdot \beta_{j}\left(O_{t}\right), \quad 2 \leq t \leq T, \quad 1 \leq j \leq N$</p><p>$\psi_{t}(j)=\underset{1 \leq i \leq N}{\arg \max }\left[\delta_{t-1}(i) \cdot a_{i j}\right] \cdot \beta_{j}\left(O_{t}\right), 2 \leq t \leq T, 1 \leq i \leq N$</p></li><li><p>结束：</p><p>$\widehat{Q}_{T}=\underset{1 \leq i \leq N}{\operatorname{argmax}}\left[\delta_{T}(i)\right], \quad \widehat{p}\left(\widehat{Q}_{T}\right)=\max _{1 \leq i \leq N} \delta_{T}(i)$</p></li><li><p>通过回溯得到路径（状态序列）：</p><p>$\widehat{q}_{t}=\psi_{t+1}\left(\widehat{q}_{t+1}\right), \quad t=T-1, T-2, \cdots, 1$</p></li></ol><h3><span id="34-参数学习">3.4 参数学习</span></h3><p><strong>对于学习问题（Learning）</strong></p><p>给定一个观察序列，得到一个隐马尔可夫模型。</p><p>已知第一天湿度为1，第二天湿度为2，第三天湿度为3。求得一个天气的隐马尔可夫模型，包括第一天的天气，天气转移概率矩阵，特定天气下树叶的湿度概率分布。</p><p>如果产生观察序列O的状态已知(即存在大量标注的样本), 可以用最大似然估计来计算 $\lambda$ 的参数：Baum-Welch 算法(前向后向算法)描述</p><p>如果不存在大量标注的样本：EM算法—期望值最大化算法(Expectation-Maximization, EM) </p><h2><span id="4-hmm应用">4 HMM应用</span></h2><p>中文分词问题，从句子的序列标注问题解决。BMES</p><p>如果有标注语料，则问题的解决过程：</p><ol><li>计算初始状态概率分布（初始字符的BMES概率）</li><li>计算转移概率矩阵（BMES之间转移的概率）</li><li>计算输出概率矩阵（BMES转为字符的概率）</li><li>使用Viterbi算法解码</li></ol><p>如果没有标注语料，则问题的解决过程：</p><ol><li>获取词的个数</li><li>确定状态的个数</li><li>参数学习（利用EM迭代算法获取初始状态概率、状态转移概率和输出概率）</li><li>使用Viterbi算法解码</li></ol><h2><span id="5-最大熵马尔可夫模型">5 最大熵马尔可夫模型</span></h2><p>普通的马尔可夫模型，考虑了隐状态的转移概率和当前隐状态到当前观测状态的输出概率。实际上，隐状态隐状态不仅仅与单个观测状态相关，还和观测状态的长度、上下文等信息相关。</p><p>最大熵马尔可夫模型考虑了所有的观测状态。</p><p>并且在局部进行了归一化，即枚举全部隐藏状态求和后计算概率。</p><p>但是，其存在标注偏置问题。</p><p>CRF在最大熵马尔可夫模型的基础上，加入了全局归一化。</p><h2><span id="reference">Reference</span></h2><p>宗成庆：《自然语言处理》讲义</p><p>《统计学习方法》</p><p>其他网络资料（侵删）</p>]]></content>
      
      
      <categories>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率图模型-HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer的简洁向解释</title>
      <link href="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/"/>
      <url>/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p><img src="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574773800747.png" alt="1574773800747" style="zoom:67%;"></p><h2><span id="1-主要结构">1 主要结构</span></h2><p>​        <strong>1. 输入：Input Embedding——$(x_1,x_2,x_3,…,x_n)$ 进行Positional Encoding，投入Encoder；</strong></p><p>​        <strong>2. Encoder：编码处理后输出——$(z_1,z_2,z_3,…,z_n)$，并将其作为Decoder的输入；</strong></p><p>​        <strong>3. Decoder：进行解码处理；</strong></p><p>​        <strong>4. 输出：最终对Decoder的输出进行处理最终的概率$(y_1,y_2,y_3,…,y_m)$</strong></p><p>​        <strong>注意！</strong>Encoder和Decoder都是并行计算的N（论文取N=6）个相同结构的堆叠。</p><h2><span id="2-encoder">2 Encoder</span></h2><h3><span id="21-encoder整体">2.1 Encoder整体</span></h3><p><img src="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574779976637.png" alt="1574779976637" style="zoom:33%;"></p><p>​        Encoder部分有两个sub-layer，<strong>Multi-Head Attention</strong>和<strong>Feed Forward</strong>。上图的Add代表残差网络，由图可知在每个sub-layer都加入了残差项。其中的Norm是Layer Normalization。</p><p>​        Multi-Head Attention是本论文的核心，主要是self Attention；Feed Forward是一个简单的全连接前馈神经网络。</p><h3><span id="22-encoder的输入">2.2 Encoder的输入</span></h3><p>​        Encoder端每个大模块接收的输入是不一样的，第一个大模块(最底下的那个)接收的输入是输入序列的embedding，其余大模块接收的是其前一个大模块的输出，最后一个模块的输出作为整个Encoder端的输出。  </p><h3><span id="23-multi-head-attention">2.3 Multi-Head Attention</span></h3><p><img src="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574775307731.png" alt="1574775307731" style="zoom: 33%;"></p><p>​        </p><p>​        对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。单个Multi-Head Attention层的输入进行处理得到QKV，通过线性变换输入到Scaled Dot-Product Attention，得到多组结果进行concat并加权后，作为Encoding后的结果。（QKV是什么见下一小节）</p><p>​        整个过程的公式：</p><script type="math/tex; mode=display">\begin{aligned} \text { MultiHead }(Q, K, V) &\left.=\text { Concat (head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>​        </p><h3><span id="24-scaled-dot-product-attention">2.4 Scaled Dot-Product Attention</span></h3><p><img src="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574777556699.png" alt="1574777556699" style="zoom:33%;"></p><p>​        Scaled Dot-Product Attention是Transformer较为重要的核心部分，整个过程：</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ol><li>Q和K计算相似度，此处使用点积的方法；</li><li>防止结果过大，除以${\sqrt{d_{k}}}$，其中${d_{k}}$是K的维度；（内积太大的话softmax后就非0即1了,不够“soft”了）（为什么除以${\sqrt{d_{k}}}$: 让qk的方差与dk无关，从${d_{k}}$变为1，参考：<a href="https://blog.csdn.net/qq_37430422/article/details/105042303）" target="_blank" rel="noopener">https://blog.csdn.net/qq_37430422/article/details/105042303）</a></li><li>经过一个Mask操作； Q，K长度是不定时，进行补齐操作，将补齐的数据设置为负无穷</li><li>进行softmax归一化得到Q和K的Attention；</li><li>Attention与V相乘，得到self-Attention的结果。</li></ol><p>​        这里的QKV论文里没有详细展开，现有的博客文章很少提到，本文引用的第二篇博客中有段话写的很好。Attention机制中，将Source中看做是由一系列的(Key,Value)对构成，此时给定某元素Query，计算Query和各个Key的相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</p><p><img src="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/att7.png" alt="image" style="zoom:33%;"></p><p>​        而对于本文的self-Attention来说，key、value和query都是其本身，也就是上一层的输出，作为下一层的输入。</p><h2><span id="3-decoder">3 Decoder</span></h2><p>​        Decoder部分有三个sub-layer，<strong>Masked Multi-head Attention</strong>、<strong>Encoder-Decoder Attention</strong>和<strong>Feed Forward</strong>。上图的Add和Norm与Encoder部分一致，分别代表残差网络和Layer Normalization。</p><p>​        decoder相对encoder，有两个不同的地方，一是第一级的Masked Multi-head，二是第二级中Multi-Head Attention的输入。</p><p><img src="/2019/11/28/shen-du-xue-xi-yu-nlp-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574780008243.png" alt="1574780008243" style="zoom:33%;"></p><p>​    </p><h3><span id="31-masked-multi-head-attention">3.1 Masked Multi-head Attention</span></h3><p>​        Masked Multi-head是decoder的第一级decoder，其key, query, value均来自前一层decoder的输出。</p><p>​        <strong>但其加入了Mask操作，因为翻译过程我们当前还并不知道下一个输出哪个词语。</strong></p><p>​        训练过程中，因为在实现中无法每次动态的输入，就一次性把目标序列通通输入第一个大模块中，然后在Multi-head Attention中对序列进行mask即可。</p><p>​        在测试过程中，先生成第一个位置的输出，第二次预测时，再将其加入输入序列，以此类推直至预测结束。</p><h3><span id="32-encoder-decoder-attention-attention">3.2 Encoder-Decoder attention Attention</span></h3><p>​        Decoder中第二级decoder是Multi-Head Attention，论文中也称为”encoder-decoder attention”。它不仅接受来自前一级的输出，还接收encoder的输出。</p><p>​        它的<strong>query来自于之前一级的decoder层的输出</strong>，但其<strong>key和value来自于encoder的输出</strong>，这使得decoder的每一个位置都可以attend到输入序列的每一个位置。</p><p>​        总结一下，k和v的来源总是相同的，Q在encoder及第一级decoder中与K，V来源相同，在encoder-decoder attention layer中与K,V来源不同。</p><h2><span id="4-其他细节">4 其他细节</span></h2><h3><span id="41-position-wise-feed-forward-networks">4.1 Position-wise Feed-Forward Networks</span></h3><p>​        除了Attention子层之外，其他子层都包含了一个全连接的前馈网络。包括两个线性变换，中间有一个ReLU：</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><h3><span id="42-positional-encoding">4.2 Positional Encoding</span></h3><p>​        模型结构本质上是忽视了数据的序列信息，因此为了充分利用序列信息，将tokens的相对和绝对位置编码进输入Embedding。</p><p>​        论文中使用sine and cosine的方式计算出来（称其效果与训练出来的比较接近），且结果与embedding维度相同。然后将Positional Encoding与embedding 进行加和。</p><script type="math/tex; mode=display">\begin{array}{c}{P E_{(p o s, 2 i)}=\sin \left(\text {pos} / 10000^{2 i / d_{\text {madel }}}\right)} \\ {P E_{(\text {pos}, 2 i+1)}=\cos \left(\text {pos} / 10000^{2 i / d_{\text {madit }}}\right)}\end{array}</script><h2><span id="5-常见问题总结">5 常见问题总结</span></h2><ul><li>Transformer的结构是什么样的？<br>Transformer本身还是一个典型的encoder-decoder模型：<ol><li>Encoder由N(原论文中<strong>N=6</strong>)个相同的大模块堆叠而成，其中每个大模块又由<strong>两个sub-layer</strong>构成，这两个子模块分别为Multi-Head Attention模块，以及一个前馈神经网络模块；</li><li>Decoder端同样由N个相同的大模块堆叠而成，其中每个大模块则由<strong>三个sub-layer</strong>构成，这三个子模块分别为Masked Multi-Head Attention模块，本质也是Multi-Head Attention的encoder-decoder attention模块，以及一个前馈神经网络模块；  </li><li>在所有sub-layer中，都附加了残差网络和layer normalization。</li></ol></li><li>Multi-Head Attention的具体结构？<br>Multi-Head Attention的结果是QKV进行不同的变换后，再进行self-Attention后进行concat得到的。</li><li>self-Attention具体内容？<br>Q和K计算相似度，此处使用点积的方法；防止结果过大，除以${\sqrt{d_{k}}}$，其中${\sqrt{d_{k}}}$是K的维度；进行softmax归一化得到Q和K的Attention；Attention与V相乘，得到self-Attention的结果。</li><li>Self-Attention为什么work？<br>self-attention的特点在于<strong>无视词(token)之间的距离直接计算依赖关系，从而能够学习到序列的内部结构</strong></li><li>Transformer相比于RNN/LSTM，有什么优势？为什么？<br>RNN由于序列依赖的原因，并行计算能力比较差；Transformer的特征提取能力比RNN系列强。</li><li>Transformer与Seq2Seq相比？<br>seq2seq最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>，并将其进行解码。在输入信息较长时，会损失大量的信息。而且这样整体交给Decoder进行编码，Decoder很难关注到真正要关注的信息。</li><li>Transformer是如何加入词序信息的？<br>模型结构本质上是忽视了数据的序列信息，因此为了充分利用序列信息，将tokens的相对和绝对位置编码进输入Embedding。论文中使用sine and cosine的方式计算出来，且结果与embedding维度相同。然后将Positional Encoding与embedding 进行加和。</li><li>为什么transformer块使用LayerNorm而不是BatchNorm？<br>从LayerNorm的优点来看，它对于batch大小是健壮的，并且在样本级别而不是batch级别工作得更好。</li><li></li></ul><h2><span id="6-参考文章">6 参考文章</span></h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">https://arxiv.org/abs/1706.03762</a></p><p><a href="https://zhuanlan.zhihu.com/p/47063917" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47063917</a></p><p><a href="https://www.cnblogs.com/huangyc/p/10409626.html" target="_blank" rel="noopener">从Encoder-Decoder(Seq2Seq)理解Attention的本质</a></p><p><a href="https://www.nowcoder.com/discuss/258321?type=0&amp;order=0&amp;pos=19&amp;page=1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/258321?type=0&amp;order=0&amp;pos=19&amp;page=1</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习与NLP基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer的简洁向解释 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++常用API</title>
      <link href="/2019/09/03/yu-yan-gong-ju-ji-zhu-deng-wen-dang/c-chang-yong-api/"/>
      <url>/2019/09/03/yu-yan-gong-ju-ji-zhu-deng-wen-dang/c-chang-yong-api/</url>
      
        <content type="html"><![CDATA[<p>Trick</p><pre><code>第一个判浮点数相等要用fabs(x -y) &lt;= eps，不能用x==y，const double eps = 1e-6;判断奇偶数，用x%2!=0 而不是x%2==1，因为c++里余数可以是奇数</code></pre><p>常用</p><pre><code>rand()%100; 生成0-99的随机数INX_MAXINT_MIN// 遍历for(int i=0;i&lt;n;i++) a[i]for(char c: a) c</code></pre><p>arrary</p><pre><code>int number[5];//= {1,2,3,4,5};    //数组初始化int *number=new int[5];        //数组初始化int[][] dp = new int[5][5];   //数组初始化swap(number[i],number[j]);</code></pre><h1><span id="include">include<algorithm></algorithm></span></h1><pre><code>sort(array,array+length);     //0-(length-1)数组排序static bool cmp(const int &amp;a,const int &amp;b) return a&gt;b;   //没有官方封装的快 静态成员函数与普通成员函数的根本区别在于：普通成员函数有 this 指针，可以访问类中的任意成员；而静态成员函数没有 this 指针，只能访问静态成员（包括静态成员变量和静态成员函数）。sort(array,array+length,cmp);  //0-(length-1)数组排序-降序</code></pre><h1><span id="include">include<math></math></span></h1><pre><code>fabs(number); //绝对值,abs只针对整数sqt(number); //开方</code></pre><h1><span id="include">include<string></string></span></h1><pre><code>str.size();  str.length();memset(a,0,sizeof(a));string subS=str.substr(begin_index,length);  //获取子串string subS=str.substr(begin_index);position = str.find(&quot;jk&quot;);    int b = to_string(a);</code></pre><h1><span id="include">include<vector></vector></span></h1><pre><code>//初始化vectorvector&lt;int&gt; v;  vector&lt;int&gt; v(n+1,-1); //n+1个数，初始化-1vector&lt;int&gt;({1,2,3,4});//直接加入元素vector&lt;int&gt;(); //返回一个空的容器vector&lt;vector&lt;int&gt;&gt; v;vector&lt;vector&lt;int&gt;&gt; v(n,vector&lt;int&gt;(n,0));vector&lt;string&gt; board(n,string(n,&#39;.&#39;));   v.size();    v[0].size();v.empty();  判断是否空v.push_back(x);v.pop_back(x);  v.clear(); //清空元素，但不回收空间v.erase(idx); // 清空某个元素v.erase(idx, last) // 清空[idx, last) 的区间swap(v[i],v[j]);sort(result.begin(),result.end());  //排列count(nums.begin(), nums.end()); //计数reverse(nums.begin(),nums.end());   //翻转reverse(nums.begin(),nums.begin()+k);   //翻转前k个</code></pre><h1><span id="include">include<stack></stack></span></h1><pre><code>stack&lt;int&gt; s;s.empty();s.push(value);s.top();s.pop();s.size();</code></pre><h1><span id="include">include<queue></queue></span></h1><pre><code>queue&lt;int&gt; q;q.empty();q.push(value);q.front();s.pop();</code></pre><h1><span id="include">include<map></map></span></h1><pre><code>map&lt;int,int&gt; m;   //初始化 int类型的value初始化默认为0m[key]=val;     //赋值m.erase(key);   m.erase(iter);   //去掉-key或指针m.find(key)!=m.end();for(auto iter=m.begin();iter!=m.end();iter++){    cout &lt;&lt; iter-&gt;first &lt;&lt; &quot; : &quot; &lt;&lt; iter-&gt;second &lt;&lt; endl; //注意是&quot;-&gt;&quot;不是&quot;.&quot;&quot;}for(auto &amp;m : M){    cout&lt;&lt;m.first&lt;&lt;endl;}//map中的元素按照key顺序排列，在对顺序有要求的问题中使用map，查找速度上慢于哈希实现的unordered_map</code></pre><h1><span id="include">include<priority_queue></priority_queue></span></h1><pre><code>priority_queue&lt;int&gt; Q;  //大顶堆# priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt;&gt; Q;  //大顶堆priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt;&gt; // 小顶堆 Q;//小顶堆priority_queue&lt;pair&lt;int,int&gt;&gt; pQ;    //pairpQ.push(make_pair(value1,value2));Q.top();Q.empty();Q.size();Q.pop();</code></pre><p>===========================================================================</p><h1><span id="include">include<set></set></span></h1><pre><code>unordered_set&lt;int&gt; hash;unordered_set &lt;int&gt; hash(_vector.begin(), _vector.end());hash.find(val)!=hash.end();hash.insert(val);*(hash.begin())   // 首个值</code></pre><h1><span id="include">include<list></list></span></h1><pre><code>list&lt;int&gt; l;l.begin();  l.end();l.size();list&lt;int&gt;::iterator current=l.begin();l.erase(current);l.push_back(val);</code></pre><h1><span id="include-双向队列">include<deque>  //双向队列</deque></span></h1><pre><code>deque&lt;int&gt; q;q.push_back(value);q.empty();q.front();q.back();q.pop_back();q.pop_front()</code></pre><h1><span id="include-自动排序的set">include<multiset>  //自动排序的set</multiset></span></h1><pre><code>multiset&lt;int, greater&lt;int&gt; &gt; leastNums;  //从大到小排序leastNums.end();  //最后一个元素之后的迭代器，不是最后一个元素leastNums.insert(val);leastNums.erase(least.begin());result=vector&lt;int&gt; (leastNumbers.begin(),leastNumbers.end());</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++常用API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保研的科普与准备攻略</title>
      <link href="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/"/>
      <url>/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>此文章适用于希望保研的大一大二大三学弟学妹们，尤其是刚入学的新生</p><p>文章的开头，对帮助过我的非常nice的学长学姐老师们表达感谢。</p><p>​       在我初高中，甚至更小的时候，我就听说了“考研”这个词，被父母和周围人灌输了考研可以获得更高的学历。而大学刚入学，就接触到了几个大我几届的学长，第一次听说了“保研”这个词。</p><p>​       现在基本所有的前985、211高校和一些重点本科高校，都会有教育部划拨的保送名额，在几年前，保研意味着你不需要考试，就可以留在“本校“读研，而随着最近几年政策的逐渐放开，“外推”成为了更普遍的保研出路。而我只针对当下的政策和形式进行解读，不排除今后保研政策改变的可能。记住，询问他人得到的结果远远不如带有印章的政策文件来的踏实。</p><p>​       保研，应该说是：推荐免试研究生。与考研作对比，考研需要初试（全国硕士研究生入学统一考试）和复试（考生在通过初试的基础上，对考生业务水平和实际能力的进一步考察）。而保研只需要通过学校对你的考核，不再需要全国统一的考试，就可以去目标学校读研。</p><p>​       </p><h2><span id="1-保研介绍">1 保研介绍</span></h2><h3><span id="11-保研名额的获取">1.1   保研名额的获取</span></h3><p>保研的第一步是获得本校的保研资格，而这个保研资格是根据学校制定的政策，同专业进行竞争得来得。每年教育部会给各大高校划拨保研名额，根据之前制定的保研政策，一些相对优秀的同学通过成绩或者竞赛等加权得来。</p><p>以东北大学15级学生为例，参照《东北大学学生手册》，东北大学关于推荐优秀应届本科毕业生免试攻读研究生工作办法。大致分为两种：具有突出竞赛奖项的、学业成绩（GPA）较高的。当然18级新生的保研政策已经修改，仔细研读文件即可，这里不再赘述。注意：不要道听途说，以政策文件为准。</p><h3><span id="12-外推过程">1.2 外推过程</span></h3><p>​       拿到了本校的保研名额，还需要意向学校的接收。最终保研的结果，是在9月末学信网上的填报系统为准，那些夏令营、九月推免都是为了让意向高校预录取你的过程。</p><p>​       大致的外推途径有：</p><ol><li>各高校某些学院举办的夏令营（大多在6、7月份）</li><li>各高校各大学院举办的预面试（九月中下旬，俗称九月推免）</li><li>一些独立实验室的招生（例如南大的lamda，哈工大SCIR等）</li><li>一些较差学校的补录（填报完系统之后，对于层次较高的院校基本不涉及）。</li></ol><p>这几个阶段如果被学校预录取，就坐等九月末填报系统就ok了。</p><h3><span id="13-夏令营和九月推免">1.3  夏令营和九月推免</span></h3><p>大多数夏令营一般是在6-7月份，计算机相关的夏令营大多在7月上旬和中旬。夏令营难度大、竞争极强。但是会有很多的机会拿到好导师好学校的offer。</p><p>九月推免竞争相对较少，对于举办过夏令营的高校和学院基本就是补补漏，好导师好方向已经没坑了；对于没有举办夏令营的高校和学院，竞争也是较大的。</p><p>对于计算机相关来说，夏令营和九月推免的考核大同小异，面试+机试（可能没有）+笔试（可能没有）。之后在准备那里会详述。</p><h2><span id="2-保研准备">2. 保研准备</span></h2><p>为了保送到较好的高校，早做准备是非常值得的。</p><h3><span id="21-本校的保研资格">2.1 本校的保研资格</span></h3><p>​       首先，为了拿到本校的保研资格，你需要好好考试、提高绩点、参加竞赛等等，尽自己的努力去拿到这个名额。</p><p>​       例如：排名尽可能的高、去争取学校认可的国家级比赛一等奖等等。做哪些取决于保研资格认定的需要。例如争取双优（如果不懂，参考学生手册）等等。</p><p>​       </p><h3><span id="22-外校的接收">2.2 外校的接收</span></h3><p>​       本校的保研资格，在政策上写的清清楚楚，但是如何拿到外校的资格，网上的信息就比较眼花缭乱了。</p><p>​       那么，在夏令营和九月推免期间哪些是体现自己竞争力的点呢？这里我以计算机相关学科为例（排名有先后）：</p><ol><li><p>学校背景。不管怎么样，本科背景是最重要的，但也是目前的你改变不了的。如果本科牌子够响就能争取到更多的机会，不够响，就只能通过自己的实力让导师刮目相看。</p></li><li><p>专业绩点排名比例。绩点排名是体现综合水平最直观的指标，是你跨入各大高校的门槛，也是考核时的重要参考依据之一。同一个高校不同专业排名的同学的去向是真的会差很多的。</p></li><li><p>科研&amp;项目。我把这项放在竞赛之前，是因为这一点会涉及到你的知识域，任何一个老师都会喜欢一个已经学会很多内容的学生，而不是需要从0开始的学生。如果能跟着实验室的老师发paper（论文）就更好了。</p></li><li><p>竞赛。竞赛放到后面，不是因为含金量小，是因为面试的时候可以说的内容较少。当然一块ACM金牌银牌这种含金量的比赛肯定是要放在3之前，但是一般的比赛例如数学建模、蓝桥杯等等含金量还是不如一些科研和竞赛的。其实换个角度讲，做一些能说的出来的比赛，例如一些算法大赛、kaggle和天池的数据竞赛等等也还是很有帮助的。</p></li><li><p>英语四六级。至少得过，不过六级会过不了很多学校的初筛，一些经管类的六级分数要求也是特高的。</p><p>最重要的是你的表达能力。如何将你的优秀展示出来，如何展示自己的亮点在众同学中脱颖而出，如果言语得体有礼貌并获得老师的好感，这才是最重要的。</p></li></ol><p>于是，我们可以看到，在拿到本校的保研名额后，如果想保送到较好的学校，你需要在大学的前三年，去：提高绩点、尽可能的参加学校实验室的科研or项目、尽可能的去参加省级国家级的比赛、提高四六级成绩甚至考个雅思托福等等。</p><h3><span id="23-临近保研的准备">2.3 临近保研的准备</span></h3><p>​       在前三年，不论是绩点、竞赛还是其他什么，该有的也都有了。但是很多同学都比较迷，自己的水平应该申请怎样的学校。</p><p>对我们人数较多的大专业而言，同专业学长学姐的去向非常有参考价值。我从学院官网和学长那里得来了他们的排名和去向，写几行代码merge了一下，得到如下：</p><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image002.jpg" alt="https://pic4.zhimg.com/80/v2-b06c07aedc1ceb5fb2504fcc25f3eb92_hd.jpg"></p><p>这些学长学姐很多都有很有含金量的竞赛、科研等等，因此也并不是排名越高去向越好，但是基本锁定了自己想去并且能去的高校区间。</p><p>院校定下来之后，就要考虑自己想读什么方向，这方面就根据自己的兴趣和行业发展去选择并大胆申请就好了。</p><p>​       在院校的选择上，有些人不惜牺牲学校档次追求导师水平，有些人追求名校光环不care导师和方向。在这些选择上，仁者见仁智者见智，追求自己想要的就好。当然，如果想要导师、方向、院校兼顾也未尝不可，提升自己的竞争力就ok了。</p><h3><span id="24-保研材料的准备">2.4 保研材料的准备</span></h3><p>我所做的准备如下：</p><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image002.jpg" alt="https://pic3.zhimg.com/80/v2-d2ea3a3f8287a1fe2fecc051d6e4a72a_hd.jpg"></p><ol><li>材料准备</li></ol><p>包括：</p><p>材料证明（成绩单、证书、排名证明、四六级成绩单、身份证等等，建议用扫描全能王app拍下来保存，打印也方面）、简历和个人陈述（简历很重要！）、夏令营要准备的材料（分各个学校，按照文件要求打包整理）、</p><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image004.jpg" alt="https://pic4.zhimg.com/80/v2-a4167cef57b96ff9a8aef5db3ec64308_hd.jpg"></p><ol><li>参考材料</li></ol><p>各个学校各大学院的招生简章和政策、同专业学长的排名和去向、各个学校各大学院历年招收的名单（用于参考）等</p><ol><li>联系老师</li></ol><p>各个相中的老师资料、跟每个老师要发的不同的邮件内容（在框架上加一些具体的信息针对不同的老师去发邮件）</p><ol><li>知识填坑</li></ol><p>参加夏令营前要复习的基础知识的整理、所做项目的整理等等</p><ol><li>网站收藏</li></ol><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image006.jpg" alt="https://pic4.zhimg.com/80/v2-4eaf2597a9dfad45e6967e8a392f7c6b_hd.jpg"></p><p>很直白了，自己看吧</p><p>其实，最最重要的还是你的简历。怎么更好的展示自己，怎么让你被导师吸引，怎么体现你的闪光点，怎么让你脱颖而出。基本缕清简历，就缕清了自己的竞争力。我的简历经过大大小小几十次改版，也慢慢变得完善。在此再次感谢帮助过我的学长学姐和老师。</p><p>保研，虽说是保送，但如果大学期间不好好努力，保研期间不好好准备，就会浪费掉很好的深造机会。</p><p>早点了解、早点准备总归是好事，但愿你们不会像我一样发出：“我要是早点开始准备就能balabala”这种感叹。</p><p>最后，预祝大家保研顺利，将来都能被理想的学校录取！~</p><p>再次感谢帮助我支持我的人！</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研的科普与准备攻略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>来自良心学长的东北大学新生攻略</title>
      <link href="/2019/07/30/jing-yan-fen-xiang/lai-zi-liang-xin-xue-chang-de-dong-bei-da-xue-xin-sheng-gong-lue/"/>
      <url>/2019/07/30/jing-yan-fen-xiang/lai-zi-liang-xin-xue-chang-de-dong-bei-da-xue-xin-sheng-gong-lue/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>​       这两天学校又搭起了一排排的彩虹门，一个个略显稚嫩的新面孔踏进了这个校园，一如我当年踏进校园的模样，还记得三年前南湖二舍东门口的条幅上：“东北大学，梦想开始的地方”，是的，梦想开始的地方。</p><p>“倘若当时…就好了”大概是最遗憾但是却最不可能避免的想法了。而从我入学到现在，虽然有很多学长学姐老师们热心的帮助，但是也不免走了很多弯路。</p><p>最近比较闲，打算分享一些自身的经验、大学期间试过的坑、吃过的亏亦或是一些可喜的收获，给将要（由于拖延症的关系，这里改成：已经）入学的新生或者离毕业还有一段时间的学弟学妹一些建议，提供一些力所能及的帮助。</p><p>如果还有大家关心的问题，欢迎在评论区提出，如果我了解一些的话，会尽可能提出一些力所能及的建议。此文所有的观点都是个人的经历带来的感受，切不可模仿，倘若能起到参考的作用，那就很ok了，如果有不太对的地方，欢迎批评指正。</p><h1><span id="1-学习相关">1. 学习相关</span></h1><p>在我还没入学的时候，就认识了一个同专业的学姐，这个学姐帮助了我很多，最最重要的是教会了我大学里最重要的是学习，虽然在一定意义上有失偏颇，但是却令我受益匪浅。所以我打算先谈谈学习相关。</p><h2><span id="11-成绩相关攻略">1.1 成绩相关攻略</span></h2><h3><span id="111-gpa绩点">1.1.1 GPA/绩点</span></h3><p>东北大学的学生都免不了接触到GPA/绩点这个词，通过学分作为权重的方法进行加权平均得到一个5以内的小数，它代表了你的学习成绩。通常，GPA与你的奖学金、保研、专业分流、转专业、出国交流机会等等息息相关，一个学霸的重要评判标准就是他的绩点。</p><p>GPA是你所有课的成绩计算得来的，包括选修课，包括人文选修。有个叫学分的东西，代表这门课的权重，学分越高，占比越大。</p><h3><span id="112-培养计划">1.1.2 培养计划</span></h3><p>你可以登陆到东北大学教务处，有个叫培养计划的东西，显示了你大学的所有课程和学分，学分没有修够是不允许毕业的，课程类型例如学位课、鼓励选修课等等在上面都可以看到，这里不再赘述。</p><p>大部分学生到了大二才了解到培养计划这个东西，建议有心的学弟学妹早点看看，应该会有一点点帮助。</p><h3><span id="113-考试攻略">1.1.3 考试攻略</span></h3><p>​       想要奖学金，想要保研，必须要把GPA尽量提高，那么这就遇到一个问题，绩点怎么提，课怎么考，分怎么高？</p><p>​       在大学，已经不像高中，要天天学，天天做题。大学生，学的好，不一定考的高！如果你想学的好点，花费时间学习理论并加以动手实践。但是有很多课程并不能跟上时代的发展，你今后的学习和工作并不能用到它，那该怎么办呢？这就涉及到大学的学习和考试技巧了。</p><p>​       相信看完培养计划，你就可以把所有的课大致分成：学位课（专业课）、学位课（非专业课）、专业选修课、非专业选修课、实践课程。我会根据课程不同类型，给大家一些提高分数的建议。</p><p><strong>1.</strong> <strong>学位课（专业课）—平时分+考试分</strong></p><p>这就是我们大学期间最重要的一类课程了，这类课程的代表有通识课程之类的高等数学线性代数等，有跟专业联系紧密的—以计算机相关专业为例有：数据结构，计算机网络等。这类课程大部分来说以后是都能用到的，考研保研找工作乃至以后的基础专业技能等等，因此不仅要学的好，还要考的好。</p><p>学：老师教的好的话，跟着老师走就ok，老师教的不好的话，找点网课课外资料什么的，也能学的很好。</p><p>考：还是那句话，学的好不一定考的好。想考好，首先看老师有没有划范围，有没有历年题，有的话就使劲参考。平常的学习，一定不要脱离课本，例如高数，期末考的大部分课后题都有，这种课到最后复习再去刷同济数学题就不太明智了。其实最简单的方法就是：找个去年考的高的学长问问他怎么考的。这里再次感谢帮助过我的学长学姐。</p><p>平时分：学位课是有平时分的，根据老师要求好好做就行了，大部分情况可以参考学位课（非专业课）的情况。</p><p>​       想提高GPA，学的好，真的不如会考试。（可怜我用大一惨的不行的绩点才明白这个道理）但是千万别有：既然能想办法考的高，何必再学呢，的这种想法。鬼知道以后会不会用上呢，例如学妹找你教高数题你只应付了下考试当然教不了就错过了跟学妹接触的机会。</p><p><strong>2. </strong>学位课（非专业课）— 平时分+期末分or平时分+考试分**</p><p>这种课一般代表着毛概，近代史，思修这种政治课，俗称水课。老师讲的没意思的话就很少有人听，学分占的还挺多，最后分低了还很难受。那么怎么去提高这种课的分数呢。</p><p>这种课是老师给的，可能根据你的作业，根据你的考勤，课堂表现等等，十分的虚无缥缈，也折射出些许的不公平。很多学霸吐槽水课分都低，那我只能说你没有去争取，下面是争取的手段。</p><p>（1）      当学委。 学委跟老师的关系肯定很熟的，天天送作业，给的平时分和期末分低了也不好看。</p><p>（2）      每堂课都抢占前排+回答问题。 也是混脸熟的手段，但是容易吃力不讨好。</p><p>（3）      老师明确了的就尽量刷满平时分。 有的老师在第一节课就会声明平时分的给分方法，例如回答够几次问题等等，积极的满足要求就ok。</p><p>（4）      展现得分决心。以我为例，大一上的心理健康期末卷子，我写了慢慢当当一大篇（写的多）。大一下的思修期末卷子，写的多再加个设计的很好的封皮（作业独特出彩）。大一的游泳课在期末考试允许游两次但是大家都游一次的情况下，请求老师再游一次给了老师好的印象，最后分数也挺高。大二下的思政课老师布置拍校园图片的任务，我做了个ppt+视频，让老师记住了直接承诺给了优。其他的课程也都这样所以我的水课分数都挺高的。</p><p>所以大家看出来了吧，这种非考试的分数，只要你让老师看到你的用心，让老师看到你想拿高分的决心，你就可以拿高分。</p><p> <strong>3. </strong>专业选修课</p><p>这种课，一般都是在大二大三才会涉及，大部分情况下因为相对落后的培养计划就是为了补满学分才选的，最后的评分和考试也会很水（当然也分老师）。老师讲的好就听，讲的不好的话不如干点更有意义的事情，看书刷题（da you xi）等等，但是切记要尊重老师。</p><p>分数技巧也参考<strong>学位课（非专业课）的攻略，大同小异。</strong></p><p> <strong>4. </strong>非专业选修课</p><p>这种课其实挺少，例如人文选修课等，也参考<strong>学位课（非专业课）</strong></p><p><strong>5. </strong>实践课程</p><p>很多专业有实践课程，例如计算机相关就是去实验室做个小项目。大部分的给分是根据报告和答辩。报告尽量写多，写好，排版一定要好。关键是答辩，让老师记住你你的分数就不会太低。依然以我为例，我每次答辩，都是在答辩前尽可能完善，并加要求之外的功能（加界面加创新点）。在答辩的时候准备好：我的亮点or创新点，直观的呈现给老师；实现中的难点，卖惨嘛。换位思考一下，如果你是答辩的老师，你怎么从这么多学生里挑出做的好的，当时是表述清晰层次分明有亮点的。与水课的绩点类似，表达出来你得高分得决心就ok了，依靠这些，我得实践课分数基本没有太低得，大多都在专业前几。所以，要表现出来自己。</p><h2><span id="12-专业技能">1.2 专业技能</span></h2><p>只学了课程，是远远不够的。想要工作的时候被大厂看重，想要考研保研的时候脱颖而出，就需要提高自己课外的专业技能。这种玄学的东西，就自己查自己专业的吧。有学弟希望我谈谈假期的时间利用。其实我觉得不放假的时候利用好时间就非常可以了。当然假期的时候放松休息咸鱼，或者是旅行，或者是充充电学学习，或者是找个实习，只要充实，都是很ok的。</p><p>啥都不懂的话，先大致百度个一二三，然后找个同专业学长问问就ok了。</p><h2><span id="13-转专业">1.3   转专业</span></h2><p>很多新生入学就奔着转专业去，确实，东大的自动化很强，信息时代计算机也很火。高考后阴差阳错录取的专业现在有了更换的机会。其实东大的转专业政策算是很开明了，很感谢东大的政策。</p><p>转专业是个需要很慎重的事情，千万要考虑好自己以后想要的是什么，并且尽可能长远的考虑。这里有我17年写的帖子，很多信息已经失去了时效性，大家以准确的转专业政策为准。政策文件！政策文件！政策文件！通过政策文件你才可以得到最权威的消息，这比学长学姐那里听到的要准的多。</p><p><a href="http://tieba.baidu.com/p/5233701747?fid=44908" target="_blank" rel="noopener">http://tieba.baidu.com/p/5233701747?fid=44908</a></p><h2><span id="14-奖学金相关">1.4   奖学金相关</span></h2><p>入学时发的学生手册上有奖学金评定相关政策。我来大概说一下政策没说到的部分。</p><p>奖学金的评定是一年一评，在学年结束，下学年刚开始的时候评定。</p><p>​       奖学金大致有：国家奖学金8000，国家励志奖学金（限贫困生）5000，各种命名奖学金（大部分在4000及以下），校级一二三等奖学金分别是2k-1k-600。每个人可以评个一等或者二等或者三等，然后再加个国奖/国家励志/命名。实力强的话一万块到手。</p><p>评定的标准是智育分数和德育分数加权（每个学院的权重和组成不太一样，找学院政策）。智育分就是你的这学期的学位课的平均分。德育分就比较复杂了，包括很多项，这个可以在政策上看到。政策上看不到的是怎么去加这些分。一般是宿舍卫生会影响，志愿时长会加分，比赛会加分。专利论文等等等等。可以说德育分的影响比智育分要大。所以想拿奖学金，要注意提高德育分，根据不同学院的政策，相应的去做对应的事情即可。</p><h2><span id="15-学习生涯规划读研or就业">1.5   学习生涯规划（读研or就业）</span></h2><p>我是一个目的性很强的人，入学就决定要转专业，大二就想着保研。现在来看，这样的提早规划是非常有必要的一件事情。</p><p>一般东大本科的出路有：直接就业，保研，考研，出国，创业等。各种出路要准备的方面是截然不同的，大家的选择也会相同，关于自己以后要干什么要早点想好早做准备。</p><p>就业的话就相对不太需要care绩点，但需要尽量提高你的专业技能，尽量在大四之前找个实习，之后找个正式的工作。不太熟悉，就不多说了</p><p>保研需要前三年的成绩，或者是拿到国家级的奖项，现在的保研政策也有了改动，关于保研我写了挺多，单独放在了另外一篇：<a href="https://zhuanlan.zhihu.com/p/42530877。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42530877。</a></p><p>考研的大多是没有拿到保研资格，但是仍然打算深造一下，一般是初试+复试，大三下大四上就要准备考研，对于新生，能保研尽量保。考研相对费力，风险也较大，很辛苦，但也很让人佩服。</p><p>出国，不太懂，不说了。</p><p>有的同学不满意自己的专业，自暴自弃，沉迷游戏，这样是不对的。大学的出路有很多的，除了本专业本行业的就业，完全可以跨专业工作，跨专业保研or考研，甚至去转销售或者去创业（虽然我不太认可），都比自暴自弃的强。</p><p>如果不满意自己的专业，尽量大一好好学习，大一结束转专业，转专业的经历是不会被歧视的。转不成专业，可以考虑跨专业工作or考研or保研，例如很多机械的学长毕业去华为，例如每年都有跨专业读研的，早点准备其他专业的专业技能。想咸鱼翻身，什么时候都不晚。</p><h2><span id> </span></h2><h1><span id="2-新生建议">2 新生建议</span></h1><h2><span id="21-社团or学生组织">2.1 社团or学生组织</span></h2><p>相对于学习，其实新生们面临的第一个问题就是社团的问题了，估计在高中被老师忽悠大学的美好生活的时候肯定有丰富多彩的社团这项。除此之外，还有大量的关于加不加入学生会的争论。</p><p>社团和学生组织是不一样的。社团大多是兴趣或者学习社团，大家根据兴趣凑到一块，没有什么任务，只有举办的活动，不太功利性。而学生组织是学校的一些组织，例如校级学生会，院级学生会，科协，社联，志协，团委四大中心等等。这些学生组织等级分明，上到主任下到部员，他们大多是学校下派的老师招生完成一些学校指派的任务，办一些相对官方的活动。</p><p>对于社团，感兴趣加就是了，但是很多社团活跃度都不高，一年也没几次活动，活动多的也就全明星、轮滑社那么几个。所以不要抱有太大期望。</p><p>那么到底要不要加入学生组织呢。对于家里有矿的，或者是以后想创业的想转销售的，毕业想留校当导员之类的，肯定是需要学生工作经历的，那么肯定是要加的。对于家里没矿的，以后想从事本专业工作的，我的建议是：在不影响自己学习的前提下，尽可能的丰富自己的大学生活。</p><p>有的学生组织，上下级分明，支使你各种干活，熬夜做ppt，熬夜ps，熬夜赶稿子，这种就没有必要了，除了增长一些可以俗称的ppt技巧并没有什么作用，只会影响学习，得不偿失。围绕着导员工作的我个人认为也没有太大必要，靠导员倾斜资源不如提高自己的能力去自己争取。</p><p>有的学生组织，气氛融洽，经常举办活动，交流较多，工作不多，有利于身心健康，这种就非常适合，例如我大一待的能力拓展中心就很不错。其他的，我听过的志协一些部门也很不错，其他的就不了解了。</p><p>家里没矿的，还是学习比较重要。</p><h2><span id="22-电脑购买建议">2.2 电脑购买建议</span></h2><p> 新生们都面临着买电脑的问题，那么买什么样子的电脑好呢，听我一句劝，一定要搞明白自己的需求。</p><p>有专业要求的，比如需要显卡渲染图形的，或者是经常打单机游戏的，要堆配置，买厚重的游戏本，毕竟刚需。</p><p>没有专业要求的，一定要轻薄！轻薄！轻薄！直到现在我还背着我的又厚又重的笔记本跑来跑去，哎，有点后悔。LOL这种游戏一般轻薄的都hold的住，之前不怎么打游戏的买了电脑也不会怎么打游戏的，要好好学习所以更不能买游戏本，所以要买轻薄的！</p><p>怎么选呢，大部分都是小白or懒癌，那么推荐个公众号：笔吧评测室，历史消息里有各价位的游戏本or万金油or超级本的推荐，挑个顺眼的，遇到双十一就买了吧，省心省事。</p><p>​       </p><h2><span id="23-驾照相关">2.3 驾照相关</span></h2><p>​       在学校学车，看起来很美好，实则很苦逼。说的有空练车就行，但是临到考试前，就得天天请假去练了。然后考试只会在周一到周五，课多的还得请假，很麻烦。可能还会比家里贵，还很慢，所以跟家里比比再选择。</p><p>​       一般留校时间长的可以考虑在沈阳考，我在浑南报的八棵树的情况可以简单说说，大概三千几百的报名费，科二模拟费200，科三300。科二练车在3公里外的练车场，骑自行车15分钟，335也是10分钟。报名的不同教练待遇和态度也不太一样，我报名的那个教练还不错有需要的可以私戳我。</p><p>​       时间上，科一科四基本就是花个报名考试时间，科二需要练个两三周，练的差不多随时可以报名考试，科三练车需要4+4+2个小时，目前挺难预约，上次考试结束之后三个月大概才可以约上。所以时间还是挺长的。</p><p>​       如果课多的话，还是不建议在沈阳考驾照。</p><h2><span id="24-买书建议">2.4 买书建议</span></h2><p>​       买书啊，，，估计新生们用不上了。</p><p>​       买书分三种，从学校定，从学长定，自己买。</p><p>​       不缺钱就从学校定，不太麻烦，书质量也好</p><p>​       从学长那定，有缺书被坑的风险，书的质量也凑活，价格应该比学校便宜一点。</p><p>​       对于家里没矿不怕麻烦的，建议重要的参考书淘宝或者南湖小北门买新书，不重要的学完就扔的书直接找老乡学长要或者买学长的二手或者淘宝影印版淘宝二手都行。</p><h1><span id="3-大学技巧分享">3 大学技巧分享</span></h1><h2><span id="31-重要信息获取建议微信公众号网站群">3.1 重要信息获取建议（微信公众号，网站，群）</span></h2><p>​       大学期间我获得的最重要的能力就是获取信息的能力。信息不对称极大的造成了人和人的差距，这也是我写这篇的初衷。考研的时候你获得更多的考试信息，你就能做对更多的题，工作的时候你获取到更好的岗位信息并为之住呢比，毕业待遇就会比其他人好。所以获取信息是相当的重要。</p><p>​       那么怎么去提高获取信息的能力呢？摸索+请教。</p><p>​       怎么摸索呢？</p><p>官方政策要牢牢把握，比如你想获取奖学金，你就应该去找政策文件，了解怎么提高自己的德育分，比别人知道的多就可能比别人准备的多，所以就能占到更好的先机。至少自己学院官方和学校官网和教务处官网要摸索一遍吧，你会收获很多的。</p><p>还有呢？各种信息渠道：微信公众号，各种论坛—贴吧，知乎，简书，保研or考研论坛等等等等。简直太多了，比如公众号能快速的告诉我校车信息澡堂超市信息的更新等等。比如这篇文章你可以在贴吧精品贴看到等等。</p><p>​       怎么请教呢？</p><p>​       学长学姐。真的，我加了很多的学长学姐，不论是期末考试的攻略，还是转专业还是保研，很大程度上都多亏了他们的帮助，再次感谢他们。</p><p>​       </p><h2><span id="32-挣钱贷款">3.2 挣钱，贷款</span></h2><p>​       关于挣钱，我大一下的时候做了半年家教，也做过学校的勤工助学岗位，算是有点经验。</p><p>​       学校有个家教中心，非常ok，一般填好一点的申请表就会被家长看中去给人孩子做家教，小学初中高中不等。当时我教过两个，都是60元一小时，不多但是也不少。相对的我也付出了很多时间，一定程度上影响了学习，但是我并不后悔，毕竟丰富了经历很有意思，那段时间很有钱过的生活很舒服。</p><p>​       关于勤工助学岗位是只针对贫困生的，贫困生的政策在通知书里的小本本里可以看到，家里条件较为困难的可以填表+盖章到学校申请，会有助学金or可以申请励志奖学金。勤工助学岗位可以从官网进入勤工助学网里申请，不定时会有岗位空出来，一般一月10天，每天4小时，基本就是看门，摆车这种，不太花时间，在门岗那里学习所以不耽误干别的，对于家里条件较差的可以去申请。如果长时间没有反应，建议直接去找对应楼的楼长，很好说话的都。</p><p>对于贷款，千万不要校园贷！千万不要校园贷！千万不要校园贷！可以通过贫困生的身份进行校园地贷款（来校之后办理），也可以进行生源地贷款（来校之前办理），一般是8000一年，毕业前没有利息，毕业后n年还清就ok，像我已经通过贷款周转，通过奖学金和软件的奖励基金已经cover掉学费了。所以不要因为学费的事情担心。</p><h2><span id="33-怎么找女朋友">3.3 怎么找女朋友</span></h2><p>​       写到这写了七千字了，已经没啥力气了。说说你们最感兴趣的。</p><p>​       大学找女朋友，无非三种途径。当然女生找男朋友也一个道理，只不过东大尴尬的男女比例导致女生不太需要担心这个问题，尤其是南湖的女生。</p><ol><li>同班级or同专业or同学院</li></ol><p>几个班一起上课or学院的讲座开会or学院举办的活动等等，见的多了就心动了，之后就可以一块上课，一块学习，一块讨论问题，既是情侣又是同行，不会有太多隔阂，缺点就是分手了会每天尴尬好多次</p><ol><li>同学生组织or同社团or同参加了某个活动</li></ol><p>这个时候就知道我为什么建议加入学生组织了吧，大家一起举办活动，一起写策划书，一起讨论，之后就约跑步约自习，balabala。</p><p>当然傻傻的你不要加入院学生会这样的学生组织，尤其是机械学院的学弟们，不听劝等到后悔的时候不要哭。</p><p>参加志愿活动，尤其是浑南的去那个什么学校的志愿活动，满满的都是妹子，志协的凑不够志愿人数的工作者们，不用谢我，学弟们，也不用谢。</p><ol><li>老乡</li></ol><p>除了上述的途径也就老乡群了，在群里水群水着水着就自来熟了，然后借口借本书啥的面基一下，就见到真人了，爱情的小火花就怕擦怕擦辣。</p><p>这种情侣最让人羡慕了，可以一起回家，可以一起返校，作为前老乡群群主我们群里也有几对老乡情侣，哎，还有一对双双保研的，真的优秀。</p><p>良心学长只能帮你们到这了，加油吧！~</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 来自良心学长的东北大学新生攻略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>北理北航中科院计算所保研经验</title>
      <link href="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/"/>
      <url>/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-前言">1. 前言</span></h2><p>放在开头：</p><p>非常感谢传授经验耐心解惑无私的学长学姐们，站在你们的肩膀上我才能看的更远</p><p>非常感谢学院悉心教导指点迷津的老师们，是你们为我增添力量为我指引方向</p><p>非常感谢辅导员和教学办老师的指导和帮助，给你们添了不少麻烦</p><p>非常感谢默默支持我的家人，我因你们高兴而满足。 </p><p>写于2018年7月30日：</p><p>笔者大一转专业到软件工程专业、大二决心保研后排名由 50+名提升到 14 名，大三经过选拔进入软件学院大数据实验班。目前已拿到了北京理工大学计算机学院、北京航空航天大学计算机学院、中科院计算技术研究所的夏令营offer，南京大学得到了入营资格但是选择了放弃参加，没有意外和变故的话最终会选择中科院计算所读研。</p><p>因为这段时间夏令营刚刚结束，而九月末才会填报系统尘埃落定，又担心届时将夏令营的细节忘掉，故先记录下来，等最终确定再将之公开作为下一届学弟学妹的参考。</p><h2><span id="2-准备的文件">2. 准备的文件</span></h2><p>我所做的准备如下：</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image002.jpg" alt="clip_image002"></p><p><strong>（1）</strong>     <strong>材料准备</strong></p><p>包括：材料证明（成绩单、证书、排名证明、四六级成绩单、身份证等等，建议用扫描全能王app拍下来保存，打印也方面）、简历和个人陈述（简历很重要！）、夏令营要准备的材料（分各个学校，按照文件要求打包整理）、</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image004.jpg" alt="clip_image004"></p><p><strong>（2）</strong>     <strong>参考材料</strong></p><p>各个学校各大学院的招生简章和政策、同专业学长的排名和去向、各个学校各大学院历年招收的名单（用于参考）等等</p><p><strong>（3）</strong>     <strong>联系老师</strong></p><p>各个相中的老师资料、跟每个老师要发的不同的邮件内容（在框架上加一些具体的信息à针对不同的老师去发邮件）</p><p><strong>（4）</strong>     <strong>知识填坑</strong></p><p>参加夏令营前要复习的基础知识的整理、所做项目的整理等等</p><p><strong>（5）</strong>     <strong>网站收藏</strong></p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image006.jpg" alt="clip_image006"></p><p>很直白了，自己看吧。</p><p>其实，最最重要的还是你的简历。怎么更好的展示自己，怎么让你被导师吸引，怎么体现你的闪光点，怎么让你脱颖而出。基本缕清简历，就缕清了自己的竞争力。我的简历经过大大小小几十次改版，也慢慢变得完善。在此再次感谢帮助过我的学长学姐和老师。</p><h2><span id="3-夏令营申请和准备">3. 夏令营申请和准备</span></h2><p>​       很多同学都比较迷，自己的水平应该申请怎样的学校。</p><p>​       对我们人数较多的大专业而言，同专业学长学姐的去向非常有参考价值。我从学院官网和学长那里得来了他们的排名和去向，写几行代码merge了一下，得到如下：</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image008.jpg" alt="clip_image008"></p><p>​       这些学长学姐很多都有很有含金量的竞赛、科研等等，因此也并不是排名越高去向越好，但是基本锁定了自己想去并且能去的高校区间。</p><p>​       我的意中高校（京津）我大致作了个优先级排名，在实验室或者导师同等水平下按照如下次序选择，如果导师较强就视情况升档：</p><p>​       北大信科、北大叉院&gt;清华软院&gt;中科院计算所、自动化所=&gt;北航计算机&gt;北大软微&gt;人大信息&gt;软件所&gt;北理&gt;天大计算机&gt;中科院信工所&gt;南开计算机</p><p>刚开始我想着不管想不想去，能报的都报一下，到时候如果过了感觉很有成就感。但是思考再三，秉着不影响排名靠后的同学报名真正想去的学校，之前想报但是并不想去的学校就都没报。</p><p>​       除了南大报名最早流程简单就随手报了，其他的都是自己真正想去的学校。下图是我当时的报名、可见很惨。基本上是当时看不上的都没报，结果看的上的都过不了。</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image010.jpg" alt="clip_image010"></p><p>​       因为我很想去北京读研，基本全报的北京的高校，想着人大计算机不是很强，学姐说也不是很难，结果就想拿来保个底。结果人大是最先出来的，第一个就把我刷掉了，顿时怀疑人生，之后各大夏令营一个一个的把我刷掉，清北就算了，天大都没让入营也是很惨……</p><p>​       可以说，在各方的大肆宣传下，全民开始转计算机，尤其是人工智能大热，导致顶尖高校竞争十分激烈，数学、自动化、计算机、软件、电子通信等等等等专业都涌向计算机。这里举几个例子：自动化所4000+人（老师说的）报名入营240人入营，南大3000+人（忘了哪看的了）报名也是入营几百个、北航去年报名800+今年1345……</p><p>​       </p><p>但是很多院校里的不同实验室水平参差不齐，有的人追求就业就不太care导师只care方向，有的人追求好导师不惜降低学校档次，其实不太强的学校里一些老师带的学生并不比一些强校老师带出的学生差。只能说，人各有志，选择适合自己的就好。</p><h2><span id="4-关于联系老师">4. 关于联系老师</span></h2><p>​       有人说联系老师没啥用，也有人说联系老师很重要。不管有用没用，联系了就不吃亏。</p><p>​       我从四月末五月初就开始给我中意的老师发邮件，现在看来确实太早了，很多官回也有很多老师告诉我等开始招生再商量不迟，当然大多数还是没有回信</p><p>​       </p><p>分析一下联系老师的以下几个时间点</p><ol><li><p>夏令营报名还未开始。适用人群：大神。适用高校：顶尖的高校。作用：看看有没有老师对你感兴趣，进行提前考核预定牛导（晚了大牛就没名额了）。当然一般人是没用的（例如我/哭）。</p></li><li><p>夏令营报名前后：适用人群：所有人。适用高校：有把握入营的高校/非常没把握入营的高校。作用：前者是提前预定老师，后者是碰运气看看有没有老师帮你入营。</p></li><li><p>夏令营确定入营：使用人群：入营的人（/摊手）。适用高校：入营的学校（/摊手）。作用：提前预定老师，晚了就没名额了。</p></li></ol><p>忠告：</p><p>很多人疑问保底的学校要不要去联系老师，我的亲身经历告诉你：不要。因为如果不是运气太差或者太自满的话，每个人基本都能找到高于自己保底学校水准的高校。因此自己把握很大，但是有更大把握去更好的高校的，就不要再联系老师了，否则到时候还要拒绝徒增烦恼。</p><p>​       另外，如果有一些把握能去更想去的学校，就不要跟老师作出保证，即使你只有这一个offer，毕竟变故太多了，谁都无法预料。我们这届就有同学鸽了老师，虽然在7月份就跟老师说明了，但是老师也很生气找到了我们辅导员这里。虽然历年这样的情况很多，但是只能说，吸取教训与老师多真诚一些沟通吧。</p><h2><span id="5-北理工夏令营">5. 北理工夏令营</span></h2><p>​       北理是我参加的第一个夏令营。出通知第一天就报了名，于是就显示出了报名早的好处，在基友还没报的时候就收到了入营通知邮件。</p><p>入营难度：</p><p>北理很看重985-211，即使是985的跨专业且排名20+%的学生都能入营。报名人数未知，入营人数270+。</p><p>考核：</p><p>水到不能水的机试、水到不能水的面试。</p><p>​       机试分两场，根据基友描述的第二场和我做的第一场，感觉难度相当。机试环境是devc++，版本落后，无补全，无stl。写完告诉老师，老师让你输入样例。一道题三个样例，分值不同，满分一百。</p><p>​       我参加了第一场，要求两个小时，两道题。半个多小时做完，小半个小时在那测无数个样例，最后发现样例真的很简单/笑哭。第一题是输入一行字符串：“1,2,3,4,1,2,3”。输出第一个重复的数字，如果没有输出-1。三个样例分别是：0输出-1；1,2,1,2输出2；1,2,3,4输出-1。第二题是输入一串字母x例如abcd。设定一个函数f(x)，可以得到f(x)=bcda，则f(f(x))=cdab。字母x的长度为l=4，则最多可以嵌套四次对这个字符串处理。区分大小写。输出int类型的结果，表示在每次f(x)的过程中得到的结果与原字符串相同的次数。三个样例如下：aa   2；aAa  1；byebyebye   3；</p><p>​       </p><p>​       面试就是中文自我介绍，问问项目和个人情况。我是带着简历进去的，所以基本就是针对简历的某个项目讲了讲，一个人七八分钟的样子。很水。</p><p>北理一行除了拿到了第一个offer外，最大的收获是有机会和北理的老师聊了聊。</p><p>去夏令营之前联系的一个视觉的老师，他可能是人收满了，把我简历转给了他实验室另一个比较厉害的老师，这个老师联系了我。我到北理的当天，专程赶到学校问了我的情况，算是简单的面试，大体比较满意。但是他说要我确定去他那里读研，我就如实告诉他我可能会去试试别的高校。那时候对自己不够自信，但是可能老师觉得我基本不会来他这里了，基本就不太理睬我了。</p><p>​       然后就跟着基友去他联系的那个老师那里，晚上九点开始聊了将近俩小时，老师也没对我们进行考核，主要交流了研究生的学习生活和科研状况，可以看得出这个老师是真的一心搞科研的……这个老师真的很棒，除了让我们开拓了眼界，还让我们增长了自信，给我们提了很多建议，鼓励我们去更好的地方试试。当然她实力也很强，指导的学生也有顶会发表，对学生也很好，研究方向也很热。礼欣老师，很感谢她，强推一波。</p><p>综合评价一下北理： </p><p>首先：住宿很破，硕博公寓四人间上床下桌，很旧，两床之间特别窄，两人无法并行的那种。但是校园还不错，校园很大但是路很窄（毕竟北京寸土寸金），葱一样的树郁郁葱葱，有的道路自行车限行，治安很好，校园很干净，食堂很多。</p><p>​       2018年计算机学院和软件学院合并，计算机学院的牛导特别多，随便抓一个就一堆顶会。还有幸听了一个九篇顶会的博士和五篇顶会的硕士做了学习分享，真的很厉害。唯一的美中不足是生源不是很好跟清北华五没法比，但是学生最终这么优秀真的是很不容易了。建议不太在意学校牌子的，去北理找个牛导真的是很不错的选择。</p><h2><span id="6-北航夏令营">6. 北航夏令营</span></h2><p>​       当时一串夏令营都没过真的很绝望，到最后只剩北航没出。北航去年（2017）报名800+，入营300+（应该没记错，官网有具体数据可以仔细看看）。而今年（2018）竟然1345人报名，牛校很多，瞬间感觉自己凉了。</p><p>​       但是幸运的是，入了营，成为500+中的一个。那个时候很忐忑，北航过不了北京的高校也没有多少可以选择并且有希望进入了。于是比较积极的准备。</p><p>​       北航是机试+面试，满分300，历年是公布分数，前120优营，120-180候补营员。今年是直接公布了不到两百的优营。</p><p>​       机试环境看运气，有的只有vc6.0。有的有codeblocks和dev。传闻可以用stl。提交可以选c和c++。但是听闻c++不稳，容易出事故，所以就纯c写的，也间接导致机试很惨。机试两道题，每道题都跟阅读理解似的，一千字可能没有，五六百还是有的。题目不难，很基础但很麻烦，细节很多，样例给了一个很复杂的，基本这个样例过了后面的就没啥问题了。两个小时两道长长的题。</p><p>​       第一个小时做出来第一道，但是仔细比对和样例有一丢丢不一样。又调了一小时bug，结果还是没调出来。第二道题就把题目的样例输出了一下，一点没做，绝望的不行，晚上回沈阳的车票都买好了，最后退票还花了20块钱，真是心疼。感觉样例应该过了简单的几个，也可能老师也参考了代码。反正幸运的通过了。</p><p>​       北航面试分很多组，问什么看运气，我很幸运的排在第二天，第一天四处打听，很多组的流程：1-政治题（瞎扯）2-念一段英文，然后翻译中文/英文自我介绍 3-专业知识考察，操作系统计算机组成原理什么的 4-项目 5-瞎扯点家常。</p><p>​       我面试在第二天，第一天的时候听到消息，赶紧复习了一天的操作系统，最后也没问尴尬。我的流程，进去之后抽小纸条政治题，跟老师说一下题号，是XXX讲话上青年爱国、立志什么什么的，我说了题号之后一个老师问我：你知道这个XXX讲话是啥吗，我说：应该是习近平在某次会议上的谈话。然后老师都开始笑我，我才意识到说了一句废话。老师追问我具体是啥，我说不知道，老师很无奈的说那你谈谈感想吧，我就瞎扯balabala。然后开始抽纸条念英文，磕磕绊绊，念完后翻译，日常卡壳，自己都不知道翻的啥，但是看老师表情还不算特别烂吧。</p><p>​       然后，特别幸运的，这几个老师只问了我项目！！！最拿手的最能扯的项目。最后看起来扯得不错。老师开始唠嗑，甚至问我简历上自我评价那“做事认真极为可靠“为什么要加极为……翻白眼，最后老师问计时的老师时间，老师说还差几分钟，然后几个老师就比较和蔼的说没事，然后就随便抛了个话题，我接住之后继续扯。能看出来老师们比较满意，美滋滋。</p><p>​       然后下午就去天安门和博物馆玩去了，排队安检的过程接到了面试我的一个老师，说他的研究方向balabala，让我有兴趣明天去聊聊，我就知道面试稳了，看看机试会不会拉分了。当天晚上就得到名单了，赫然在列。</p><p>​       北航通过夏令营的同学会发一张导师意向表，最后一天找意向导师签字，九月底填系统就是这个老师收你了。虽然机试面试还算顺利，但是找老师让我吃尽了苦头。我相中了做视觉的一个老师，夏令营前两个月就先后给他发过三封邮件，结果依然没啥用，没要我，关键是中午才出结果告诉我已经确定学生了。这个时候我就慌了，因为觉得这个老师比较稳，就没联系别的老师。然后下午就一直不停给各个老师发邮件，直接上办公室找，结果耽误了黄金时间的我自然找不到太好的老师了，之前联系过的老师也都满了。五点就要交表了，结果我还没有老师，在四点的时候一个老师给我发邮件让我去另一个老师的办公室，让那个老师面试我，面试了我一下觉得我很满意，但是那个老师给我的印象特别不好，由于并不是这个老师招我，所以我也没太care，最终签了那个老师。当然最最后发现这个老师以前是和cxw一个组的……然后经过后面几天的思考还是觉得不太合适，夏令营没过几天就早早的拒绝了这个offer。</p><p>综合评价一下北航：</p><p>​       首先：校园环境相对北理还是不错的。住宿听说还不错，屋小了点但是还算新。校园和北理差不多，但是显得宽敞一点，综合来看，环境比北理强一档。</p><p>​       北航软件工程学科评估A+,计算机A。但是不代表软院比计院强，其实，这里的软件工程学科大部分是由计算机学院的软件工程学科评上去的，有个软工重点实验室比较强，所以要搞清楚概念。</p><p>​       北航有一点比较坑，就是虽然收的人多，但是很多都要外派到青岛、合肥、杭州等研究院学习一年，大概比例20-30%，很多人的意向好老师最后只剩了外地的名额。例如我联系的一个青千最后只剩了青岛的名额……当然果断的拒绝了。</p><p>​       还有一点，我接触到的几个老师感觉就像是雇佣你去做项目的，根本不是培养学生为目的，不让实习，补助还很少（学院700+导师400），感觉没有那种人文关怀，很不舒服。当然也可能是偶然，我接触的少。北航接的纵向项目很多都跟军工有关，很多人具体研究方向取决于他接到的项目。但是也有不少很好的实验室，偏学术，乐于培养学生，这样的就很不错。个人觉得实验室整体招生或者培养的比某个老师带一个组的氛围要舒服的多。</p><h2><span id="7-计算所">7. 计算所</span></h2><p>​       计算所真的是意外之喜。在只拿到北理offer，被拒了一堆（包括计算所也没有入营），只剩北航的时候，感觉很慌张，很忐忑，北航过不了就凉凉了。这个时候计算所的招生老师说没过的可以填问卷，如果实验室同意了可以去面试。</p><p>​       于是我就每个实验室（网数、智信、前瞻）联系了一个老师，看看哪个老师回我了就报哪个老师。结果到问卷截至日期快到了才收到智信实验室联系的老师的官方回复。于是问卷就填了智信，也没抱希望。</p><p>​       结果过了几天，网数负责招生的老师通知我去面试，如下图：</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image012.jpg" alt="clip_image012"></p><p>​       邮件把整个过程都记录了下来：我联系的老师A并不了解招生事项à老师A询问老师B是否可以招夏令营没过的学生 à 老师B告诉老师A可以，告知负责招生的老师C就行à 老师A告知老师Cà老师C通知我去面试。</p><p>​       于是我依靠我的简历、依靠我详细的说明、依靠我诚恳的态度，通过我一直以为没用的提前联系老师的过程，为我挣到一个面试机会！<strong>感谢xx老师</strong>（为了不给老师的邮箱添加负担还是不写名字了）</p><p>​       </p><p>​       当然挣到了面试机会，还是得想办法通过。这个时候我强大的人品又一次登陆了！计算所入营的学生被要求只能填写一个实验室去面试，虽然也有很多学生去多个实验室面试，但是最热门的三个实验室网数、智信、前瞻基本只能报一个。于是我这个没入营的和其他入营的同学的差别就是那张几百块的饭卡和价值几百块的住宿了。</p><p>网数的面试学生听学长说去年是94进28，竞争还是蛮激烈的，不抱希望的我看到面试的只有65个人是相当激动的。网数是机试+面试，没有笔试所以不用太过于准备专业知识。       机试两个小时五道题。在自己准备的笔记本上编写，然后u盘拷贝代码（没错看起来很low）。这样看来应该是不会自动过样例，顶多手动编译一下。于是我依然秉着大学三年来充分展示自己的机会，多行注释+各种思路说明+整洁的代码，没做出的题也写了大概思路和可能的解法，不放弃任何展示自己独特的机会。题目各种类型都有，简单的hash、堆栈实现一个较为负责的符号计算器、dp问题/背包、DFS/BFS。我的水平勉强做出了不到三道题，其中一道差一个函数，return true并且加注释给凑过去了。后来才了解到机试不计权重，做出来两道算通过，蓝桥杯C/C++省二选手还是没啥问题的/滑稽。</p><p>面试的话，一个人10-25分钟，由于姓名排序，我机试完的那天下午就开始等待面试。刚开始每个同学都是15-25分钟，过了饭点后就快了，基本每个人10分钟，轮到我的时候不出意外的长了点15分钟（/笑）。</p><p>流程很简单：自我介绍+很深入的问项目。这我最拿手了，可是一群研究文本的老师追着我问视觉的项目我也是很迷。问完项目，有个老师问我数学怎么样，数学哪科好，虽然我数理不差，但是没复习，大一学的早忘光了。但是也不能说都不行吧，我就说最近看了点线代，很多都是大一学的，可能记得不太清楚。然后就问我线代的内容，我就是挨个章节扯：行列式、矩阵、向量、二次型balabala。二次项没想起来，想起来个二，然后旁边几个老师帮我打圆场23333。然后问了我线代的问题，都不是简单的概念而是偏理解，例如矩阵秩的意义是什么、伴随矩阵为啥有俩，正交矩阵有什么含义……于是脸不红心不跳的瞎扯一丢丢，扯一些绝对正确的大概念，然后再说细节不会，虽然没全答出来，但是看来那个老师也不是特不满意。到最后就开始问要不要读博balabala这种问题，我就balabala说，说因为目前做的都是偏应用的，不知道做学术能不能做下去，准备先读个硕看看，如果可以、会考虑读博（标准答案，但是也是实话）。然后表达了一下河北人对北京的偏爱，balabala多么想去计算所等等。感觉老师探你的底问你会不会来的时候，其实已经八成稳了。</p><p>忘了说了，面试时是一屋子老师面试，大概十三四个，感兴趣的就问你问题，最终能不能通过，老师们投票，通过半数就ok。美滋滋。</p><p>然后就胸有成竹但还是很忐忑的等了两天，等来了中科院计算技术研究所网络数据科学与技术实验室学硕的通知电话，我还专门录了音，每天睡前听一遍233333.</p><p>计算所综合评价：</p><p>​       计算所在一幢写字楼里，学生们在一间超大超开阔的实验室有各个隔开的工位，比北航看起来舒服多了。计算所的楼里环境特别好，很喜欢。虽然不是校园式的而是研究所性质的，但是整体比较舒服。</p><p>​       计算所的生源比北航还要好一档，入营名单就一水的985，而且是排名靠前的那些。优营名单绝大部分都是985，其他的都是没入营的211甚至双非，相信霸面的他们不会差到哪里。关键是有五个北大七个清华，想想就刺激。</p><p>​       计算所最热的实验室当属智信、网数、前瞻。其他的实验室不是差，而是方向不够热或者招生人数少，所以就没有那么火。研一在雁栖湖校区（北京郊区）听院士们上课，研二研三回所做项目/写论文。当然计算所补助也远远多于高校的平均水平，研一1500，研二将近3000（根据实验室有所差距）。学费硕士交8000返8000，博士交10000返13000。要不是直博风险太大，真的想读博去2333.</p><h2><span id="8-南大计算机">8. 南大计算机</span></h2><p>南大计算机也在计算所结果出来之后补录了我，南大今年竞争也异常激烈，据说3000+报名，入营几百个，基本上是暴力筛，除了学校背景、专业排名、英语水平、论文、比赛之外都不用填别的信息了。所以卡在5%外真的难受（虽然还是补录了），如果六级没过也很惨，下面这个哥们就是一个特别想去南大但是六级没过没入营，替他祈祷，希望九推能进。</p><p> <img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image014.jpg" alt="clip_image014"></p><p>​       最后因为在北京奔波了几天真的很累，离的又远，又秉着把机会留给真正想去的同学，就放弃了公费旅游住星级酒店的机会，哎可惜了，因为我觉得如果我去了还能再拿一个offer233333.</p><p>综合评价一下南大：</p><p>​       校园很棒（尤其是新校区，虽然在郊区），有幸去过老校区，感觉很不错。华五计算机，生源肯定不差，一水儿的985。Lamda是南大必须要提到的，周boss传闻很强，因为当时也没想去南方，所以也没报lamda，错过了收到lamda贼棒拒信（写的可好了，一点生不出被拒绝的难过）的机会。但是这种水平的高校，一般牌子都够了，要看导师和实验室的水平，这么大的学院导师水平参差不齐，一定要鉴别好，不要入坑！</p><p>​       </p><h2><span id="9-写在最后">9. 写在最后</span></h2><p>​       其实总结起来，不论是跟老师的交流、面试机会的争取、面试的通过全靠我的简历，展示了我的闪光点：动手能力强、学习能力强有上进心、项目多。其中那些项目真的真的很好的帮助了我。所以不论是竞赛、科研、实习经历等等，把亮点展示出来，是真的非常非常重要。当然，从大一开始到现在，再到将来，我也始终秉持着这一点，去争取更多的机会！</p><p>截止到现在，保研告一段落了，九月末才会填报系统尘埃落定，先记录下来，等最终确定再将之公开得瑟一波，希望一切顺利。</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 北理北航中科院计算所保研经验 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐喻识别survey2019</title>
      <link href="/2019/04/30/nlp-jin-jie/yin-yu-shi-bie-survey2019/"/>
      <url>/2019/04/30/nlp-jin-jie/yin-yu-shi-bie-survey2019/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-隐喻metaphor">1. 隐喻（Metaphor）</span></h2><p>隐喻不仅是人类语言中常用的一种修辞手法，更代表着人类对世界的认识和理解方式。</p><p>目前查到的最贴切的一个解释：隐喻就是人类利用在某一个领域的理解和经历，去解释另一个领域的认知行为。</p><p>隐喻识别属于隐喻计算的一个子任务。隐喻计算可以分为隐喻识别、隐喻理解和隐喻生成三个子任务。</p><p>隐喻理解的子任务举例：CCL2018 中文隐喻的情感分析(隐喻句子的情感分类)</p><h2><span id="2-隐喻识别的意义">2. 隐喻识别的意义</span></h2><p>自然语言处理研究领域中,隐喻是一个不可回避的问题.</p><p>一些研究表明,<strong>中文和英文的语料中存在着大量隐喻表达</strong>.      因此,机器翻译、文本处理、信息检索等若局限于获取字面意义,而无法理解隐喻等“言外之意”是远远不够的.</p><p>若能在这些领域中引入隐喻计算系统,则有希望进一步提高机器翻译质量,改善搜索引擎反馈,提高人工智能系统的水平等等.</p><h2><span id="3-隐喻的分类根据语言表达">3. 隐喻的分类（根据语言表达）</span></h2><h3><span id="31-名词性隐喻">3.1 名词性隐喻</span></h3><p>用一个名词去形容另一个名词</p><p>由指称词“是”“像”“如”等连接： 知识就是力量。    人生如梦<br>用标点符号“,” “—”等连接：  书 ,人类进步的阶梯</p><h3><span id="32-动词性隐喻">3.2 动词性隐喻</span></h3><p>动作（谓语）的实施者（主语）或者承受者（宾语）不符合人类对该动作的正常的搭配认知。</p><p>例如： &lt;狗，吃，良心&gt;</p><h3><span id="33-形容词性隐喻">3.3 形容词性隐喻</span></h3><p>表现为某个名词拥有了本身所不具备的（形容词）属性，即某个实体本身的属性与形容词所描述的没有关联。</p><p>如：愤怒的大海</p><h3><span id="34-副词性隐喻">3.4 副词性隐喻</span></h3><p>动作因为某种属性被描述成另一种具备类似属性的动作</p><p>如：他们的头点的跟小鸡啄米似的。（用小鸡啄米形容点头快）</p><h3><span id="35-常规隐喻-死喻">3.5 常规隐喻 - 死喻</span></h3><p>用太多成为常规搭配</p><p>如：尖锐的声音</p><h2><span id="4-隐喻识别的方法">4. 隐喻识别的方法</span></h2><h3><span id="对句子的分类-or-序列标注任务识别隐喻词汇">对句子的分类 or 序列标注任务识别隐喻词汇</span></h3><h3><span id="基于知识库-or-语义规则的方法">基于知识库 or 语义规则的方法</span></h3><p>常见的知识库有：Hownet（董振东、董强的知网）、Wordnet（英语词汇数据库）、同义词词林、vernet等</p><p>知网：以汉语和英语的词语所代表的概念为描述对象，以揭示概念与概念之间以及概念所具有的属性之间的关系为基本内容的常识知识库。 （<a href="http://www.keenage.com/zhiwang/c_zhiwang.html）" target="_blank" rel="noopener">http://www.keenage.com/zhiwang/c_zhiwang.html）</a><br>描述了上下位、同义、反义等各种关系。</p><p>偏语言学的角度，进行依存分析抽取文本语义关系，建立语言模型</p><ul><li>与人工定义的语义规则是否匹配 —&gt; 识别动词的常规表达、转喻、隐喻和异常表达;</li><li>名词间的上下位关系来识别名词性隐喻;       (语言学概念—概括性较强的单词叫做特定性较强的单词的上位词)</li><li>词语在语料库中的共现频率来识别动词和形容词性隐喻;</li><li>使用诗歌作为语料进行隐喻识别</li><li>…….</li></ul><p>最高精度为75%  —  2018 ACL Word Embedding and WordNet Based Metaphor Identification and</p><h3><span id="基于统计的方法">基于统计的方法</span></h3><p>主要思想是利用大规模的语料库进行统计和分析，再对具体的隐喻问题进行分类和识别。</p><p>隐喻识别被看作一个分类问题</p><p>使用概念的不同特征训练隐喻识别的分类器 (概念的属性特征和语义特征)</p><p>例如：</p><ul><li>使用属性特征为概念进行语义建模进而训练可解释的隐喻识别分类器</li><li>对数据库中的概念进行属性特征抽取,使用回归方法进行隐喻识别</li><li>……</li></ul><h3><span id="神经网络的方法">神经网络的方法</span></h3><p>2018 EMNLP Neural Metaphor Detection in Context  使用Bi-LSTM 识别隐喻词汇和句子分类</p><p>最高精度 2018 EMNLP </p><p>句子分类：MOH-X 79.1%<br>序列标注：MOH-X 75.6%</p><h2><span id="5-隐喻数据">5 隐喻数据</span></h2><ol><li>CCL2018  5000条人工标注隐喻数据用于评测(已过期，暂时未找到)</li><li>隐喻语料资源/知识库(英文)(<a href="http://www.jos.org.cn/html/2015/1/4669.htm#top" target="_blank" rel="noopener">http://www.jos.org.cn/html/2015/1/4669.htm#top</a>)</li></ol><p>Master Metaphor List    基于源域和目标域映射的在线知识库</p><p>Sense-Frame       词例化隐喻知识库</p><p>MetaBank、ATT-Meta、Metalude、Hamburg Metaphor Database、ItalWordNet等隐喻知识库</p><ol><li>目前最大的隐喻数据集（链接已过期）</li></ol><p>Introducing the LCC Metaphor Datasets<br><a href="https://pdfs.semanticscholar.org/edf9/b7367660d7f8255633717bf050f000a7c8fd.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/edf9/b7367660d7f8255633717bf050f000a7c8fd.pdf</a></p><ol><li>Mohammad等人(2016)开发的数据集。(<a href="http://saifmohammad.com/WebPages/metaphor.html" target="_blank" rel="noopener">http://saifmohammad.com/WebPages/metaphor.html</a>)</li></ol><p>该数据集包含1230个普通文本和409个隐喻句子</p><p>2018 ACL Word Embedding and WordNet Based Metaphor Identification and Interpretation:  75%的精度</p><ol><li>TroFi and MOH\MOH-X（已获得）<a href="https://github.com/htfhxx/metaphor-in-context" target="_blank" rel="noopener">https://github.com/htfhxx/metaphor-in-context</a></li></ol><p>句子的分类</p><p>TroFi (Birke and Sarkar, 2006) </p><p>MOH (Mohammad et al., 2016)</p><p>MOH-X refers to a subset of MOH dataset used in previous work (Shutova et al., 2016)</p><ol><li><p>VUA<br>隐喻词的检测（序列标注）<br>VUA (Steen et al., 2010)<br>verb + sentence + verb_index + sentence_lable</p></li><li><p>动词的隐喻分类数据集<br><a href="https://github.com/EducationalTestingService/metaphor" target="_blank" rel="noopener">https://github.com/EducationalTestingService/metaphor</a></p></li></ol><p>2016 ACL Semantic classifications for detection of verb metaphors</p><ol><li>中文情感词汇本体库</li></ol><p>词语+情感信息（情感分类，强度，极性等）</p><p>2018 ACL Construction of a Chinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions  大工-林鸿飞 </p><ol><li>其他</li></ol><p>隐喻的新颖分数</p><p>2018 AAAI Exploring the Terrain of Metaphor Novelty: A Regression-Based Approach for Automatically Scoring Metaphors</p><p>对隐喻的新颖性进行打分</p><p>对隐喻新颖性起作用的语言和概念特征</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 隐喻识别survey2019 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Neural Network Methods in NLP》笔记</title>
      <link href="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/"/>
      <url>/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>在刚接触到机器学习的时候，看了周志华老师的《机器学习》，其中的概念和举例对入门机器学习很有帮助，李航老师的《统计学习方法》更是用严谨的定义和详细的公式对机器学习进行了补充。而在理论基础支撑的实践层面，使用scikit-learn库可以对<strong>结构化数据</strong>进行分析、预测等各种各样的操作。</p><p>结构化数据比较直观，可是<strong>文本数据</strong>该怎么表示和处理呢？ 《Neural Network Methods in Natural Language Processing》这本书给了答案，这本书是一本非常适合入门自然语言处理的书籍，足够薄，最关键的是有中文版。。。是哈工大车万翔老师团队翻译的，在一定程度上做到了权威。不过有的地方翻译的意思有出入，对照英文版就可以了 </p><p>本书可分为四部分。第一部分介绍神经网络的基础。第二部分介绍自然语言数据的处理。第三部分介绍特殊的深度学习结构。第四部分是一些非核心主题，我觉得相比之下，一些会议的Tutorials更值得阅读。</p><p>以下是我在阅读过程中的笔记，或者说是我摘录的重点，可以快速重复阅读的那种。</p><h2><span id="1-引言">1. 引言</span></h2><ul><li><strong>自然语言三个特性</strong>。离散性（ 语言是符号化和离散的）、组合性(字母形成单词，单词形成短语和句子) 和 稀疏性（以上性质的组合导致了数据稀疏性）</li><li><p>有两种主要的神经网络结构，即前馈网络（ feed-forward network）和循环／递归网络(recurrent/ recursive network），它们可以以各种方式组合。</p></li><li><p>循环神经网络 （RNN）是适于序列数据的特殊模型，<strong>循环网络很少被当作独立组件应用</strong>，其能力在于可被当作可训练的组件“喂”给其他网络组件，然后串联地训练它们。例如，循环网络的输出可以“喂”给前馈网络，用于预测一些值 。循环网络被用作一个输入转换器，其被训练用于产生富含信息的表示，前馈网络将在其上进行运算。</p></li><li>前馈网络，也叫 多层感知器（ Multi Layer Perceptron , MLP），其输入大小固定，对于变化的输入长度，我们可以忽略元素的顺序。</li><li><strong>循环网络打破自然语言处理中存在几十年的马尔可夫假设</strong>，设计能依赖整个句子的模型，并在需要的情况下考虑词的顺序，同时不太受由于数据稀疏造成的统计估计问题之苦。</li><li><strong>语言模型 (language modeling）指的是预测序列中下一个单词的概率</strong>（等价于预测一个序列的概率），是许多自然语言处理应用的核心 。</li><li><strong>大部分情况下，全连接前馈神经网络 CMLP 能被用来替代线性学习器</strong> 。 这包括二分类或多分类问题，以及更复杂的结构化预测问题 。网络的非线性以及易于整合预训练词嵌入的能力经常带来更高的分类精度</li></ul><h2><span id="2-学习基础与线性模型">2. 学习基础与线性模型</span></h2><h3><span id="21-有监督学习和参数化函数">2.1 有监督学习和参数化函数</span></h3><ul><li>假设类 ：搜索所有可能的程序〈或函数〉是非常困难（和极其不明确）的问题，通常把<strong>函数限制在特定的函数簇内</strong>，比如所有的具有d_in个输人、 d_out个输出的线性函数所组成的函数空间，或者所有包含 d个变量的决策树空间 。 <strong>这样的函数簇被称作假设类</strong>.</li><li>归纳偏置： 通过把搜索限制在假设类之中，我们向学习器中引入了归纳偏置（ inductive bias ）， 当学习器去预测其未遇到过的输入的结果时，会做一些假设。而学习算法中归纳偏置则是这些假设的集合，同时也使得搜索结果的过程更加高效。补充：归纳偏置可以<strong>看作学习算法自身在一个庞大的假设空间中对假设进行选择的启发式或者“价值观”</strong>。即天涯何处无芳草，却为什么偏偏选择你！！！</li><li>假设类也确定了学习器可以表示什么、不可以表示什么 。学习器的目标是确定参数的值，因此，搜索函数空间的问题被缩减成了搜索参数空间的问题 。</li><li><strong>线性函数的假设类相当有限，有很多它无法表示的函数</strong>（的确，它只限于线性关系〉。具有隐层的前馈神经网络同样是参数化函数，但构成了一个非常强大的假设类一一它们是通用近似器，可以表示任意的波莱尔可测量函数。</li><li>线性模型虽然表示能力有限，但它也有几个我们想要的性质 ： 训练简单高效，通常会得到凸优化目标，训练得到的模型也有点可解释性，它们在实践中往往非常有效 。它们也是更强大 的非线性前馈网络的基本模块。</li></ul><h3><span id="22-训练集-测试集和验证集">2.2 训练集、测试集和验证集</span></h3><ul><li>关于数据集划分：<strong>留一法——留一交叉验证</strong>：训练 k个函数  ，每次取出一个不同的样例 x作为测试集，剩余样本作为训练集，评价得到的函数 f预测的能力，最后选中一个表现最好的函数。非常浪费计算时间，只会在已标注样例数量 走特别小（小于 100 左右〉的时候使用 。在《统计学习方法》中，被叫做<strong>k折交叉验证</strong></li><li><strong>留存集</strong>：划分训练集为两个子集，可以按80% /  20%划分，在较大的子集（训练集）上训练模型，在较小的子集（留存集， held-out set ）上测试模型的准确率。《统计学习方法》中，被叫做<strong>简单交叉验证。</strong></li><li>进行划分时要注意一些事情一一通常来说在划分数据前打乱样例，保证一个训练集和留存集之间平衡的样例分布是更好的。有时随机的划分不是一个好的选择：与时间相关的训练集如果进行随机划分就会有影响。</li><li><strong>三路划分：</strong>留存集（测试集）进行误差估计不能代表新实例的效果，不知道最终的分类器的设置在总体上是好的，还是仅仅在留存集中的特定样例上是好的 。已被接受的方法是使用一种把数据划分为训练集、验证集（也叫作开发集）和测试集的三路划分方法 。 这会给你两个留存集： 一个是验证集，一个是测试集 。 所有的实验、调参 、误差分析和模型选择都应该在验证集上进行 。 然后，最终模型在测试集上的一次简单运行将会给出它在未见实例上的期望质量的一个好的估计 。</li></ul><h3><span id="23-线性模型">2.3 线性模型</span></h3><ul><li>非线性可分的情况下（不能使用一条直线或一个线性超平面将数据点分开), 解决方法<strong>要么是转换到更高维的空间</strong>（加入更多的特征），要么是<strong>转换到更丰富的假设类</strong>，或者<strong>允许一些误分类</strong>存在。</li><li><strong>特征表示</strong>：在大多数情况下这些数据点不会以特征列表的形式直接提供给我们，而是作为真实世界中的对象 。例如，我们被给予一系列公寓来分类。然后，我们需要做有意识的决定，手动选择我们认为对于分类任务有用的可测量属性 。就特征集合做出决定之后，我们创造一个特征抽取（ feature extraction）函数，它把真实世界的对象（公寓）映射成一个可测量量度（价格和大小）的向量，该向量可以作为我们模型的输入。</li><li><strong>线性模型设计的一个核心部分是特征函数的设计</strong>（所谓的<strong>特征工程</strong> ）。深度学习的开创性之一是，它通过让模型设计者指定一个小的核心、基本或者自然的特征集，让可训练的神经网络结构将它们组合成更有意义的、更高层次的特征或者表示，从而大大地简化了特征工程的过程 。</li><li><strong>sign和sigmoid</strong>：将原本值域为负无穷到正无穷的结果，使用sign函数将结果映射到[-1,+1]可以得到二分类的结果，使用sigmoid 函数将结果映射到 [0,1]范围之内，即可计算决策的置信度或者分类器分配这个类别的概率。</li></ul><h3><span id="26-对数线性多分类">2.6 对数线性多分类</span></h3><ul><li><p>多分类问题：不同于二分类的求得能将其分开的超平面的一个模型，n分类需要构建n个模型，每个模型预测一种结果的可能，最终的结果取决于最高分的模型所属于的结果。</p></li><li><p><strong>BOW( Bag-of-words）</strong>表示包含文档中所有单词的不考虑次序的信息 。一个独热表示可以被认为是一个单一单词的词袋 。<strong>CBOW(Continuous Bag Of Words) 连续单词词袋</strong>，表示可以通过求单词表示向量和或者通过将一个单词词袋向量乘以一个每一行对应于一个稠密单词表示的矩阵(嵌入矩阵 embedd ingmatricy)来得到。     1=”A B C”     2=”A D”     BOW=[A B C D]  CBOW=[A B C A D ]  </p></li><li><p><strong>Sigmoid和Softmax这两个概念有什么区别和联系</strong></p><ul><li>sigmoid把一个值映射到0-1之间，常常使用于二分类，把线性预测转变为一个概率估计，从而得到一个<strong>对数线性模型</strong>。</li></ul><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083751895.png" alt="img"></p><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083760411.png" alt="img"></p><ul><li>softmax。多分类情况中是把分数向量通过一个softmax，它将一个K维的任意实数向量映射成另一个K维的实数向量，其中向量中的每个元素取值都介于(0,1)之间并且和为1。 </li></ul></li></ul><h3><span id="27-训练和最优化">2.7 训练和最优化</span></h3><ul><li>关于损失函数：在大多数情况下，<strong>推荐使用常见的损失函数而不是自行定义</strong>。</li><li>hinge （二分类）、hinge （ 多分类 ）、对数 （log ） 损失、二元交叉熵—逻辑斯蒂 (logistic）损失、分类交叉熵 （ categorical cross-entropy） 损失（* ?）、等级损失</li><li><strong>正则化方法</strong>：L1、L2、dropout</li><li>随机梯度下降和基于minibatch的随机梯度下降：一个样本和一小批样本的区别</li></ul><h2><span id="3-从线性模型到多层感知器">3. 从线性模型到多层感知器</span></h2><ul><li>线性模型的局限性（只限于线性关系）：异或(XOR)函数,没有一条直线能够分割这两个类别。</li></ul><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083791339.png" alt="img" style="zoom: 33%;"><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083795864.png" alt="img"></p><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083791339.png" alt="img" style="zoom: 33%;"><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083795864.png" alt="img"></p><ul><li><strong>非线性输入转换</strong>：¢(x1,x2)＝［x1 * x2, x1 + x2］函数 ¢ 将数据映射为适合线性分类的表示。该解决方案有一个明显的问题，我们需要人工 。定义函数¢，此过程需要依赖特定的数据集 ，并且需要大量人类的直觉。</li><li><strong>核方法</strong>：通过定义一些通用的映射来解决上述问题。多项式映射， ¢(x)=(x)^d。d=2时，¢(x1,x2)=(x1x1,x1x2,x2x1,x2x2).对所有的变量进行两两组合.</li><li><strong>可训练的映射函数</strong>：映射函数可以采用参数化的线性模型形式，接一个作用于每一个输出维度上的非线性激活函数 g :    y= ¢(x)(W+b)， ¢(x)= g(xW’+b’)。自行定义g(x)和训练W‘、b’得到一组参数，即可获得一个映射。</li></ul><h2><span id="4-前馈神经网络">4. 前馈神经网络</span></h2><ul><li>完全连接层或仿射层：每个神经元连接到下一层中的所有神经元</li><li>最简单的神经网络称作感知器，是一个简单的线性模型 ：NN(x)=xW+b.</li><li>由线性变换产生的向量称为层 。 最外层的线性变换产生输出层，其他线性变换产生隐层 。 非线性激活操作接在每个隐层后面。由线性变换产生的层通常被称为完全连接的或仿射的。</li><li><strong>MLPl 是指带有单一隐层的多层感知机</strong></li><li>线性变换的维度计算：对一个输入维数 d_in ， 输出维数 d_out 的全连接层，有 l(x) =xW+ b ，其中 x 、 W、 b 的维数分别为 1 × d_in 、 d_in × d_out 、1× d_out。当输入的向量为行向量</li></ul><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083817021.png" alt="img"></p><ul><li><strong>常见的非线性函数</strong>:根据经验来看， ReLU 和 tanh 单元表现良好，显著地超过了 sigmoid<ul><li>sigmoid：σ(x)= l / (1 + e^(-x) )  逻辑斯蒂函数，近来被认为不适合用在神经网络的内层</li><li>tanh：tanh(x)=(e^(2x)-1) / (e^(2x)+1)    值域 [-1,1]</li><li>hard tanh=    -1(x&lt;-1)    1(x&gt;1)    x(其他)</li><li>修正线性单元（ ReLU)   ：max(0, x)  在很多任务中都表现优异，尤其是使用dropout正则化的时候</li></ul></li><li>称为权重衰减的 L2 正则化在许多情况下能有效地实现良好的泛化性能。</li><li>dropout也是一种防止神经网络过度拟合训 练数据的有效技术，旨在防止网络学习依赖于特定权重。在对每个训练样本进行随机梯度训练时，它随机丢弃网络(或特定层)中的一半神经元 。</li><li>dropout细节：考虑具有两个隐层 (MLP2 )的多层感知器：<ul><li>从x到y的过程：NN(x)=y;    h^1=g^1(xW^1+b^1);   h^2=g^2(h^1 <em> W^2+b^2);  y=h^2 </em>W^3;</li><li>当在 MLP2 中使用dropout时，我们在每轮训练中<strong>随机地设置 h^1 和 h^2 的部分值为0,</strong> 通过将h^1乘一个掩码向量(元素值为0或1)，将乘得的结果代替原隐层h^1，传入下一层。</li></ul></li><li>相似和距离层：希望基于两向量计算一个标量值，反映出两向量间的相似性、兼容性或距离 <ul><li>点积：两向量的同index数值相乘求和。</li><li>欧式距离：两向量的同index数值的差的平方和。这是一个距离度量而不是相似度, 在这里, 小值表示相似，较大值表示不相似</li><li><strong>可训练形式</strong>：有时希望使用一个参数化函数，通过关注向量的特定维度，来进行训练以产生所需的相似度。常见的可训练相似度函数是<strong>双线性形式（ bilinear form)</strong> sim(u,v)=uWv</li></ul></li></ul><p><strong>五、神经网络训练</strong></p><ul><li>计算图：一个有向无环图(DAG)，其中结点对应于数学运算或者变量，边对应于结点间计算值的流 。计算(a <em> b + 1) </em> (a * b +2): </li></ul><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083655556.png" alt="img"></p><ul><li>a * b 的计算是共享的 。有一个限制条件，就是计算图是连通的。</li></ul><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083827624.png" alt="img" style="zoom: 50%;"></p><ul><li><strong>实践经验：</strong><ul><li>优化算法：SGD收敛速度慢、Adam有效</li><li>初始化：减少陷入局部最小的可能</li><li>随机重启：多次训练，每次都随机初始化，选取验证集最好的一个</li><li>梯度消失和梯度爆炸、饱和神经元与死神经元</li><li>训练样本的随机打乱</li><li>学习率的调整</li></ul></li></ul><h2><span id="6-文本特征构造">6. 文本特征构造</span></h2><h3><span id="61-nlp-分类问题中的拓扑结构">6.1 NLP 分类问题中的拓扑结构</span></h3><p>词、文本、成对文本、上下文中的词、词之间的关系</p><h3><span id="62-nlp-问题中的特征">6.2 NLP 问题中的特征</span></h3><p>特征通常表现为标量indicator和可数count两种形式：条件是否出现，事件出现频率</p><h4><span id="621直接观测特征">6.2.1直接观测特征</span></h4><ul><li><strong>单独词特征</strong><ul><li>词元（booking, booked, books）和词干（book）</li><li>词典资源（和其他词语连接起来或者提供额外的信息）例如wordNet</li><li>分布信息（哪些词和当前词是类似的）</li></ul></li><li><strong>文本特征：字符和词再文本中的数量和次序</strong><ul><li>词袋（bag-of-words）：表示包含文档中所有单词的不考虑次序的信息 。词袋中各个词在文本中出现的数量，作为特征。</li><li>权重：TF-IDF（TF <em> IDF，词频</em>逆向文件频率）：某词对于一个语料库中的其中一份文本的重要程度。某词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</li><li>n元组</li></ul></li><li><strong>上下文词特征</strong><ul><li>词在句子和文本中，其在句子的位置、围绕它的词和字符都可以作为特征。越近信息量越大</li><li>窗口：{ word-2=brown, word-l=fox, word+l=over, word+2=the ｝</li></ul></li></ul><h4><span id="622-可推断的语言学特征">6.2.2 可推断的语言学特征</span></h4><ul><li>自然语言中的句子除了是词语的线性排序外还是有结构的。这种结构遵循复杂的不易于直接观察到的规律，这些规律被归类为语法。文本的语言学特性并不能够从词在句子中或其顺序的外在表现直接观察到，它们能够以不同程度的准确率从文本句子中推断出来。</li><li>语言学标注：<ul><li>成分树（ constituency tree），或短语结构树。</li><li>依存树（ dependency tree），除了句子的中心词（通常是动词），每个句子中的词都被另一个词所引领，每个词都是另一个词的修饰。</li><li>词性标签、句法角色、篇章关系、回指等概念是基于语言学理论的</li></ul></li></ul><p>6.2.3 核心特征和组合特征   多用组合特征</p><p>6.2.4 n 元组特征    在给定的长度下由连续的词序列组成，比单独的词富含更多信息</p><p>6.2.5 分布特征</p><h2><span id="7-nlp特征的案例分析">7. NLP特征的案例分析</span></h2><ul><li>7.1 文本分类：语言识别：字母级文法词袋（字母级别的特征）</li><li>7.2 文本分类：主题分类：字母级别特征不合适，词为基本单位，例如二元文法词袋等</li><li>7.3 文本分类：作者归属：避开内容词，侧重于文体属性（可以表现出来的特征list: 功能词与代词词袋，词性词袋，词性的n元文法词袋 ）</li><li>7.4 上下文中的单词：词性标注：内部线索（单词、单词的字母前缀与后缀，是否大写、是否包含连字符、是否数字）+外部线索（上下文的单词的内部线索）</li><li>7.5 上下文中的单词：命名实体识别：是一个序列分割任务，但通常被建模为序列标注任务，类似于词性标注。特征类似于词性标注，内部线索+外部线索</li><li>7.6 上下文中单词的语言特征：介词词义消歧。特征：词本身（单词、词元、词性、前后缀、词簇or分布式表示）+信息量较大的上下文（固定窗口上下文可能不富含太多信息，可用启发式规则，举例：左边第一个动词右边第一个名词形成三元组特征、依存分析器-好用），也可以用外部资源（知识库wordNet）</li><li>7.7 上下文中单词的关系：弧分解分析（依存分析任务）：给定长度为n的句子，有n*n个词关系，为每个词关系分配分数，得到一个最大化总体分数的有效的树。可构建特征：头词和修饰词的字面形式、词性、距离、方向、两者之间的词or词性等等。</li></ul><h2><span id="8-从文本特征到输入">8. 从文本特征到输入</span></h2><p>8.1 编码分类特征：独热编码(one-hot)和稠密嵌入向量</p><p>稠密表示的主要益处是具有很强的泛化能力 (Page95)</p><p><strong>8.2 组合稠密向量（个人认为这章的特征很实用）</strong></p><p>每个特征对应一个稠密向量，需要用某种方式将不同的向量组合起来，主要有拼接、相加（或者平均）和同时使用拼接与相加 。</p><ul><li><strong>基于窗口的特征</strong></li></ul><p>​       位置i为中心词时，需要编码位置在 i-2、i-1、i+1、i+2 上的词。不关心窗口内词的位置：<strong>词向量求和</strong>a+b+c+d；关心：<strong>词向量拼接</strong>[a:b:c:d]；不关心顺序但是关心距离：<strong>加权求和</strong>0.5a+b+c+0.5d（距离越远权重越小）；不关心距离但是关心前后：<strong>拼接和相加组合</strong>，[a+b:c+d] </p><p>​       显式拼接（[x: y: z]W＋b)，<strong>仿射变换</strong> （ xU+ yV+zW+ b ）,如果后者参数矩阵互不相同，则两种方式是等价的。</p><ul><li><p><strong>可变特征数目：连续词袋CBOW</strong></p><p>用于使用固定维度的向量表示任意数量的特征。</p></li></ul><p>CBOW 和传统的不考虑顺序信息的词袋表示非常相似，通过相加或者平均的方式组合特征的嵌入向量。or 加权CBOW，为不同向量赋予权重，权重可以是TF-IDF</p><p><strong>8.3 独热和稠密向量间的关系</strong></p><p>​       在训练神经网络时，使用稀疏独热向量作为输入，意味着使网络的第一层从训练数据中学习特征的稠密嵌入向量。当遇到多层前馈神经网络时，稠密和稀疏输入之间的区别会比起初看起来的更小。</p><p>独热向量，通过与一个嵌入矩阵E的乘法操作，得到稠密向量。这个矩阵E的维度是V <em> d，d是每个词的表示维度，V是单词数表示有V行，每一行代表一个独热向量的对应的稠密向量。这样的<em>*嵌入层/查找层</em></em>，即可将独热向量映射为稠密向量。</p><p><strong>8.4 杂项</strong></p><ul><li><strong>距离与位置特征</strong></li></ul><p>​        一个句子中两个词的线性距离可能作为一个提供信息的特征。在传统 NLP 情景下，通常将距离分配到若干组（如 1, 2, 3, 4, 5-10, 10+）中，并且为每一组赋予一个 one hot向量。在神经网络框架中，输入并没有直接分配一个距离数值的入口，而是将传统分组的one hot向量映射到一个类似词嵌入的d(6)维向量，这些距离嵌入向量作为网络参数进行训练（个人理解，欢迎指正，Page104有例子）</p><ul><li><p><strong>一些处理数据的tips</strong></p><ul><li><strong>补齐</strong><br>当使用拼接方法时，可以用零向量进行填充，这对于某些问题来说可能只是次优解，也许知道左侧没有修饰成分是有益的。推荐的做法是添加一个特殊符号（<strong>补齐符号</strong>〉到嵌入词表中，并且在上述情况中使用相应的补齐向量。根据要处理的问题不同，在不同情况下可能会使用不同的补齐向量 （比如左侧修饰缺失向量与右侧修饰缺失向量不同）。 这样的补齐对预测的准确性是很重要的，并且非常常用 。很多论文没提，只有源码才会看到。</li><li><strong>未登录词</strong><br>没有对应词嵌入向量时，保留一个特殊符号 UNK 表示未知记号来应对这种情况。同样对于不同的同表可能会使用不同的未知符号，但不管怎样，都不建议共享补齐和未登录向量，因为它们代表两种不同的情况 </li><li>词签名<br>处理未登录词的另一种技术是将词的形式回退到词签名 。使用 UNK 符号表示未登录词是将<strong>所有未登陆词回退到同样的签名</strong>，但是根据要解决的问题的不同，可能使用更加细粒度的策略，比如用 一ing  符号代替以 ing 结尾的未登录词，等等。</li><li>词丢弃<br>在训练集中抽取特征时，用未登录符号随机替换单词。基于词频</li><li>使用词丢弃进行正则化</li></ul></li><li><p>特征组合<br>特征设计者不仅需要人工指定感兴趣的核心特征，同时要指定特征间的交互。（如除引人“词为 X”，“词性为 Y”的特征外，还有组合特征表示“词为 X 并且词性为 Y”）</p></li><li><p>向量共享<br>构建输入时，是否需要对特征用两个不同向量表示。经验问题。</p></li><li><p>维度  ——每个特征分配的维度。速度和准确度取得一个良好平衡。</p></li><li><p>网络的输出：k分类问题输出一个k维向量，每一维表示类别强度。</p></li></ul><p>8.5  例子：词性标注     词序列提取特征的细节</p><p>8.6  例子：弧分解分析    包含两个词和中间文本的弧的特征提取细节</p><h2><span id="9-语言模型">9. 语言模型</span></h2><p>9.1 语言模型任务</p><ul><li><p>给某单词在一个词序列之后出现的可能性分配概率；给句子分配概率(一系列的词预测)</p></li><li><p><strong>马尔可夫假设</strong>：规定未来的状态和现在给定的状态是无关的。形式上， 一个走阶马尔可夫假设假设序列中下一个词<strong>只依赖</strong>于其前k个词。</p></li></ul><p>9.2 语言模型评估：困惑度</p><ul><li>以应用为中心的度量方法通过在<strong>更高级别的任务中的性能</strong>来进行评价 。 例如， 当将翻译系统中的语言模型组件从 A 替换为 B 后,测量翻译质量提高的程度 。</li><li>一个更直观的评估语言模型的方法是对于未见的句子使用<strong>困惑度</strong></li></ul><p>是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好 。给定一个包含n个词的文本语料和一个基于词语历史的用于为词语分配概率的语言模型函数 LM ,LM 在这个语料的困惑度最低，语言模型越好。</p><p>9.3 语言模型的传统方法</p><ul><li>语言模型的传统方法假设 h 阶马尔可夫性质</li><li>预测从未在语料中观察到的序列，会出现0概率，造成极大困惑度，这是个很糟糕的情况。一种避免 0 概率事件的方法是使用平滑技术，确保为每个可能的情况都分配一个概率（可能非常小）。</li></ul><p>add-α： 它假设每个事件除了语料中观测的情况外，至少还发生 α 次</p><p>退避(back off)：如果没有观测到 h 元文法，那么就基于（k-1) 元文法计算一个估计值</p><ul><li>传统语言模型的限制：平滑技术错综复杂而且需要回退到低阶；将基于最大似然估计的语言模型应用于一个规模更大的 n 元文法是一个固有的问题；基于最大似然估计的语言模型缺乏对上下文的泛化。</li></ul><p>9.4 <strong>神经语言模型</strong></p><ul><li>非线性神经网络语言模型可以解决一些传统语言模型中的问题：它可以在增加上下文规模的同时参数仅呈线性增长，缓解了手工设计退避规则的需要，支持不同上下文的泛化性能 。</li><li>介绍：神经网络的输入是k元文法，输出是下一个词的概率分布。输入的词经过词嵌入后，被传给一个拥有一个或多个隐层的多层感知机（MLP）</li><li>训练、内存与计算效率、大规模输出空间（层次化softmax—构建huffman树、自归一化方法—没搞懂=_=）、期望特性、局限</li></ul><p>9.5 使用语言模型进行生成</p><p>9.6 副产品：词的表示</p><h2><span id="10-预训练的词表示">10. 预训练的词表示</span></h2><p>词嵌入一一将每个特征表示为低维空间中的向量。这些向量来自哪里？本章对常见方法进行了调研</p><h3><span id="101-随机初始化">10.1 随机初始化</span></h3><p>当有足够的有监督训练数据可用时－，可以将特征嵌入看作与其他模型参数相同 ： 将词嵌入向量初始化为随机值，使用网络训练过程将它们调整为“好”的 向量 。</p><h3><span id="102-有监督的特定任务的预训练">10.2 有监督的特定任务的预训练</span></h3><h3><span id="103-无监督的预训练">10.3 无监督的预训练</span></h3><p>训练词向量的技术本质上是有监督学习的技术，不是对我们关心的任务进行监督，而是<strong>从原始文本创建几乎无限数量的有监督训练实例</strong>，并希望我们创建的任务将匹配（或足够接近）我们关心的最终任务。</p><p>使用与训练的词嵌入，第一个选择是关于预处理，第二个选择是在这个任务上对预训练的向量进行微调 。</p><h3><span id="104-词嵌入算法这章建议看英文原版参考">10.4 词嵌入算法（这章建议看英文原版参考）</span></h3><p>神经网络社区倾向于从<strong>分布式表示 distributed representations</strong>的角度思考 。自然语言处理社区倾向于从<strong>分布语义 distributional semantics</strong>的角度思考</p><h4><span id="1041-分布假设和词表示-distributional-hypothesis-and-word-prepesentations">10.4.1 分布假设和词表示 Distributional hypothesis and word prepesentations</span></h4><p>explore the distributional approach to word representation <strong>词的分布表示</strong><br>关于语言和词义的分布式假设表明，在相同上下文中出现的词倾向于具有相似的含义。</p><ul><li><strong>词－上下文矩阵</strong> — i行j列，表示 i 个单词 <em> j 维度。每列表示词出现的语言学上下文。<br><em>*相似性度量</em></em>，词被表示为向量就可以通过计算向量相似度来计算词的相似度。 </li><li>余弦相似度(向量之间角度的余弦)、广义 Jaccard 相似度</li><li>词——上下文权重和<strong>点互信息(PMI)</strong><br>在求得词上下文矩阵时，其通常基于来自大规模语料库的计数。<br>PMI (w, c）通过计算词和上下文的联合概率（它们在一起的频率）与它们的边界概率（它们单独出现的频率）之间的比率的对数来测量词 w 和上下文 c 之间的关联 。减少过高的权重带来的不好的影响。（例如：the cat 和 a cat 可 以获得高于 cute cat 和 small cat 的分数，即使后者的信息量更多）</li></ul><ul><li><p>通过矩阵因式分解进行降维</p><p>将词表示为其出现的上下文的显式集合的潜在障碍是数据稀疏性。显式词向量具有非常高的维度。通过使用诸如奇异值分解(SVD)的降维技术来考虑数据的低阶表示，可以缓解这两个问题。</p></li></ul><h4><span id="1042-从神经语言模型到分布式表示-from-neural-language-models-to-distributed">10.4.2 从神经语言模型到分布式表示 From neural language models to distributed</span></h4><p>explore the distributed approaches.   <strong>词的分布式表示</strong></p><p>与词分布表示——基于计数的方法相比，神经网络社区主张使用分布式来表示词义。</p><ul><li>考虑语言建模网络：通过词嵌入矩阵和上下文矩阵确定给定上下文的不同词的概率，为上下文的k元组生成一个概率，上下文矩阵和词嵌入矩阵就是分布式的词表示：训练过程决定了词嵌入的理想值。</li></ul><ul><li>Collobert 和 Weston</li><li>word2vec：CBOW、Skip-gram(觉得这章讲的有点过于细了，有点乱)<br>word2Vec 模型训练带来了两个词嵌入矩阵 ，分别代表同和上下文矩阵 。 上下文嵌入会在训练后被丢弃，但保留了词嵌入。</li></ul><h4><span id="1043-词语联系">10.4.3 词语联系</span></h4><p>分布式“基于计数”的方法和分布式“神经”方法都是基于分布假设，尝试基于它们出现的上下文之间的相似性来捕获词之间的相似性。这两个方法之间的关系 比乍看之下更深。</p><h4><span id="1044-其他算法-nce-glove">10.4.4 其他算法    NCE   Glove</span></h4><p>10.5 上下文的选择</p><p>窗口方法(CBOW)、句子段落或文档(Skip-gram)、句法窗口（使用依存句法解析器自动解析文本，并将词的上下文视为解析树中邻近的词以及与之相关的句法关系 ）、多语种（使用多语种、基于翻译的上下文，例如与其对齐的外语单词）、基于字符级别和子词的表示（从一个词的组成字符中生成向量表示）</p><ol><li>6 处理多字单元和字变形</li></ol><ul><li>问题：1. 对多字分配单一变量、2. 形态学变形使有相同潜在概念的词不同。</li><li>可以通过对文本进行确定性预处理从而达到合理的程度，以至于更好地适应所想要的词语定义。可以生成多符号串词条列表，并用文本替换单个实体（即用 New_ York 替换 New York 的出现）</li><li>在变形的情况下，对语料预处理，包括对部分或全部词汇抽取词干，对词干嵌入而不是对其变形形式嵌入。</li></ul><ol><li>7 分布式方法的限制</li></ol><p>相似性的定义、害群之马（词的琐碎属性）、反义词（反而会出现在相似的情境）、语料库偏好、语境缺乏</p><h2><span id="11-使用词嵌入">11. 使用词嵌入</span></h2><p>本章讨论一部分词向量的用途。</p><ol><li>1 词向量的获取</li></ol><ul><li>word2Vec    可获取的单独的二进制文件、GenSim的python包、允许使用任意上下文信息的改进版二进制 Word2Vec、斯坦福发布的GloVe 模型的有效实现等等……</li></ul><ol><li>2 词的相似度（利用向量之间的相似度函数计算两个词的相似度：<strong>余弦相似度）</strong>11.3 词聚类、11 .4 寻找相似词、11. 5 同中选异、11. 6 短文档相似度、11. 7 词的类比（king - man - woman = queen）、11. 8 （对词向量的）改装和映射、11. 9 实用性和陷阱（当使用现成的词向量时，最好使用与源语料相同的切分词项的方法和文本规范化方法。仍然建议不要这样像黑盒方法一样盲目地下载和使用这些资源）</li></ol><h2><span id="12-案例分析一种用于句子意义推理的前馈结构">12. 案例分析：一种用于句子意义推理的前馈结构</span></h2><ol><li>1 自然语言推理与 SNLI 数据集</li></ol><ul><li>文本蕴含任务：两文本的关系有蕴含（前者推断出后者）、矛盾（不可同时为真）、中立</li><li>SNLI 是包括 57 万人类手写的句子对，每对都由人工标注为蕴含、矛盾和中立。</li></ul><ol><li>2 文本相似网络</li></ol><ul><li>计算句子对a、b的相似度<ul><li>依次计算a中某词与句子b中所有词的相似度，得到该词的对齐向量，softmax处理</li><li>每个对齐向量都有对应的权重</li><li>向量结果加和，传递到一个MLP分类器中，用于预测句子关系。</li><li>然后训练</li></ul></li><li>网络的目标是为了找到有助于蕴含的关键词。最终，决策网络从词对中聚合数据，并且据此提出决策 。 推断分为三个阶段：第一阶段根据相似度对齐找到较弱的局部证据，第二阶段查看带权重的多个词单元并加入方向性，第三阶段将所有局部证据整合成全局决策 。</li></ul><p>第三部分、特殊的结构（这个介绍不错，建议详细看）</p><h2><span id="13-n元语法探测器卷积神经网络">13. n元语法探测器：卷积神经网络</span></h2><ol><li>1 基础卷积＋池化</li></ol><ul><li>13.1.1 文本上的一维卷积<ul><li>卷积操作：滑动窗口向量与滤波器的内积，其后通常会使用一个非线性激活函数。</li><li><strong>宽卷积与窄卷积</strong>：窄卷积：窗口大小k，句子长度n，共有n-k+1个序列开始位置；             宽卷积：对句子每端填充k-1个填充词，n+k-1个序列开始位置(书写错了？)</li><li>两种卷积方式：实质上等价，不同的是卷积核矩阵维度为 k<em>d</em>l，和 l 个k*d维的卷积核</li><li>信道：多信道(图像数据)，通常对每个信道使用不同的滤波器集合，文本也可能多信道</li></ul></li><li>13.1.2 向量池化<ul><li>最大池化：获取整个窗口位置中最显著的信息</li><li>平均池化：一种理解是对句子中连续词袋 (CBOW) 而不是词进行卷积得到的表示。</li><li>k-max：每一维度保留前k个，同时保留文本中的顺序。保留了特征间的序关系，但对具            体位置不敏感</li><li>动态池化：卷积后得到的向量进行分组，每组分别池化，结果拼接。即分区域池化(例如关系抽取中两个词分割开的三块文本区域）</li></ul></li><li>13.1.3 变体<ul><li>平行的使用多个卷积层（窗口大小不等，捕获序列中不同长度的 n 元语法）</li></ul></li></ul><ol><li>2 其他选择：特征哈希</li></ol><p>用于文本的CNN计算开销大。直接使用 n元语法的词嵌入，然后池化得到连续词袋 n 元语法表示。使用特征哈希，n元语法通过哈希函数指派给N行词嵌入矩阵的一行，映射到[1，N]，使用对应行作为词嵌入。</p><ol><li>3 层次化卷积</li></ol><ul><li>一个窗口大小为k的卷积层学习识别输入中具有指示性的k元语法。这种方法可以扩展成层次化卷积层，卷积序列逐层相连。得到的向量捕获了句子中更有效的窗口(“感受野”)</li><li>步长、膨胀和池化</li></ul><p>膨胀的方法：步长为k-1；步长为1但是每层间使用局部池化缩短序列长度</p><ul><li>参数捆绑和跨层连接</li></ul><p>层次化卷积结构的一种变形是进行参数捆绑，对于所有参数层使用相同的参数集合 U, b 。</p><h2><span id="14-循环神经网络序列和栈建模">14. 循环神经网络：序列和栈建模</span></h2><p>处理序列：前馈网络(CBOW忽略序关系)、CNN(局限于局部)、RNN(<strong>序列输入翻译成定长向量</strong>)</p><ol><li>1 RNN 抽象描述：R、O决定网络类型</li></ol><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083912624.png" alt="img" style="zoom:67%;"></p><ol><li>2 RNN 的训练</li></ol><p>展开形式的RNN就是一个深度神经网络、不同部分在计算过程中  <strong>参数共享</strong></p><p>为了训练一个RNN，需要对输入序列构建一个展开的计算图，为展开的图添加损失节点，反向传播更新参数</p><ol><li>3 RNN 常见使用模式</li></ol><ul><li>接收器：监督信号仅置于最后的输出向量 y_n 上。观测最后一个状态，然后决策一个输出。</li><li>编码器：仅使用最后的输出向量y_n，把 y_n 与 输入相关的其他特征 进行后续任务。</li><li>传感器：对于每一个读取的输入产生一个输出（序列标注任务）</li></ul><ol><li>4 双向 RNN</li></ol><ul><li>动机：计算第i个单词，计算过程基于历史信息，然而后继的单词对于预测同样有效。</li><li>输出变动：第i个位置的输出基于<strong>前向后向两个输出向量的拼接</strong>，即同时考虑历史和未来的信息。</li><li>输出之后：拼接得到的输出向量，<strong>在接下来可以直接用来进行预测，或作为更复杂网络输入的一部分</strong></li><li>Bi-RNN 在一个输人向量对应一个输出向量的标注任务中非常有效 </li></ul><ol><li>5 堆叠 RNN</li></ol><ul><li><strong>Deep RNN:</strong>RNN逐层堆叠成网格，第n个RNN的输入是其下方RNN的输出。</li><li>在某些任务的观测经验中看， deep RNN 的确比浅层的表现更好 </li></ul><ol><li>6 用于表示梭的 RNN</li></ol><p>一些语言处理算法，需要对栈进行特征提取，RNN可以用来对整个栈的固定大小的向量编码。</p><ol><li>7 文献阅读的注意事项</li></ol><p>从学术论文的描述中推测出准确的模型形式往往是很有挑战性的 。</p><ul><li>RNN输入是独热向量还是词嵌入</li><li>输入序列有没有对开始字符和结束字符填充处理</li><li>一些论文假定RNN输出送入的softmax层是RNN自身的一部分</li><li>多层RNN中，状态向量可以是顶层输出，也可以是所有层状态向量的拼接。</li><li>在编码器－解码器框架下，作为解码器条件输入的编码器的输出可以有不同的诠释？</li></ul><h2><span id="15-实际的循环神经网络结构">15. 实际的循环神经网络结构</span></h2><ol><li>1 作为 RNN 的 CBOW</li></ol><p>选择加法函数：RNN内部是一个简单的加法：</p><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083954371.png" alt="img"></p><p>本质上是一个连续的词袋模型，输出是输入之和。</p><p><strong>15. 2 简单 RNN  ——</strong>  S-RNN</p><pre><code> 简单RNN是对前一个状态s_(i-1)和输入x_i 分别线性变换，结果相加，连同同一个偏置项，加一个激活函数tanh或ReLU。输出与隐藏状态相同。</code></pre><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083962745.png" alt="img"></p><p>S-RNN仅仅比 CBOW 稍微复杂了一点，主要不同之处在于非线性的激活函数 g 。然而，这个不同之处却至关重要，因为加入线性变换后跟随非线性变换的机制使得网络结构对于序列顺序敏感 。</p><p><strong>15. 3 门结构</strong></p><p><strong>LSTM要解决的问题：</strong>梯度消失，使得 S-RNN 很难有效地训练 。误差信号（梯度）在反向传播过程中到达序列的后面部分时迅速减少，以至于无法到达先前的输入信号的位置，这导致 S-RNN 难以捕捉到长距离依赖信息。</p><p><strong>LSTM的考虑：</strong>考虑S-RNN 其中的状态代表一个有限的记忆 。每一个单元都会读人一个输入x_i以及当前的记忆s_(i-1) ，对它们进行某种操作，并将结果写人记忆得到新的记忆状态 s_(i+1) 。从这种方式看来，S-RNN 的一个明显的问题在于记忆的获取是不受控制的。在每一步的计算过程中，整个记忆状态都被读入，并且整个记忆状态也被改写。</p><p><strong>改进：</strong>提出一种更加受控的记忆读写方式。由当前记忆状态和输入共同控制的门向量，来控制记忆状态的读写。</p><ul><li><strong>15. 3. 1 长短期记忆网络（下面有PPT做的图）</strong></li></ul><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083970476.png" alt="img"></p><ul><li><ul><li>时刻 j 的状态由两个向量组成：<strong>记忆组件c_ j 和 隐藏状态组件h_ j</strong></li><li><strong>输入门 i、遗忘门 f、输出门 o</strong>: 门的值由当前输入x_ j 和前一个状态 h_(j-1) 加一个sigmoid决定</li></ul></li></ul><ol><li><strong>遗忘门</strong>控制先前的记忆c_(j-1)，记忆c_ j 更新，</li><li><strong>产生更新候选项z：</strong> 由当前输入x_j 和前一个状态h_(j-1) 加一个tanh得到</li><li><strong>输入门</strong>控制更新候选项，记忆c_ j 更新</li><li><strong>记忆h_ j (y_j的输出) 生成</strong>，c_j 通过tanh非线性激活函数，并受输出门控制</li></ol><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083979477.png" alt="img"></p><ul><li><ul><li>实践的考虑：在训练 LSTM 网络的时候，强烈建议将遗忘门的偏置项设置为接近 1 的值 </li></ul></li><li><p><strong>15.3.2 门限循环单元</strong></p></li></ul><p>GRU是一种LSTM的替代方案。GRU 也基于门机制，但是总体上使用了更少的门并且网络不再额外给出记忆状态。</p><p><img src="/2019/04/08/nlp-jin-jie/neural-network-methods-in-nlp-bi-ji/clipboard-1579083866812.png" alt="img"></p><ul><li><ul><li>门 r 合并遗忘们和输入门，控制前一个状态s_(j-1)并计算一个新提出的 s_j~ </li><li>更新的状态s_j 由前一个状态和s_j~ 插值决定，比例通过门控z控制。</li></ul></li></ul><ol><li>4 其他变体</li></ol><ul><li>非门结构的改进</li><li>在可微门结构之外</li></ul><ol><li>5 应用到 RNN 的丢弃(dropout)机制</li></ol><p>在 RNN 中应用丢弃（ dropout）机制需要一定的技巧，因为在不同的时间点丢弃不同的维度会损害 RNN 有效携带信息的能力。</p><p>Gal 的变分 RNN 丢弃方法是目前应用于 RNN 中效果最好的丢弃机制。</p><h2><span id="16-通过循环网络建模">16. 通过循环网络建模</span></h2><ol><li>1 <strong>接收器</strong>（14. 3 有RNN 常见使用模式）</li></ol><p>接收器：监督信号仅置于最后的输出向量 y_n 上。观测最后一个状态，然后决策一个输出。读入一个序列，最后产生一个二值或者多分类的结果。</p><ul><li><ol><li><ol><li>1 情感分类器</li></ol></li></ol></li><li><ul><li>句子级情感分类任务中，给定一个句子，将这个句子分成两类中的一个 ： 积极的或消极的。这个任务通过使用 RNN 接收器能够很直接地进行建模： RNN 读人序列化后的词语 。RNN 的最终状态送人一个 MLP ，这个 MLP 的输出层是一个二输出的 softmax 层 。</li><li>文本级的情感分类。<strong>层次化结构的方法</strong>：每个句子通过门结构的RNN编码得到一个向量，然后将向量集合送入第二个门结构的RNN中，得到最终的向量，套上MLP和softmax用来预测。</li></ul></li><li><ol><li><ol><li>2 主谓一致语法检查</li></ol></li></ol></li><li><ul><li>英语句子必须要遵守“现在时态谓语动词的单复数必须与动词的主语一致”的规则</li><li>模型直接作为一种接收器被训练。这是一项困难的任务，只有非常间接的监督：监督信号不包括任何找到语法信息的线索 。</li><li>RNN 处理学习任务表现非常好，成功解决了测试集中的绝大多数句子</li></ul></li></ul><p>16.2 <strong>作为特征提取器的 RNN</strong></p><p>RNN 的一个最主要的应用场景就是作为灵活可训练的特征提取器，在处理序列问题时能够替代传统的特征提取通道 。 特别地， RNN 是基于窗口的特征提取器的良好替代者 。</p><ul><li><ol><li><ol><li>1 <strong>词性标注</strong></li></ol></li></ol></li><li><ul><li>n个词语的句子，使用特征提取函数转换为输入向量，输入到双层RNN中，产生的输出向量被送入MLP中，用于从可能的 h 个标签中预测这个词语的标签 。</li><li>通过训练程序，双向 RNN 将着重于学习序列中 预测词语标签 所需要的那些信息 ，并将其编码在输出向量中 </li><li>词语转换成输入向量：可以通过使用一个词嵌入矩阵，也可以通过字符级的 RNN 将词语转化为输入，使用字符级的卷积和池化神经网络来表示词语，</li></ul></li><li><ol><li><ol><li>2 <strong>RNN-CNN 文本分类</strong></li></ol></li></ol></li><li><ul><li>与标注任务相同的方式，将向量送入逐个读入单词的字符级RNN 或逐个读入单词的卷积一池化层中得到结果</li><li>或在字符上应用一个层次化卷积一池化网络，得到更短的向量序列，把结果的向量序列送入RNN 及分类层中</li></ul></li><li><ol><li><ol><li>3 <strong>弧分解依存句法分析</strong></li></ol></li></ol></li><li><ul><li>输入词语、词性标签和对应的特征向量，投入到RNN中，得到每个词语的编码。将这些编码进行两两拼接送入MLP中，得到一个核心词-修饰词 候选对的打分。</li><li>如果一个任务对词语顺序或者句子结构敏感，并且这个任务使用词语作为特征，那么这个任务中的<strong>词语</strong>就能够<strong>被使用词语自身训练的双向 LSTM输出的词向量代替</strong> 。</li></ul></li></ul><p><strong>十七、条件生成</strong></p><ol><li>1 RNN 生成器</li></ol><p>使用 RNN 转换器结构进行语言建模的一个特殊例子：<strong>序列生成</strong>。i 时刻的输出作为i+1时刻输入。</p><p>生成器的训练：将其当做<strong>转换器</strong>训练。长度为n的句子，生成具有n+1个输入和n+1个输出的RNN转换器，其中第一个输入是句首符，最后一个输出是句尾符。</p><p>局限性：不能更好的处理与黄金序列（观测序列）之间的偏差。在训练时，生成器使用实际词作为每一时刻的输入，而不是前一时刻的预测结果。</p><p><strong>17.2 条件生成（编码器－解码器）</strong></p><p>RNN 转换器的能力直到应用于条件生成的框架中才真正地显现出来 。</p><p>条件生成的框架中，下一个词项的生成依赖于己生成的词项以及一个额外的条件上下文 c，几乎任何我们能够获得并且认为有用的数据都可以被编码至上下文c</p><ul><li><ol><li><ol><li>1 序列到序列模型</li></ol></li></ol></li></ul><p>上下文 c 可以有很多种形式。一种方法是c本身就是一个序列，最常见的是一个文本片段。这种方式下产生了<strong>序列到序列（ sequence to sequence）条件生成框架</strong>，也被称为<strong>编码器一解码器框架</strong>。</p><p>给定一个长度n的源序列，生成一个长度m的输出序列。<strong>通过一个编码函数(通常是RNN)将源句子编码，再使用一个条件生成器（解码器）生成输出序列。</strong></p><p>编码器将源句子抽象表示为向量 c，作为解码器的 RNN 则根据当前已预测出的词以及编码后的源句子 c 来预测目标词序列。</p><p>用于编码器与解码器的 RNN 是<strong>联合训练</strong>的 。 监督信息只出现在解码器 RNN 端，但是梯度能够沿着网络连接反向传播至编码器 RNN 中</p><ul><li><p>17.2.2 应用</p></li><li><ul><li>机器翻译</li><li>邮件自动回复</li><li>形态屈折 — 输入基本词和期望的形态变化需求，输出该词的曲折形式</li><li>几乎所有任务都可以用编码-生成的方法建模，但也许存在更适合此任务的结构</li></ul></li><li><p>\17. 2.3 其他条件上下文</p></li></ul><p>条件上下文向量可以是基于单个词，或一种连续词袋 CCBOW）的编码，也可以由 一个卷积网络生成，或基于一些其他的复杂计算 。甚至不一定基于文本(对话任务用户的信息)</p><p>17.3 无监督的句子相似性</p><ul><li>为句子学习向量表示，将相似的句子编码为相近的向量。</li><li>大部分方法是基于序列到序列框架的：首先训练一个 RNN 编码器来产生上下文向量表示 c，该向量将用于一个 RNN 解码器来完成某个任务 。</li><li>作为解码器的 RNN 将被丢弃，而编码器则用来生成句子表示 c，最终得到的句子间相似度函数将严重依赖于训练解码器所完成的任务 。</li><li>这些任务可以是：自动编码、机器翻译、skip-thought、句法相似度</li></ul><p><strong>17.4 结合注意力机制的条件生成</strong></p><ul><li><p>编码器-解码器结构<strong>强制编码器中包含生成时所需要的全部信息</strong>，要求生成器中从该定长向量中提取出所有信息。  增加一个注意力机制，可以充分改进。</p></li><li><p>结合注意力机制的<strong>编码器-解码器结构预测过程</strong>：</p></li><li><ul><li>编码器对长度为n的输入序列编码，产生n个向量。</li><li>预测第j+1个词项时，注意力由前 j 个编码器输出的向量和前 j 个词项决定</li><li>解码器第j+1个状态，  由注意力、前 j 个词项、第 j 个状态决定</li><li>由解码器的最终状态，得到第j+1个词项</li></ul></li><li><p><strong>注意力函数</strong>是一个可训练、参数化的函数。</p></li><li><ul><li>注意力权重向量由解码器的第 j 个输出、编码器的每个向量，+MLP，决定</li><li>再使用 softmax 函数将权重归一化至一个概率分布</li><li>最终权重与编码器的输出，结合得到最终的注意力</li></ul></li><li><p>为何不省去编码器，把注意力机制直接作用于源序列？编码过程有重要的收益。</p></li><li><p>计算的复杂性：</p></li><li><ul><li>不含注意力的编码O(n)解码O(m)，整体时间复杂度是O(m+n)，</li><li>含注意力，编码O(n)，解码每一步需要计算注意力O(n)，整体O(m*n)</li></ul></li><li><p>可解释性：生成的注意力权重可以用来观察在产生当前输出时解码器认为源序列中哪些区域是相关的 。</p></li></ul><ol><li>5 自然语言处理中基于注意力机制的模型</li></ol><ul><li>机器翻译     子词单元  融合单语数据   语言学标注</li><li>形态屈折 — 输入基本词和期望的形态变化需求，输出该词的曲折形式</li><li>句法分析</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Neural Network Methods in NLP》笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>可控文本生成</title>
      <link href="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/"/>
      <url>/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/</url>
      
        <content type="html"><![CDATA[<h2><span id="可控文本生成survey">可控文本生成survey</span></h2><h2><span id="1-问题描述">1. 问题描述</span></h2><p><strong>文本生成</strong>是一个较宽泛的概念，广义上只要输出是自然语言文本的各类问题都属于这个范畴，包括从文本、数据、图像等生成文本。</p><p>目前我们侧重于text -&gt; text。data -&gt; text 虽然也较为常用，在一些端到端的生成方法中，基本也都是处理成text进行输入。</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616136959580.png" alt="1616136959580" style="zoom: 67%;"></p><p>但是只有text-&gt;text是不够的，在真实的应用场景中，文本都是受限或者可控的。</p><p>在实际应用中，除text-&gt;text之外，还有<strong>约束可控的文本生成</strong>，是在传统NLG的基础上，通过加入受限的条件，比如主题信息、情感倾向、句子长短等，从而让输出的文本带有受限的属性。   </p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137024841.png" alt="1616137024841"></p><p>例如最常见的，控制句子长短、控制要生成的文本中包含一些关键词等等。例如上图 1.在对话场景中生成不同感情的回复；2.通过主题写文章，本质上是通过关键字来控制生成文本。</p><p>本次分享侧重于这种 受限文本生成 或 约束可控文本生成。</p><h2><span id="2-任务及方法分类">2. 任务及方法分类</span></h2><p>接下来是我基于目前见过的文章和初步的调研，对目前可控文本生成任务的一个分类和总结，后续会随着进行扩充和改进。</p><h3><span id="21-无约束的文本生成"><strong>2.1 无约束的文本生成</strong></span></h3><p> text -&gt;text：本质上是不需要在模型和数据层面添加额外的约束</p><p>例如对联，摘要，以输入为开头的故事等等，这类很大程度上依赖数据 和 模型拟合的程度</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137286754.png" alt="1616137286754" style="zoom: 67%;"></p><h3><span id="22-主题限定">2.2 <strong>主题限定</strong></span></h3><p> topic(+text) -&gt;text：根据主题生成文本  </p><p>单纯的topic 到 text，本质上也是text-&gt;text。所以我觉得更应该看做在text -&gt;text的基础上加入topic的限制，这样能更好的进行拓展。</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137359406.png" alt="1616137359406"></p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137354498.png" alt="1616137354498"></p><h3><span id="23-形式or风格限定的文本生成">2.3 <strong>形式or风格限定的文本生成</strong></span></h3><p>  text + keywords（形式or风格限制）-&gt; text</p><h4><span id="231-文本形式限定">2.3.1 文本形式限定</span></h4><ol><li><strong>关键词限定： 必须包含原词</strong><br>下图，textrank 关键词提取。通过知识图谱或者类似知网的语言知识库，扩充词汇。<br><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137467028.png" alt="1616137467028"></li><li><strong>感叹句疑问句 or 是否第一人称 or 文本长度</strong><br><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137481590.png" alt="1616137481590"></li></ol><h4><span id="232-文本风格限定">2.3.2 文本风格限定</span></h4><ol><li><p><strong>generic or specific</strong><br>问题生成的任务，给定一段文本，生成这段文本中包含答案的问题。生成generic or specific的问题。</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137531732.png" alt="1616137531732"><br>适用于很多种产品，还是只针对这一个产品。<br>明确的直率的 or 含蓄的委婉的：<br><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137538778.png" alt="1616137538778"></p></li><li><p><strong>positive or negative</strong><br>早期在情感控制生成这方面的研究，只局限于有限的分类，比如上面的五类，甚至是positive negative两类，近年细粒度的逐渐增多。</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137568531.png" alt="1616137568531"></p><p>先标注了一个情感分类器，标注每句对话，构造平行语料进行训练。为整首诗和诗里的每一句都进行细粒度的情感标记，提出了一个VAE模型。<br><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137573113.png" alt="1616137573113"></p></li></ol><ol><li><strong>修辞（比喻、拟人）限制等</strong><br>少有的现代诗歌的文章,通过对某几句进行控制，生成带有比喻和拟人的句子。任务是输入上下文，主题、每一句的标签（可以标记也可以预测），生成下一句诗。训练了一个句子分类器，为训练集的句子打上标签，将比喻和拟人的标签应用于修改后的模型中进行训练</li></ol><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137764169.png" alt="1616137764169" style="zoom:67%;"></p><ol><li>文本简化（含义、长短、词句）<br>任务是生成不同简化等级的句子。引入了词级别的loss，根据词频敲定一个词级别的loss。<br><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137797585.png" alt="1616137797585" style="zoom:50%;"></li></ol><h2><span id="3-任务及方法分类">3. 任务及方法分类</span></h2><p><strong>模型方面</strong></p><ul><li>GAN</li><li>seq2seq</li><li>VAE</li><li>RL</li><li>预训练生成模型，例如GPT2、CTRL等</li></ul><p>文本控制方面</p><ul><li>通过前向特征输入并且构造平行语料来对最终的文本进行控制</li><li>通过后向的调节或后处理的方式<ul><li>后向调节中，loss的设计</li><li>后处理模式则直接对生成的结果进行beam_search采样和判断</li></ul></li></ul><p>分别举例：</p><p>2018 AAAI Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory</p><p>通过训练了一个情感分类器，将对话内容打标签，数据驱动进行训练。模型结构加入了内部记忆和外部记忆模块，有助于情感文本的</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137915393.png" alt="1616137915393"></p><p>2019 ACL Controllable Text Simplification with Lexical Constraint Loss</p><p>引入了词级别的loss，根据词频敲定一个词级别的loss，Grade体现在文档l上</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137945458.png" alt="1616137945458"></p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137974103.png" alt="1616137974103"></p><p>•华为诺亚方舟 2019 GPT-based Generation for Classical Chinese Poetry</p><p>经过一个大力出奇迹的过程</p><p><img src="/2018/12/28/nlp-jin-jie/ke-kong-wen-ben-sheng-cheng-survey/1616137988231.png" alt="1616137988231"></p><h2><span id="reference">Reference</span></h2><p>①Nishihara D, Kajiwara T, Arase Y. Controllable text simplification with lexical constraint loss[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop. 2019: 260-266.</p><p>②Keskar N S, McCann B, Varshney L R, et al. Ctrl: A conditional transformer language model for controllable generation[J]. arXiv preprint arXiv:1909.05858, 2019.</p><p>③Chen M, Tang Q, Wiseman S, et al. Controllable Paraphrase Generation with a Syntactic Exemplar[J]. arXiv preprint arXiv:1906.00565, 2019.</p><p>④Oraby S, Harrison V, Ebrahimi A, et al. Curate and generate: A corpus and method for joint control of semantics and style in neural nlg[J]. arXiv preprint arXiv:1906.01334, 2019.</p><p>⑤Cao Y T, Rao S, Daumé III H. Controlling the Specificity of Clarification Question Generation[C]//Proceedings of the 2019 Workshop on Widening NLP. 2019: 53-56.</p><p>⑥Liu Z, Fu Z, Cao J, et al. Rhetorically controlled encoder-decoder for modern chinese poetry generation[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 1992-2001.</p><p>⑦Ke P, Guan J, Huang M, et al. Generating informative responses with controlled sentence function[C]//Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2018: 1499-1508.</p><p>⑧Feng X, Liu M, Liu J, et al. Topic-to-Essay Generation with Neural Networks[C]//IJCAI. 2018: 4078-4084.</p><p>⑨Song Z, Zheng X, Liu L, et al. Generating Responses with a Specific Emotion in Dialog[C]//Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019: 3685-3695.</p><p>⑩Zhou H, Huang M, Zhang T, et al. Emotional chatting machine: Emotional conversation generation with internal and external memory[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 2018.</p><p>⑪Liao Y, Wang Y, Liu Q, et al. GPT-based Generation for Classical Chinese Poetry[J]. arXiv preprint arXiv:1907.00151, 2019.</p><p>⑫Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI Blog, 2019, 1(8): 9.</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 可控文本生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>近五年(2014-2018)中文分词论文及趋势整理</title>
      <link href="/2018/12/28/nlp-jin-jie/jin-wu-nian-2014-2018-zhong-wen-fen-ci-lun-wen-ji-qu-shi-zheng-li/"/>
      <url>/2018/12/28/nlp-jin-jie/jin-wu-nian-2014-2018-zhong-wen-fen-ci-lun-wen-ji-qu-shi-zheng-li/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-前言">1. 前言</span></h2><p>整理了近五年中文分词的文章，意图探索研究趋势。</p><p>文章来源来自于几大nlp顶会：ACL、EMNLP、COLING、NAACL.</p><p>在阅读16、17年文章的时候，发现了引用了这些文章的来自于AAAI、IJCAI的一些文章，也算是一个小Tip。</p><p>主要从以下几方面进行总结：</p><pre><code>论文名作者机构主要工作（解决了什么）主要方法（使用了什么）数据集取得的成果缺陷和不足下一步工作</code></pre><p>简要的统计与总结如下：</p><div class="table-container"><table><thead><tr><th></th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th><th>count</th></tr></thead><tbody><tr><td>ACL</td><td>5</td><td>3</td><td>4</td><td>3</td><td>0</td><td>15</td></tr><tr><td>EMNLP</td><td>3</td><td>2</td><td>0</td><td>2</td><td>1</td><td>8</td></tr><tr><td>NAACL</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr><tr><td>COLING</td><td>1</td><td>/</td><td>0</td><td>/</td><td>1</td><td>2</td></tr><tr><td>AAAI</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>3</td></tr><tr><td>IJCAI</td><td>/</td><td>0</td><td>0</td><td>2</td><td>1</td><td>3</td></tr><tr><td>Count</td><td>9</td><td>6</td><td>6</td><td>7</td><td>4</td><td>32</td></tr></tbody></table></div><p>ps：“/”代表未进行统计或当年未举办会议</p><p>自己总结的一些趋势（不够专业，仅供参考）：</p><div class="table-container"><table><thead><tr><th></th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th></tr></thead><tbody><tr><td>无监督</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>半监督</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr><tr><td>有监督</td><td>6</td><td>4</td><td>5</td><td>6</td><td>3</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>神经网络</td><td>1</td><td>3</td><td>5</td><td>6</td><td>4</td></tr><tr><td>传统方法</td><td>8</td><td>2</td><td>0</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>联合建模</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>特定领域</td><td>专利</td><td></td><td></td><td>微文本</td><td>医学</td></tr><tr><td>重点在于搭建or改进神经网络</td><td>1/9</td><td>3/5</td><td>3/5</td><td>3/7</td><td>2/4</td></tr><tr><td>解决领域适配问题</td><td>3/9</td><td>1/5</td><td>0/5</td><td>2/7</td><td>3/4</td></tr></tbody></table></div><div class="table-container"><table><thead><tr><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th></tr></thead><tbody><tr><td>无监督分词 *  2</td><td>双语半监督</td><td></td><td>joint S&amp;T task 词性标注，联合学习</td><td></td></tr><tr><td>领域适配问题 *  2  扩充语料库 全部和部分标注数据</td><td>神经网络、嵌入匹配</td><td>神经网络分词模型</td><td>更简单、更快、更准确。神经网络模型</td><td>改进模型—pretrained  embeddings、dropout、超参数调整</td></tr><tr><td>神经网络分词模型</td><td>门控递归神经网络，</td><td></td><td>对抗性多准则学习</td><td>新的医学领域中文分词框架</td></tr><tr><td>有监督模型精度  单词与字符结合的模型</td><td>LSTM</td><td>汉语自动分词用于口语理解和命名实体识别</td><td>多粒度中文分词，构建了数据集</td><td>利用未标记和部分标记的数据来完成跨领域的CWS任务</td></tr><tr><td>用于机器翻译的分词</td><td></td><td>基于依赖的门控递归神经网络</td><td>丰富的外部资源（预训练方法），以加强神经分词</td><td>词典与神经网络相结合</td></tr><tr><td>专利领域模型精度  手动标注并改进CRF</td><td>CRF改进，提升OOV召回率</td><td>基于词的汉语分割神经网络模型</td><td>中文微文本分割</td><td></td></tr><tr><td>半监督分词 词典  CRF</td><td></td><td>汉语分词评价指标</td><td>半监督神经网络的词-上下文字符嵌入</td><td></td></tr><tr><td></td><td>小说，神经网络，跨领域</td><td>半监督联合模型</td><td></td></tr></tbody></table></div><h2><span id="2-具体整理内容">2. 具体整理内容</span></h2><h3><span id="21-2014年">2.1 2014年</span></h3><p><strong>论文名：A Joint Model for Unsupervised Chinese Word Segmentation</strong></p><p><strong>作者：</strong>Miaohong Chen  Baobao Chang  Wenzhe Pei</p><p><strong>机构：</strong>教育部计算语言学重点实验室， 北京大学电子工程与计算机科学学院</p><p><strong>主要工作（解决了什么）：</strong>一种无监督中文分词的联合模型，提升了分词的精读</p><p><strong>主要方法（使用了什么）：</strong>结合了基于字符的模型、非参数贝叶斯语言模型和基于良好性的模型的优点。</p><p><strong>数据集：</strong> PKU and MSRA </p><p><strong>取得的成果：</strong>提高了无监督分词模型的精度、较强的中文分词歧义解决能力</p><p><strong>论文名：Effective Document-Level Features for Chinese Patent Word Segmentation</strong></p><p><strong>作者：</strong>Si Li     Nianwen Xue</p><p><strong>机构：</strong>Brandeis大学中文处理组</p><p><strong>主要工作（解决了什么）：</strong>手工分割了大量的专利数据，设计了文档级的特性来捕捉专利中科学和技术术语的分布特征，</p><p><strong>主要方法（使用了什么）：</strong>CRF模型，提取了字符、词语、文档级别的特征</p><p><strong>数据集：</strong>142 Chinese patents（自己标注）</p><p><strong>取得的成果：</strong>解决了专利领域的分词问题</p><p><strong>论文名：Domain Adaptation for CRF-based Chinese Word Segmentation using</strong></p><p><strong>Free Annotations</strong></p><p><strong>作者：</strong>Yijia Liu , Yue Zhang , Wanxiang Che , Ting Liu , Fan Wu</p><p><strong>机构：</strong>新加坡科技与设计大学，麻省理工学院社会计算和信息检索研究中心，哈尔滨工业大学</p><p><strong>主要工作（解决了什么）：</strong>通过将各种免费标注源转换为部分标注数据的一致形式。并构造一种可以同时使用全部标注和部分标注数据进行训练的CRF变体，研究了分词的<strong>领域适配问题</strong>。</p><p><strong>数据集：</strong>自由数据+免费标注数据</p><p><strong>取得的成果：</strong>研究了分词的<strong>领域适配问题。</strong>自由数据的有效性，发现它们对于提高分割精度是有用的。</p><p><strong>论文名：</strong>Max-Margin Tensor Neural Network for Chinese Word Segmentation</p><p><strong>作者：</strong>Wenzhe Pei Tao Ge Baobao Chang∗</p><p><strong>机构：</strong>教育部计算语言学重点实验室，北京大学电子工程与计算机科学学院</p><p><strong>主要工作（解决了什么）：</strong>提出了一种新的中文分词神经网络模型——最大边缘张量神经网络(MMTNN)，显式地模拟了标签和上下文字符之间的交互。提出了一种有效地提高模型效率和避免过拟合风险的张量因子分解方法。</p><p><strong>主要方法（使用了什么）：</strong>模型明确地模拟了标签和上下文字符之间的交互，从而获取了更多的语义信息。在神经网络模型中引入张量分解用于序列标记任务，加快了模型的训练和推理，防止了过拟合。</p><p><strong>数据集：</strong>PKU and MSRA datasets</p><p><strong>取得的成果：</strong>我们的模型比以前的神经网络模型具有更好的性能，并且我们的模型可以在最小的特征工程条件下获得具有竞争力的性能。</p><p><strong>下一步工作：</strong>进一步扩展模型，并将其应用于其他结构预测问题。</p><p><strong>论文名：</strong>Automatic Corpus Expansion for Chinese Word Segmentation by Exploiting the Redundancy of Web Information</p><p><strong>作者：</strong>Xipeng Qiu, ChaoChao Huang and Xuanjing Huang</p><p><strong>机构：</strong>上海市智能信息处理重点实验室，复旦大学计算机科学学院</p><p><strong>主要工作（解决了什么）：</strong>处理一个新的领域而没有足够的标注语料库时，监督方法就不能很好地工作。</p><p>提出了一种自动扩充外域文本训练语料库的方法</p><p><strong>主要方法（使用了什么）：</strong>通过网络提供的大量相关的容易分割的句子，来分割一个复杂和不确定的分段。挑选出一些可靠的分段句，并将它们添加到语料库中，从而扩充语料库。</p><p><strong>数据集：</strong>both CTB6.0 and CTB7.0 datasets</p><p><strong>取得的成果：</strong>提出了一种自动扩充外域文本训练语料库的方法</p><p><strong>下一步工作：</strong>我们的方法的长期目标是建立一个在线的、不断学习的系统，能够识别困难的任务，并从众包中寻求帮助。</p><p><strong>论文名：</strong>Two Knives Cut Better Than One:Chinese Word Segmentation with Dual Decomposition</p><p><strong>作者：</strong>Mengqiu Wang、Rob Voigt、Christopher D. Manning</p><p><strong>机构：</strong>Stanford University  （Computer Science &amp; Linguistics） Department</p><p><strong>主要工作（解决了什么）：</strong>中文分词主要有两种方法:基于单词的分词和基于字符的分词，两种方法各具优势。先前的研究表明，将这两种模型结合起来可以提高分割性能。提出了一种有效地结合两种分割方案的强度的方法，使用一种有效的双分解算法进行联合推理。</p><p><strong>主要方法（使用了什么）：</strong></p><p><strong>数据集：</strong>SIGHAN 2005  第二届国际汉语分词</p><p><strong>取得的成果：</strong>本文提出了一种基于对偶分解的中文分词方法。可以对现有的CWS系统进行联合解码，比单独的任何一个系统都更加准确和一致，实现了迄今为止在该任务的标准数据集上所报告的最佳性能。</p><p><strong>论文名：</strong>Empirical Study of Unsupervised Chinese Word Segmentation Methods for SMT on Large-scale Corpora</p><p><strong>作者：</strong>Xiaolin Wang、 Masao Utiyama 、Andrew Finch、 Eiichiro Sumita</p><p><strong>机构：</strong>国家信息与通信技术研究所 （日本？）</p><p><strong>主要工作（解决了什么）：</strong>无监督分词(UWS)可以在没有标注数据的情况下为统计机器翻译(SMT)提供领域自适应分词，双语的UWS甚至可以优化分词的对齐。提出了一种有效的统一的基于pyp的单语和双语无监督方法。</p><p><strong>主要方法（使用了什么）：</strong>本研究旨在为统计机器翻译建立一个更好的中文分词模型。该算法利用基于双语特征的对齐自动学习的词界信息，提出了一种较好的分割模型。</p><p><strong>数据集：</strong>Chinese SIGHAN-MSR corpus</p><p><strong>取得的成果：</strong>提高无监督分词模型的精度</p><p><strong>论文名：</strong>Semi-Supervised Chinese Word Segmentation Using Partial-Label Learning With Conditional Random Fields 基于条件随机场的部分标签学习的半监督中文分词方法</p><p><strong>作者：</strong>Fan Yang 、Paul Vozila </p><p><strong>机构：</strong>Nuance Communications Inc. 从事语音识别软件、图像处理软件及输入法软件研发销售的公司</p><p><strong>主要工作（解决了什么）：</strong>Wikipedia数据中的标点符号和实体标记在句子中定义了一些单词边界。采用条件随机场的部分标签学习方法，将这些有价值的知识用于半监督汉语分词。</p><p><strong>数据集：</strong>CTB-6 corpus</p><p><strong>取得的成果：</strong>CTB-6 test set is 95.98% in F-measure</p><p><strong>论文名：</strong>Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</p><p><strong>作者：</strong>Xiaodong Zeng、 Lidia S. Chao、 Derek F. Wong、 Isabel Trancoso、 Liang Tian</p><p><strong>机构：</strong>澳门大学计算机与信息科学系</p><p><strong>主要工作（解决了什么）：</strong>针对SMT任务提出了一种新的CWS模型。该模型的目的是维护树库数据的语言分割监督，同时集成由bitexts产生的有用的双语分割。</p><p><strong>主要方法（使用了什么）：</strong>从基于字符的对齐中学习词的边界; 将学习到的单词边界编码为GP约束; 在GP约束下，利用PR框架对CRFs模型进行训练。</p><p><strong>数据集：</strong>BLEU、NIST、METEOR 机器翻译质量</p><p><strong>取得的成果：</strong>该模型能较好地实现用于机器翻译的分词</p><h3><span id="22-2015年">2.2 2015年</span></h3><p><strong>论文名：</strong>Semi-supervised Chinese Word Segmentation based on Bilingual</p><p>Information</p><p><strong>作者：</strong>Wei Chen、Bo Xu</p><p><strong>机构：</strong>自动化所</p><p><strong>主要工作（解决了什么）：</strong>半监督的分词模型，包含三个层次的双语语言特征学习的层叠对数线性模型</p><p><strong>数据集：</strong>1998人民日报语料库、Bakeoff-2  PKU、AS</p><p><strong>取得的成果：</strong>精度超过优于目前最先进的单语和双语半监督方法。</p><p><strong>论文名：</strong>Gated Recursive Neural Network for Chinese Word Segmentation</p><p><strong>作者：</strong>Xinchi Chen, Xipeng Qiu∗ , Chenxi Zhu, Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海市智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>提出了一种用于中文分词的门控递归神经网络(GRNN)。由于GRNN相对较深，我们还采用了一种有监督的分层训练方法来避免梯度扩散问题。</p><p><strong>数据集：</strong>PKU, MSRA and CTB6  NLPCC 2015 dataset微博文本</p><p><strong>取得的成果：</strong>模型优于以往的神经网络模型和最先进的方法。</p><p><strong>下一步工作：</strong>GRNN在其他序列标记任务上的应用。</p><p><strong>论文名：</strong>Long Short-Term Memory Neural Networks for Chinese Word Segmentation</p><p><strong>作者：</strong>Xinchi Chen, Xipeng Qiu∗ , Chenxi Zhu, Pengfei Liu, Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海市智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>提出了一种新的汉语分词神经网络模型，采用长短时记忆(LSTM)神经网络</p><p><strong>数据集：</strong>PKU, MSRA and CTB6 </p><p><strong>取得的成果：</strong>模型优于以往的神经网络模型和最先进的方法。</p><p><strong>下一步工作：</strong>改为双向</p><p><strong>论文名：</strong>Synthetic Word Parsing Improves Chinese Word Segmentation</p><p><strong>作者：</strong>Fei Cheng、 Kevin Duh、 Yuji Matsumoto</p><p><strong>机构：</strong>奈良信息科学研究所</p><p><strong>主要工作（解决了什么）：</strong>提出了一种利用人工分词器提高汉语分词性能的新方法，该解析器分析单词的内部结构，并试图将未登录词转换为词汇内的细粒度子单词。我们提出了一个管道CWS系统，该系统首先预测这种细粒度的分割，然后对输出进行分段，以重构原始的分词标准。</p><p><strong>数据集：</strong>PKU和MSR</p><p><strong>取得的成果：</strong>提升了精度。在OOV召回方面有了实质性的改进。</p><p><strong>论文名：</strong>Accurate Linear-Time Chinese Word Segmentation via Embedding Matching</p><p><strong>作者：</strong>Jianqiang Ma 、Erhard Hinrichs</p><p><strong>机构：</strong>德国图宾根大学语言学部</p><p><strong>主要工作（解决了什么）：</strong>本文提出了一种嵌入匹配的中文分词方法，该方法对传统的序列标记框架进行了推广，并利用了分布式表示的优点。在此基础上，提出了一种基于基准语料库的贪心分割算法。</p><p><strong>数据集：</strong>PKU  MSR</p><p><strong>取得的成果：</strong>提升了精度</p><h3><span id="23-2016年">2.3 2016年</span></h3><p><strong>论文名：</strong>Neural Word Segmentation Learning for Chinese</p><p><strong>作者：</strong>Deng Cai and Hai Zhao∗</p><p><strong>机构：</strong>上交计算机系</p><p><strong>主要工作（解决了什么）：</strong>在本文中，我们提出了一种新的神经网络框架，它彻底消除了上下文窗口，可以利用完整的分割历史。我们的模型采用基于字符的门控组合神经网络生成候选词的分布式表示，然后将其输入长期短期记忆(LSTM)语言评分模型。</p><p><strong>主要方法（使用了什么）：</strong>新的中文分词神经网络框架</p><p><strong>数据集：</strong>PKU、MSR</p><p><strong>取得的成果：</strong>提升精度。神经网络分词模型</p><p><strong>论文名：</strong>An Empirical Study of Automatic Chinese Word Segmentation for Spoken Language Understanding and Named Entity Recognition 汉语自动分词用于口语理解和命名实体识别的实证研究</p><p><strong>作者：</strong>Wencan Luo∗     Fan Yang</p><p><strong>机构：</strong>匹兹堡大学</p><p><strong>主要工作（解决了什么）：</strong>分词通常被认为是许多汉语自然语言处理任务的第一步，但它对这些后续任务的影响却相对缺乏研究。在对新数据应用现有的分词器时，如何解决不匹配问题?一个更好的分词器会产生更好的NLP后续任务性能吗?</p><p><strong>主要方法（使用了什么）：</strong>将分词输出作为附加特征，采用部分学习的自适应技术，利用n-best分词表。</p><p><strong>数据集：CTB6、PKU、NER、</strong>NER 3-fol</p><p><strong>取得的成果：</strong>解决分词应用到新数据不匹配的问题—3个方法。（提高了SLU、NER精度）</p><p><strong>论文名：</strong>A New Psychometric-inspired Evaluation Metric for Chinese Word Segmentation</p><p><strong>作者：</strong>Peng Qian Xipeng Qiu∗ Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>一种新的基于心理测量学的汉语分词评价指标</p><p><strong>主要方法（使用了什么）：</strong>从心理测量学的基本思想入手，在困难与容易、奖励与惩罚之间取得平衡，提高评价的准确性。</p><p><strong>数据集：PKU、MSR、NCC</strong></p><p><strong>取得的成果：</strong>一种新的基于心理测量学的汉语分词评价指标。实际评价结果表明，所提出的评价指标给出的评价结果更加合理、易于区分，与人的评价结果具有较好的相关性。传统的精度、召回率和F-score。</p><p><strong>论文名：</strong>Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation</p><p><strong>作者：</strong>Jingjing Xu and Xu Sun</p><p><strong>机构：</strong>北京大学计算机语言学教育部重点实验室</p><p><strong>主要工作（解决了什么）：</strong>一种基于依赖的门控递归神经网络</p><p><strong>主要方法（使用了什么）：</strong>为了将局部特征与长距离依赖相结合，提出了一种基于依赖的门控递归神经网络。局部特征首先由双向长短期记忆网络收集，然后通过门控递归神经网络将其组合并细化为长距离依赖。</p><p><strong>数据集：PKU、MSRA、CTB6</strong></p><p><strong>取得的成果：提高了精度</strong></p><p><strong>论文名：</strong>Transition-Based Neural Word Segmentation</p><p><strong>作者：</strong>Meishan Zhang1 and Yue Zhang2 and Guohong Fu1</p><p><strong>机构：</strong>黑龙江大学计算机科学与技术学院</p><p><strong>主要工作（解决了什么）：</strong>一种基于词的中文分词神经模型，将人工设计的离散特征替换为基于词的分词框架中的神经特征。</p><p><strong>数据集：</strong>PKU、MSR</p><p><strong>取得的成果：</strong>基于词的汉语分割神经网络模型。该模型可以方便地利用离散特征，从而得到了一个与以往工作相比性能最好的组合模型。提高精度。</p><h3><span id="24-2017年">2.4 2017年</span></h3><p><strong>论文名：</strong>Fast and Accurate Neural Word Segmentation for Chinese</p><p><strong>作者：</strong>Deng Cai1,2, Hai Zhao1,2,∗, Zhisong Zhang1,2, Yuan Xin3, Yongjian Wu3, Feiyue Huang3</p><p><strong>机构：</strong>上海交通大学计算机科学与工程系</p><p><strong>主要工作（解决了什么）：</strong>现有神经模型的训练和工作过程都存在计算效率低下的问题。提出了一种具有均衡的字和字符嵌入输入的贪婪神经词分割器，以克服现有的缺陷。</p><p><strong>主要方法（使用了什么）：</strong>一种新型的字符 - 字平衡机制，用于字的表示生成。通过去掉不必要的设计，建立更有效的字符组合模型。最大间隔训练期间的早期更新策略。</p><p><strong>数据集：PKU  95.8、MSR 97.1 （速度快）</strong></p><p><strong>取得的成果：</strong>更简单、更快、更准确。神经网络模型</p><p><strong>论文名：</strong>Adversarial Multi-Criteria Learning for Chinese Word Segmentation </p><p>汉语分词的对抗性多准则学习</p><p><strong>作者：</strong>Xinchi Chen, Zhan Shi, Xipeng Qiu∗ , Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>提出了针对CWS的对抗性多准则学习，将来自多个异构分割准则的共享知识集成在一起。</p><p><strong>数据集：</strong>MSRA AS PKU CTB CKIP CITYU NCC SXU</p><p><strong>取得的成果：</strong>在8个具有异构分割标准的语料库上的实验表明，与单标准学习相比，每个语料库的性能都有了显著的提高。</p><p><strong>论文名：</strong>Multi-Grained Chinese Word Segmentation     </p><p><strong>作者：</strong>Chen Gong, Zhenghua Li∗ , Min Zhang, Xinzhou Jiang</p><p><strong>机构：</strong>苏州大学</p><p><strong>主要工作（解决了什么）：</strong>利用三个SWS数据集的注释异构性，构建了用于模型训练和调优的大型伪MWS数据集。然后我们用真正的MWS注释手工注释1500个测试句子。提出了三种将MWS转换为构成解析和序列标记的基准测试方法。</p><p><strong>数据集：CTB、MSR、PPD</strong></p><p><strong>取得的成果：</strong>提出并解决了多粒度中文分词。</p><p><strong>论文名：</strong>Neural Word Segmentation with Rich Pretraining</p><p><strong>作者：</strong>Jie Yang∗ and Yue Zhang∗ and Fei Dong</p><p><strong>机构：</strong>新加坡科技与设计大学</p><p><strong>主要工作（解决了什么）：</strong>对大型中文文本进行预处理，嵌入字符和单词，可以提高分割准确率</p><p><strong>主要方法（使用了什么）：</strong>预训练是利用外部资源提高精度的一种方法</p><p><strong>数据集：</strong>CTB6  96.2  PKU 96.3  MSR 97.5  AS 95.7  CityU 96.9  Weibo 95.5</p><p><strong>取得的成果：</strong>研究了丰富的外部资源（预训练方法），以加强神经分词。。</p><p><strong>论文名：</strong>Segmenting Chinese Microtext: Joint Informal-Word Detection and Segmentation - IJCAI</p><p>with Neural Networks</p><p><strong>作者：</strong>Meishan Zhang, Guohong Fu∗, Nan Yu</p><p><strong>机构：</strong>黑龙江大学</p><p><strong>主要工作（解决了什么）：</strong>微文本中的非正式词汇问题</p><p><strong>主要方法（使用了什么）：</strong>提出了一种通过分词和非正式词检测同时进行的方法来增强中文微文本分割的联合模型。</p><p><strong>数据集：</strong>CTB6.0 and the released Weibo corpus</p><p><strong>取得的成果：</strong>该联合模型可以显著提高微博数据集的分割性能，提高幅度超过3%。</p><p><strong>论文名：</strong>Word-Context Character Embeddings for Chinese Word Segmentation</p><p><strong>作者：</strong>Hao Zhou∗ Zhenting Yu∗ Yue Zhang  Shujian Huang  Xinyu Dai Jiajun Chen</p><p><strong>机构：</strong>南京大学</p><p><strong>主要工作（解决了什么）：</strong>本文提出了一种基于分词标签信息的文本上下文字符嵌入方法。该方法将标签分布信息打包到嵌入中，可以看作是知识参数化的一种方式。</p><p><strong>数据集：</strong>CTB6 96.2    PKU 96.0   MSR    97.8</p><p><strong>取得的成果：</strong> 提出了半监督神经网络的词-上下文字符嵌入，使分割模型对域内数据更加准确，对域外数据更加稳健。</p><p><strong>论文名：</strong>A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</p><p><strong>作者：</strong>Xinchi Chen, Xipeng Qiu∗ , Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>本文提出了一种用于汉语分词和词性标注联合作业的特征丰富的神经模型。具体来说，为了模拟传统离散特征模型的特征模板，我们使用不同的滤波器对复杂的混合特征进行卷积和池化建模，然后利用递归层的长距离依赖信息。</p><p><strong>数据集：CTB、PKU、NCC</strong></p><p><strong>取得的成果：</strong> joint S&amp;T task 词性标注精度提高</p><h3><span id="25-2018年">2.5 2018年</span></h3><p><strong>论文名：</strong>State-of-the-art Chinese Word Segmentation with Bi-LSTMs</p><p><strong>作者：</strong>Ji Ma、 Kuzman Ganchev、 David Weiss</p><p><strong>机构：</strong>Google AI Language</p><p><strong>主要工作（解决了什么）：改进模型—</strong>pretrained embeddings、dropout、超参数调整</p><p><strong>数据集：</strong>AS  CITYU CTB6 CTB7 MSR PKU UD</p><p> 98.03 98.22 97.06 97.07 98.48 97.95 97.00</p><p><strong>取得的成果：</strong>为汉语分词的进一步发展提供了两条重要的证据。首先，比较不同的模型体系结构需要仔细地调整和应用最佳实践，以便获得严格的比较。其次，如果不进一步努力收集数据，对神经体系结构的迭代可能不足以解决剩余的类分割错误。</p><p><strong>论文名：</strong>Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text</p><p><strong>作者：</strong>Junjie Xing、 Kenny Q. Zhu、 Shaodian Zhang</p><p><strong>机构：上交</strong></p><p><strong>主要工作（解决了什么）：</strong>提出了一个自适应多任务转移学习框架和三个不同设置的模型实例。</p><p><strong>数据集：</strong>PKU  MSR   WEIBO</p><p><strong>取得的成果：</strong>提出了一种新的医学领域中文分词框架。提高了医学领域分词。三个医学数据集的标注工作。</p><p><strong>论文名：</strong>Neural Networks Incorporating Unlabeled and Partially-labeled Data for Cross-domain Chinese Word Segmentation</p><p><strong>作者：</strong>Lujun Zhao, Qi Zhang, Peng Wang, Xiaoyu Liu</p><p><strong>机构：</strong>复旦大学智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong></p><p><strong>主要方法（使用了什么）：</strong>提出了一种新的神经网络模型，利用未标记和部分标记的数据来完成跨领域的CWS任务。为了利用无标记数据，通过门机制将两种字符级语言模型与分割模型相结合。通过修改损失函数，利用部分标记数据对模型进行训练。</p><p><strong>数据集：</strong>the labeled People’s Daily (PD) dataset. SIGHANBakeoff 2010 Chinese Wikipedia</p><p><strong>取得的成果：</strong>解决缺乏注释数据的资源贫乏领域的CWS问题</p><p><strong>论文名：</strong>Neural Networks Incorporating Dictionaries for Chinese Word Segmentation</p><p><strong>作者：</strong>Qi Zhang, Xiaoyu Liu, Jinlan Fu</p><p><strong>机构：</strong>复旦大学智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>试图解决汉语分词任务中词典与神经网络相结合的问题。研究了基于神经网络的汉语分词任务字典集成问题。针对CWS任务，我们提出了两种将从字典中提取的信息集成到基于神经网络的方法中的方法。</p><p><strong>数据集：</strong>PKU MSR AS CITYU CTB6   2010bakeoff  </p><p>96.5 97.8 95.9 96.9 96.4</p><p><strong>取得的成果：</strong>合并字典可以显著提高神经分词能力。比目前最先进的神经网络模型和领域适应模型取得了更好的效果。</p>]]></content>
      
      
      <categories>
          
          <category> NLP进阶 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 近五年(2014-2018)中文分词论文及趋势整理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cmake由浅入深</title>
      <link href="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/"/>
      <url>/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>代码见：<a href="https://github.com/htfhxx/cmake_htfhxx" target="_blank" rel="noopener">https://github.com/htfhxx/cmake_htfhxx</a></p><h2><span id="1-windows-和linux下执行单文件">1. Windows 和linux下执行单文件</span></h2><p>在windows环境下，大家都熟悉怎么编写并执行一份代码：</p><ol><li>先打开编译器例如codeblocks编写源代码，例如一个c++文件；</li><li>点击编译按钮，编译代码，生成.o的目标文件。</li><li>点击执行按钮，生成.exe的可执行文件，运行完毕。</li></ol><p>例如codeblocks下的build &amp; run：</p><p><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079026002.png" alt="1579079026002"></p><p>在linux环境下呢？</p><p>用vim编辑器编写代码，得到一个文本文件 main.cpp</p><p><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079040506.png" alt="1579079040506"></p><ol><li><p>使用g++编译main.cpp得到a.out文件<br><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079080005.png" alt="1579079080005"></p></li><li><p>执行a.out文件，执行完毕得到结果<br><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079074703.png" alt="1579079074703"></p></li></ol><p>当然，更普遍的是使用-o来编译和执行的</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079089888.png" alt="1579079089888"></p><h2><span id="2-windows-和linux下执行多文件or项目">2. Windows 和linux下执行多文件or项目</span></h2><p>Windows自不必多说，在编译器下编译运行main函数即可</p><p>至于linux，有如下三个文件speak.h speak.cpp hellospeak.cpp</p><p>​    <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079098670.png" alt="1579079098670"></p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079114625.png" alt="1579079114625"></p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079124731.png" alt="1579079124731"></p><p>编译执行多个文件：</p><p> g++ hellospeak.cpp speak.cpp -o hellospeak</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079128351.png" alt="1579079128351"></p><p>这个时候会发现，如果源文件太多，一个一个编译时就会特别麻烦。于是人们想到了制作一种类似批处理的程序，来批处理编译源文件，于是就有了make工具。它是一个自动化的编译工具，你可以使用一条命令实现完全编译。但是你需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile。</p><h2><span id="3-解决多文件编译的困难makefile">3. 解决多文件编译的困难：makefile</span></h2><p>文件下包含一个头文件和两个cpp文件，以及一个写好的makefile：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079133459.png" alt="1579079133459"></p><p>Makefile大致内容就是要编译两个文件得到hellospeak.o和speak.o，再生成可执行文件hellospeak，最后删掉.o文件。</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079136368.png" alt="1579079136368"></p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079141053.png" alt="1579079141053"></p><p>对于一个大工程，编写makefile实在是件复杂的事，于是就出现了cmake工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。</p><h2><span id="4-cmake工具编译运行文件">4. Cmake工具:编译运行文件</span></h2><p>如果没有安装的话就通过sudo apt-get install cmake命令安装cmake工具：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079147001.png" alt="1579079147001"></p><p>准备好cmakelist.txt文件和要执行的main.cpp，以及一个build文件，用于放入cmake编译的繁多的中间文件：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079150587.png" alt="1579079150587"></p><p>要执行的main.cpp：</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079154023.png" alt="1579079154023"></p><p>cmakelist.txt文件（内容撰写待会再说）：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079156649.png" alt="1579079156649"></p><p>第一步，cmake + (cmakelists.txt所在文件夹)。</p><p>此处“..”指的是上一级文件夹，会从文件夹中找到cmakelists.txt：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079162873.png" alt="1579079162873"></p><p>系统自动生成了：CMakeFiles, CMakeCache.txt, cmake_install.cmake 等文件，并且生成了Makefile</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079166055.png" alt="1579079166055"></p><p>进行工程的实际构建，在这个目录输入<code>make</code> 命令，大概会得到如下的彩色输出：</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079171352.png" alt="1579079171352"></p><p>到这里就已经编译完成了，接下来执行这个项目得到hello world的输出：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079174053.png" alt="1579079174053"></p><p>即，整个流程为（网图，侵删）：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079177639.png" alt="1579079177639"></p><h2><span id="5-使用cmake方便的编译执行单文件demo">5. 使用cmake方便的编译执行单文件Demo</span></h2><p>先分析上一例子中的cmakelists.txt：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079183726.png" alt="1579079183726"></p><p>cmake_minimum_required(VERSION 2.8) </p><p>//指的是支持的cmake版本，可省略，但是为了方便后人，尽量加上自己所用的版本。</p><p>project(HelloWorld)</p><p>//指定项目名称，编译完成后生成的名字就是HelloWorld</p><p>add_executable(HelloWorld main.cpp) </p><p>//加入执行文件，此处是单文件，待会展开来讲</p><h2><span id="6-使用cmake方便的编译执行多文件项目">6. 使用cmake方便的编译执行多文件项目</span></h2><p> 两个cpp文件一个.h文件和一个build，这次我们试着用cmake编译执行多文件。</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079188704.png" alt="1579079188704"></p><p>这个example的cmakelists.txt里包含了之前讲的版本和项目名，加了一个include_directories，这个参数是把.h文件所在目录包含进去，可以是一堆的.h文件。其中cmake_source_dir是系统变量，可以通过set关键字来设置，默认来说是cmakelists.txt所在的文件。</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079192181.png" alt="1579079192181"></p><p>接着就是执行两个cpp文件，当然两个cpp文件也可以通过set来设置成变量Sources_code.</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079196573.png" alt="1579079196573"></p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079199576.png" alt="1579079199576"></p><h2><span id="7-一个复杂的例子关于cmakelists子目录-and-生成库">7. 一个复杂的例子：关于CMakelists子目录 and 生成库</span></h2><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079203657.png" alt="1579079203657"></p><p>这个文件夹下包含build文件、一个要执行的cpp文件和CMakelists.txt。除此之外，还有一个MathFunctions文件夹。</p><p>先来看子目录MathFunctions：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079206479.png" alt="1579079206479"></p><p>这里的CMakeLists.txt只有几行，将speak.cpp中的函数生成一个MathFunctions库。</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079211710.png" alt="1579079211710"></p><p>在顶层目录下的CMakeLists.txt中，通过add_subdirectories加入子目录的CMakeLists.txt。</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079215728.png" alt="1579079215728"></p><p>前两行是版本和项目名，五六行加入.h文件的目录，10行设置一个变量默认为ON,12行判断是否OK，如果OK为ON的话就可以执行13-23行。</p><p>13-23行，是确定使用自己本地（即MathFunctions文件夹中的库），分别是加入.h文件、加入子目录（划重点，下面讲）、设置EXTRA_LIBS变量，如果未设置28行就不再链接这个库。</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079221077.png" alt="1579079221077"></p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079224414.png" alt="1579079224414"></p><p>  <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079227552.png" alt="1579079227552"> </p><h2><span id="8-总结cmakeliststxt的整理内容">8. 总结cmakelists.txt的整理内容</span></h2><p> cmakelists.txt内容不需区分大小写。</p><p>一般的cmakelists.txt的编写，包括以下几部分：</p><ol><li><p>指定cmake版本，就像上面所说的：为了方便后人，尽量加上自己所用的版本cmake_minimum_required(VERSION 2.8) </p></li><li><p>指定项目的名称，一般和项目的文件夹名称对应</p></li></ol><p>project(HelloWorld)</p><ol><li>设置环境变量 SET(变量名 变量值)<br>一般包括（但不仅仅包括）：</li></ol><pre><code>CMAKE_C_COMPILER：指定C编译器CMAKE_CXX_COMPILER：指定C++编译器CMAKE_C_FLAGS：编译C文件时的选项，如-g；也可以通过add_definitions添加编译选项EXECUTABLE_OUTPUT_PATH：可执行文件的存放路径LIBRARY_OUTPUT_PATH：库文件路径CMAKE_BUILD_TYPE:：build 的类型(Debug, Release, ...)</code></pre><p>变量很多很复杂，根据需要使用即可，可以从官方文档中查找：<a href="https://cmake.org/cmake/help/v3.0/manual/cmake-variables.7.html" target="_blank" rel="noopener">https://cmake.org/cmake/help/v3.0/manual/cmake-variables.7.html</a></p><p>当然还有一些自己定义的变量名，也用set设置</p><ol><li><p>LINK_DIRECTORIES  添加需要链接的库文件目录,即链接库搜索路径<br>link_directories(directory1 directory2 …)</p></li><li><p>添加可执行文件要链接的库文件的名称<br>TARGET_LINK_LIBRARIES(PROJECT_NAME libname.so)</p></li></ol><ol><li><p>头文件目录</p><pre><code>INCLUDE_DIRECTORIES(  Include )如果文件夹较多，则可以这样写：INCLUDE_DIRECTORIES( ${CMAKE_SOURCE_DIR}/include/ ${CMAKE_SOURCE_DIR}/include/a/ ${CMAKE_SOURCE_DIR}/include/b/)</code></pre></li></ol><ol><li><p>源文件目录<br>AUX_SOURCE_DIRECTORY(src DIR_SRCS)</p></li><li><p>添加要编译的可执行文件<br>ADD_EXECUTABLE(PROJECT_NAME TEST_CPP)</p></li><li><p>生成动态库or 静态库<br>这里多说两句，用cmake生成静态动态库，是将在cmakelists.txt文件中加入的源文件头文件等等，生成一个类似于.h/.a的文件<br>这与<8>中加入想要编译的可执行文件是二选一的关系。<br>add_library(person SHARED ${srcs})<br>add_library(person_static STATIC ${srcs})</8></p></li></ol><p>install（TARGETS）<br>创建规则以将列出的目标安装到给定目录中。</p>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cmake由浅入深 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>东北大学史上最全转专业攻略</title>
      <link href="/2017/07/20/jing-yan-fen-xiang/dong-bei-da-xue-shi-shang-zui-quan-zhuan-zhuan-ye-gong-lue/"/>
      <url>/2017/07/20/jing-yan-fen-xiang/dong-bei-da-xue-shi-shang-zui-quan-zhuan-zhuan-ye-gong-lue/</url>
      
        <content type="html"><![CDATA[<p>此文是大三时发在贴吧里的，博客建立后整理到这里，原帖链接：<a href="http://tieba.baidu.com/p/5233701747?fid=44908" target="_blank" rel="noopener">http://tieba.baidu.com/p/5233701747?fid=44908</a></p><h2><span id="1-背景介绍">1. 背景介绍</span></h2><p>本人是15级转专业学生，非大神，因为大一结束后成绩较差，又很想转专业，所以研究了近两年的转专业政策，打听了真实的转专业状况，期间联系到二十多位转入到不同专业的学长学姐，在此郑重感谢他们。</p><p>因为有不少的面临转专业机会的学弟学妹甚至刚入学的新生私下问我怎么转专业该不该转专业能不能转专业等问题，而转专业又涉及到你接下来三年的大学生活和以后甚至一辈子工作的方向，是一个应该慎之又慎的事情，我既不想三言两语说不清楚，又不想因为我的主观观点给你们带来影响，因此，我只是把我知道的了解的告诉你们，由你们自己来判断，选择自己接下来要走的路。</p><p>在这里难免要提到一些专业的名字，本人表示并无恶意，如果有观点不一的地方，欢迎讨论，有说的不对的地方，欢迎批评指正。</p><h2><span id="2-为什么要转专业-浅析各专业行业形势">2. 为什么要转专业-浅析各专业行业形势</span></h2><p>东大官网机构设置可得到东北大学所有的学院，对于院系划分，基本上类似或者同种的专业放在同一个学院中，也就是说，同学院即同行业（当然不同学院也可能是同行业，例如计算机学院与软件学院）。</p><p>为方便介绍转专业相关，我将这些专业划为三类：体育类，艺术类，普通类（包括文史类和理工类）。本人是工科生，对体育类和艺术类了解较少，就不瞎说了，而普通类大同小异，只需要注意，文史类不可转入理工类，理工类可转入文史类。</p><p>众所周知，东北大学听着像综合大学，但是他实际上是一个工科类大学，他强在理工类专业，在文史类有强专业但是整体偏弱。而在这些专业中，东北大学在冶金，采矿，机械等传统工业上极强，然而这些传统工业行业低迷，在信息时代，其相同努力程度带来的成果略不及自动化、计算机、软件等信息相关的行业。在这里我要普及一个常识，学校好的专业放在外面不一定好（例如采矿等，排名好然而待遇差），学校差的专业不一定出去以后就差（比如机械学院的车辆工程，分流后成绩靠前顺利保研岂不是美滋滋）。</p><p>因此，每年都有大量大量的大神从理学院、资土学院、冶金学院、材料学院、机械学院、建筑学院等转入信息学院、计算机学院、软件学院。然而，并不是说机械材料等专业不如自动化计算机软件电气，它就特别不好了，在机械材料这些专业中，仍然有一批一批的大神签到好工作，保研到好学校等等。只不过是行业的影响使得他们获得同种水平的回报前，需要更努力一点。网络上各种论坛上的各种信息，大家要有辨别的能力，不要听人说啥就是啥（毕竟哪一行都有混的不好的喷子），而一些学长学姐的切身经验就比较真实了，多途径获取信息，也是大学生最重要的能力之一。</p><p>因此，要不要转专业，你需要去通过各种途径去了解你的本专业和目标专业。如果你有转专业的资格，那么在高考后仓促报下的志愿，你又有了第二次机会去选择。如果没有资格还不喜欢本行业，那么自学其他专业知识，跨考研究生，甚至转销售、创业（虽然这些我不太赞成）也不是行不通的。关键在个人！</p><h2><span id="3-转专业需要做的事情">3. 转专业需要做的事情</span></h2><p>转专业，如果你是准大二，在大类专业分流后，你需要在下个学期提前去学校，填表格交给教务处等结果（也可能有面试），一般在开学前你就能得到转专业的结果了。</p><p>在这之前，你需要的是，本专业所在学院的转专业政策，目标专业所在学院的转专业政策。这些在学院的官网上或者教务处上都有，没有的话年级群也会发，如果找不到目标专业的政策，就去找一个那个专业的同学，问他要一份就ok了。仔细研读政策，这是你需要干的。</p><h2><span id="4-转专业政策解读以信息学院为例">4. 转专业政策解读（以信息学院为例）</span></h2><p>转专业，分两步，1.转出；2转入。转出需要满足本学院的要求，转入需要满足目标学院的要求。</p><p>百度：东北大学信息学院，进入官网，右边的学院通知有16级转专业政策，这就是传说中的转专业政策，接下来我一点点解读。</p><p>一、 工作原则：介绍了转专业的基本要求，以及部分本学院的特殊班级的转专业要求。</p><p>二、 组织机构：都是学院的院长教授副教授，不用管。、</p><p>三、 转出转入计划（重点）：这涉及到你们学院的转专业名额，一般转出的人会少于转出名额，不用担心，尤其是排名在名额内的，更不用担心。转入的名额就很重要了，一般转入名额越多，转入越容易。在后面我会简单介绍各专业转入难度。</p><p>四、 转出工作机制（重中之重）：</p><p>两种：</p><p>1.学科专长（很少，但是有成功的案例，我不太了解，就不多说了，接下来默认说的都是学业优秀类）。</p><p>2.学业优秀类，即靠成绩。这里比较重要的是，你的绩点排名占比（排名/总人数），必须在学院转出的比例之内，一般有5%，10%，20%，越高越容易转出，5%的那几个学院转出就很难。如果是大类招生，转专业是在分流后进行的，但是你的排名按照大类专业的比例，但是转入转出的名额是按照分流后的专业进行限制的。</p><p>五、 转入工作机制（重中之重）：</p><p>这里与转出的资格筛查很像，不一样的是除了必须满足的绩点排名外，多一个高数英语的排名。高数英语成绩，共四门，这里说的是四门加起来排名，择优录取（后面我会说大概能转入的分，这里只介绍政策，方便一些不了解政策的同学阅读）。</p><p>六、 工作进度安排（重点）：这个就不用我说了吧，别误了交表的点。</p><h2><span id="5-转专业潜规则">5. 转专业潜规则</span></h2><p>潜规则？不存在的，我只是想介绍一下转专业的真实难度。</p><p>可能你们在一些学长学姐的口中得知，转专业的都是大神，专业前几才能转专业的balabala什么的，这些学长都是好心，不是吓唬你们，只不过是他们可能是大四及以上的学长，他们转专业的那会，转专业确实很难。然而现在不一样了，转专业的不一定是大神了。</p><p>经过转专业政策解读后，你们应该知道了，转专业分转出本学院，转入目标学院两步。</p><p>关于转出：一般来说，只要绩点排名符合本学院的要求，就能转出，这是一个死规定，因为学院也不想丢失人才。</p><p>关于转入：只要你能转出，基本都能转入（有但是的，耐心看）。</p><p>转入，首先满足转入学院的绩点排名（基本条件），这个应该也是死规定。但是，之后的择优录取，各学院是不一样的，例如信息学院的高数英语排名，例如软件学院的70%高数英语、30%面试，例如机械学院的：1.品德优良，无违纪记录；2.热爱转入的专业，能够在转入专业安心学习；3.成绩无不及格记录（你懂的）。也就是说，在满足转入的绩点排名要求后，学院自行考核学生择优录取。</p><p>对于信息学院计算机学院这种抢手的学院，你需要高数英语分数很高，每年的高数英语平均分不一致，因此单凭分数估算太过片面。对于计算机科学与技术，因为名额少（去年十几个，但是听说录的远远多于这个数字），因此竞争很强。自动化虽然很抢手，但是每年招的人比计算机多，所以在10%的进入的可能性都很大。大概高数英语四门课的排名都在专业前10%应该稳进（可以互补），这个没办法估，因为转专业的人数基数还是少，统计的历年成绩变动也会很大，只能说，只要满足绩点排名要求就大胆去试。大概高数英语四门加起来320左右就可以一试，基本340以上就稳了。</p><p>重点来了：填表的时候可以填俩专业，这俩专业的录取和高考不一样！不一样！不一样！举个例子，学生ABC，分数分别为350，330，310。他们分别报的志愿：A（1计算机2机械）B（1计算机2机械）C（1机械2.材料）。录取顺序是这样的：A的1、B的1、C的1，A的2、B的2、C的2。因此：第一志愿相当重要，基本就决定了你能不能转成功（第二志愿开始录取前一般名额就满了）</p><p>听完是不是很担心，一志愿进不了就完蛋了。不存在的！首先，你需要评估一下你的高数英语成绩，要是只有300多，计算机物联网这样的就别想了。如果你高数英语成绩稍微高了点，但是别的专业名额还没满，老师会跟你打电话，问你调到其他专业行不行balabala，所以，老师录入你的时候，第二志愿也实际上有用，因此，在此我建议，一二志愿填同一学院！</p><p>转入难度：计算机（包括计算机科学与技术和物联网）&gt;自动化（包括自动化和电气）≈电子信息和通信&gt;软件（往年竞争不激烈，报的人数都不够，今年肯定竞争激烈，相信我）&gt;&gt;机械材料等等。其他的例如金融啥的，我不了解，就不瞎说了。</p><p>综上：只要你绩点排名满足转入，大胆报，一般都能进，最不济就是换了个同学院的专业，比如报的计科换成通信什么的（这对冶金的来说，能脱坑都是好的）。但是如果你的高数英语实在低的不像话，就别冲太好的专业，或许比你专业稍微好点的理学院的机械材料院的也是个选择（这些专业一般报了都要）。</p><h2><span id="6-转专业后学习生活和良心学长的寄语">6. 转专业后学习生活和良心学长的寄语</span></h2><p>转完专业，你就需要补相应的课程了，不用担心学不过来，完全学的过来，能转专业的学习能力不会太差。一般来说，有的学院强制要求补某些课（例如计算机学院），有的学院补够相关课群的学分就好了（根据培养计划）（例如信息学院和软件学院），一般来说，各个学院不一样，多和学院的教学办的老师交流，多多询问就好了，最好能抱上去年转进这个专业的学长的大腿。</p><p>还要考虑的是，你选择转入后 的下一步规划，例如想保研，又转计科自动化，那么保研的难度指数型增大，因为计科大牛太多了而且保研名额也没办，然后你会发现电子信息或者软件也不错，相对竞争也少一点。这些都是需要考虑的因素。转专业的同学都不会太差，但是你需要提高自己获取信息的能力，例如你会发现教务处能找到前几年转专业的名单就知道哪些专业抢手哪些不抢手了。相应的其他信息你也能收集到，然后综合起来，选择自己想要的。</p><p>洋洋洒洒敲了四千多字，其实也没多少内容，搜集信息，然后大胆尝试，才是最重要的。如果还有想问的，可以在楼下回复，也可以私信我要QQ。</p><p>嗯，希望能在九月听到你们成功的消息。</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 东北大学史上最全转专业攻略 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
