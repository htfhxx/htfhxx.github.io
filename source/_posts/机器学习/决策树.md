---
title: 决策树
date: 2019-12-02  
author: 长腿咚咚咚
toc: true
mathjax: true
categories: machine-learning
tags:
	- 决策树
---

#  决策树

[TOC]

## 1 简介

决策树是一个分而治之的递归过程。 

- 开始，构建根节点，将所有训练数据都放在根节点。
- 然后，**选择一个最优特征**，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
- 如果子集未分类完毕，则在子集中选择一个最优特征，继续进行划分，直到所有训练数据子集都被正确分类或没有合适的特征为止。

1. 构建根节点；2. 选择最优特征并划分子集；3. 继续划分直到所有训练数据基本划分正确。

决策树学习通常包括3个步骤：**特征选择、决策树的生成和决策树的修剪。**

## 2 特征选择

**选择最优划分属性**是决策树的关键。

几种常见的划分属性的策略：ID3、C4.5、CART。

### 2.1 ID3、C4.5

* 熵：随机变量的不确定性（熵越大不确定性越大），用于度量样本的集合纯度。

$$
H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$

* 条件熵：在随机变量X的条件下，随机变量Y的不确定性

$$
H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)
$$

* 信息增益：**数据集D的经验熵$D(D)$**与**特征A下的数据集D的经验条件熵**之差

$$
g(D, A)=H(D)-H(D | A)
$$

* 信息增益的算法

  * 输入：数据集D和特征A
    输出：特征A对数据集D的信息增益$g(D, A)$

    1. 计算数据集D的经验熵H(D)： 

    $$
    H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
    $$

    2. 计算特征A对数据集D的经验条件熵H(D|A)：

    $$
    H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D |} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
    $$

    3. 计算信息增益： $g(D, A)=H(D)-H(D | A)$

* 信息增益比：

$$
g_{R}(D, A)=\frac{g(D, A)}{H(D)}
$$

* 信息增益与信息增益比的协调
  信息增益对**取值数目较多的属性**有偏好(分支多，划分后的信息增益大)
  增益比对取值数目较少的属性有所偏好，于是：
  		c4.5**先从侯选属性中找出信息增益高于平均水平的，再从中选择增益率最高的**



### 2.2 CART（Classification And Regression Tree)



* 基尼指数：表示集合 D 的不确定性
  $$
  \operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
  $$

* 给定样本的基尼指数
  $$
  \operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
  $$

* 如果样本集合 $D$ 根据特征 $A$ 是否取一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分， 那么在特征 A 的条件下， 集合 D 的基尼系数定义为：
  $$
  \operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
  $$

## 3 决策树的生成

### 3.1 ID3、C4.5决策树的生成

* 输入：训练数据集D，特征集A，信息增益（比）的阈值；
  输出：决策树T。
  1. 计算特征集中各个特征对数据集D的信息增益（比），选择信息增益较大的特征$A_g$
  2. 如果特征$A_g$的信息增益（比）小于阈值，则置为单节点的树
  3. 否则：对特征$A_g$的各个取值划分子集作为其子树
  4. 递归这一过程（数据集为空或者特征集为空的情况返回树）

​                                                                         

### 3.2 CART决策树的生成

- 输入：训练数据集D，特征集A，终止条件；
  输出：决策树T

  1. 从根节点开始，对节点计算现有特征的基尼系数。对于每一个特征，以及特征的每一种取值，根据 “是” 与 “否” 计算划分过后的基尼系数
  2. 选择基尼指数最小的**特征及其对应的取值**作为**最优特征和最优切分点**。然后根据最优特征和最优切分点，将本节点的数据集二分，生成两个子节点。
  3. 对两个字节点递归地调用上述步骤。

  **终止条件**一般是：节点中的样本个数小于阈值，样本中的基尼指数小于阈值，没有更多特征

尽可能多的划分，然后进行后剪枝

## 4 决策树的剪枝处理

防止过拟合并减少训练时间和测试时间

剪枝通过：极小化决策树整体的损失函数

* 决策树整理的损失函数：包括信息增益与模型规模
  $$
  C_{\alpha}(T)=C(T)+\alpha|T|
  $$

* 剪枝过程
  1. 计算每个节点的经验熵（基尼指数）
  2. 从叶子节点自底向上递归，回缩到对应的父节点上，计算损失函数的变化
  3. 损失变小则剪枝


## 5 三种决策树的区别

特征选择方式不同：信息增益、信息增益率、基尼指数

其中，信息增益偏向属性取值多的分支进行划分，信息增益率偏向属性取值少的分支进行划分，CAR树既可以回归也可以分类



 

## 6 面试常问

递归的终止条件是什么呢？

1. 直到每个叶子节点都**只有一种类型**时停止，（这种方式很容易过拟合） 
2. 当叶子节点的**样本个数**小于一定的阈值
3. 节点的**信息增益**（或基尼系数）小于预定阈值    **推荐**

4. 没有更多特征

 

为什么信息增益偏向属性取值多的分支？

* 当特征取值较多时， 根据此特征划分得到的子集纯度提升的更多（对比取值较少的特征）



 

## Reference：

```
《统计学习方法》
《机器学习》——西瓜书
https://github.com/NLP-LOVE/ML-NLP
https://github.com/htfhxx/NLPer-Interview
```

