---
title: 线性回归与逻辑回归
date: 2019-12-05
author: 长腿咚咚咚
toc: true
mathjax: true
categories: 机器学习
tags:
	- 线性回归与逻辑回归
---

[TOC]



# 1 线性回归

## 1.1 简介

简单来说，线性回归算法就是**找到一条直线（一元线性回归）或一个超平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。**
$$
y = w_0x_0 + \cdots  + w_px_p + b = wx+b
$$

## 1.2 如何计算

* 损失函数为：
  $$
  J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \qquad  \\
  $$

* 利用**梯度下降法**找到最小值点，也就是最小误差，最后把 w 和 b 给求出来

## 1.3 过拟合如何解决

* 使用岭回归-Ridge（加入L2正则项）
  $$
  \hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i w_i^2
  $$

* 使用Lasso回归（加入L1正则项）
  $$
  \hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i |w_i|
  $$

* 使用场景
  * 只要数据线性相关，用LinearRegression拟合的不是很好，**需要正则化**，可以考虑使用岭回归
  * 如果输入特征的维度很高，而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。
  * **L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0**，从而增强模型的泛化能力 。



# 2 逻辑回归

## 2.1 简介

logistic回归用于解决的是分类问题，**其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。**

也就是说，logistic 回归不是对所有数据点进行拟合，而是要对**数据之间的分界线**进行拟合。

* 表达式是：

$$
h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}
$$

* 其中，sigmoid函数的参数就是线性回归的结果

<img src="%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/00630Defly1g4pvk2ctatj30cw0b63yq.jpg" alt="image" style="zoom: 50%;" />



## 2.2 如何求解

- 线性回归的拟合函数本质上是对 **输出变量 y 的拟合**， 而逻辑回归的拟合函数是对 **label 为1的样本的概率的拟合**。

- 线性回归其参数计算方式为**最小二乘法**， 逻辑回归其参数更新方式为**极大似然估计**。

- 逻辑回归满足伯努利分布（0-1分布）：
  $$
  P(Y=1|x; \theta) = h_{\theta}(x) \\
  P(Y=0|x; \theta)  = 1 - h_{\theta}(x) \\
  p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}
  $$
  
- 逻辑回归的极大似然函数是：
  $$
  \begin{align}
  L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x(i);\theta) \\ 
  &= \prod_{i=1}^m  (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}\\
  \end{align}
  $$

- 对数似然函数为：
  $$
  \begin{align}
  L(\theta) &= log L(\theta ) \\
  &= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))
  \end{align}
  $$

* 求解梯度：
  $$
  \begin{align}
  \frac{\partial L(\theta)}{\partial \theta_j} &= (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} \\
  &= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} \\
  &= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j \\
  &= [y - h_{\theta} (x)]x_j \\
  \end{align}
  $$

* 优化参数：
  $$
  \begin{align}
  \theta_j &= \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} \\
  &= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}   
  \end{align}
  $$

## 2.3 如何实现多分类

- **方式1：** softmax。修改逻辑回归的损失函数，将sigmoid改为softmax函数构造模型从而解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。
- **方式2：** 根据每个类别都建立一个二分类器。本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。

若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。



## 2.4 逻辑回归特征的离散化

很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型

LR 为何要对特征进行离散化

- **非线性。** 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
- **速度快。** 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展
- **鲁棒性。** 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
- **方便交叉与特征组合**： 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。
- **稳定性：** 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。
- **简化模型：** 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。



# Reference

```
《统计学习方法》
《机器学习》——西瓜书
https://github.com/NLP-LOVE/ML-NLP
https://github.com/htfhxx/NLPer-Interview
```









