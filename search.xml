<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>个人生产力工具推荐</title>
      <link href="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/"/>
      <url>/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/</url>
      
        <content type="html"><![CDATA[<p>经常会搜集一些常用的工具，在工作学习的时候会带来一些便利，也经常安利给周围的朋友。趁着清明假期，整理一下常用的几个，不涉及具体的使用方法和破解方法，只介绍应用的场景和实用的功能，希望能给大家带来一些帮助吧。</p><p>目录如下，防止耽误大家时间：</p><p>[TOC]</p><h3><span id="1-美化">1. 美化</span></h3><p>在工作环境的舒适方面，我从来都是不遗余力的，首先尽可能有一个干净的桌子，放上电脑鼠标等东西，再加上个机械键盘和降噪耳机，就可以舒舒服服的开始工作了。</p><p>比如：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586166830210.png" alt="1586166830210" style="zoom: 33%;"></p><h4><span id="11-让人极其赏心悦目的壁纸软件wallpaper-engine">1.1 让人极其赏心悦目的壁纸软件——wallpaper engine</span></h4><p>这是个壁纸软件，steam上18块钱，里面的创意工坊里有大量的动态静态壁纸，颜值颇高，非常推荐。</p><p>集显占用不会太高，只是会占用一丢丢内存，轻薄笔记本完全可以带的动。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586166100829.png" alt="1586166100829" style="zoom: 33%;"></p><h4><span id="12-比startisback还好用的任务栏透明软件translucenttb">1.2 比StartIsBack还好用的任务栏透明软件——translucentTB</span></h4><p>一个让任务栏透明的工具，可以做到如下的效果：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586166264896.png" alt="1586166264896" style="zoom: 33%;"></p><p>还有个软件叫：StartIsBack。也可以做到如上效果，还可以让windows弹窗更换为不同风格，例如win7变win10。也很好用，但是破解较麻烦，而translucentTB在windows搜索栏直接搜索，跳转到windows的应用商城就可以下载，功能简单，无需破解，因此更为推荐。</p><h4><span id="13-各种电脑管家功能里唯一有用的硬件监控软件trafficmonitor">1.3 各种电脑管家功能里唯一有用的硬件监控软件——TrafficMonitor</span></h4><p>看到我的任务栏那里显示了 网速、cpu占用、内存占用的地方吗，那也是一个工具，比各大电脑管家内存占用小很多（当然我也没测试过），TrafficMonitor就可以做到这个，还可以通过设置进行不同的显示。</p><h4><span id="14-任务栏打上美滋滋的生活~">1.4 任务栏打上“美滋滋的生活~”？</span></h4><p>可能任务栏上的“美滋滋的生活~”大家也很感兴趣。</p><p>有两种方式在任务栏上添加自定义文字，一是修改时间那边的格式来添加，可以百度搜索“windows10 任务栏添加自定义文字”类似的词条，可以查到具体操作步骤。</p><p>还有一种方法是我这种效果，1、桌面新建文件夹，名字为你想显示 在任务栏的文字，如“美滋滋的生活~”；2、右键任务栏，工具栏，新建工具栏，选择你刚建的文件夹；3、删除刚建的文件夹。ok了。</p><h3><span id="2-日常使用工具">2. 日常使用工具</span></h3><h4><span id="21-简洁好用的日程软件滴答清单">2.1 简洁好用的日程软件——滴答清单</span></h4><p>一个日程提醒软件，最主要的是全平台可用且简洁好用，windows、android、ios。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586167881662.png" alt="1586167881662" style="zoom: 33%;"></p><h4><span id="22-不用说大家都懂的笔记软件有道云笔记">2.2 不用说大家都懂的笔记软件——有道云笔记</span></h4><p>日常的记录，大多通过有道云笔记来做的，也是全平台可用，很方便。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168034210.png" alt="1586168034210" style="zoom: 33%;"></p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168089603.png" alt="1586168089603" style="zoom: 33%;"></p><h4><span id="23-随手一记的便利贴sticky-notes">2.3 随手一记的便利贴——Sticky Notes</span></h4><p>电脑前的你，想随手记点东西，只需要在任务栏一点，就可以调出来这个小工具。他可以悬浮在桌面上，就像是书籍上的便利贴一样的东西。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168235990.png" alt="1586168235990"></p><h3><span id="3-代码-写作工具">3. 代码、写作工具</span></h3><h4><span id="31-最好用的markdown工具typora">3.1 最好用的Markdown工具——typora</span></h4><p>markdown真的是个好东西，尤其是有typora这种软件的加持，更是好用的不行。</p><p>它可以设置一番变成本地文本文档、word等工具的非常好的替代品：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168678508.png" alt="1586168678508" style="zoom:50%;"></p><p>在涉及到公式、各级标题的时候，格外的好用：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586168731554.png" alt="1586168731554" style="zoom: 50%;"></p><p>markdown的编辑内容可以很好的移植到word、pdf、个人博客、微信公众号、知乎等地方，除了图片的存储等极个别几个点之外都很方便。具体的功能和使用就不详细介绍了。</p><h4><span id="32-不仅仅是代码或许可以用来备份同步文档的github-desktop">3.2 不仅仅是代码或许可以用来备份同步文档的Github Desktop</span></h4><p>github是程序员必备的工具之一，但是自从其开放了个人用户private权限后，完全可以将其当成云盘使用，也可以轻轻松松对文档尽心版本的更迭。其PC端也很简易好用，比命令行方式好入门很多，可以尝试其当做备份和文档更迭的工具，比如毕业论文、精心制作的ppt等等。</p><p>我的博客文件都是用github备份的23333</p><h4><span id="33-史上最好用的pdf软件adobe-acrobat-pro-dc">3.3 史上最好用的pdf软件——Adobe Acrobat pro DC</span></h4><p>这是目前我用过的最好用的pdf软件，秒杀国内一众 福昕阅读器类似的软件，只是需要破解or付费。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169010616.png" alt="1586169010616"></p><p>福昕阅读器上的收费项目，比如pdf页面的无水印拼接，页面增删等，这个软件都可以做到：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169083740.png" alt="1586169083740" style="zoom:67%;"></p><h4><span id="34-还在手打latex截图转公式的好帮手mathpix-snipping-tool">3.4  还在手打latex？截图转公式的好帮手Mathpix Snipping Tool</span></h4><p>可以将带有公式的图片（一般是去截图）转变为markdown or latex 的代码，识别率很高。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169235595.png" alt="1586169235595"></p><p>缺点就是每月只有50次免费使用的机会，使用频率高的用户需要付费，当然本没钱的学（白）生（嫖）党是不会告诉你多注册几个账号是完全可行且方便的。</p><h4><span id="35-打开文档找这仨notepad-editplus-atom">3.5 打开文档找这仨：Notepad++、Editplus、Atom</span></h4><p>本地文本文档打开很慢？现在谁还会用本地文本文档呢，一般的文档可以使用notepad++，完美打开各种文本文件，还可以标识代码：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169472038.png" alt="1586169472038"></p><p>如果是打开数据文件呢，动辄好几G的那种？使用editplus！</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169521998.png" alt="1586169521998"></p><p>如果是更大的呢，或者是更小众的文件格式呢？宇宙编辑器atom了解一下！</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586169588587.png" alt="1586169588587"></p><h3><span id="4-论文工具">4. 论文工具</span></h3><p>这块关注一些论文管理和论文阅读的工具</p><h4><span id="41-界面简洁笔记好看的谷歌云-kami">4.1 界面简洁笔记好看的——谷歌云 + kami</span></h4><p>之前一段时间我使用谷歌云备份文件，kami来阅读，界面简洁赏心悦目。但是需要科学上网。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170038424.png" alt="1586170038424"></p><h4><span id="42-论文的管理同步还是最重要zotero坚果云papership">4.2 论文的管理同步还是最重要——Zotero+坚果云+Papership</span></h4><p>后来是使用zotero来进行论文的整理，在chrome中加载好了的论文可以一键加入到zotero，论文的本体文件备份到坚果云上，在ipados端使用papership来进行阅读。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170143504.png" alt="1586170143504"></p><p>ipad上是这样的，也可以保存到ipad的一些编辑软件比如notablity简单的做一些笔记：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170265638.png" alt="1586170265638"></p><p>具体的使用教程，我这里收藏的，不保证一直存在：<a href="https://zhuanlan.zhihu.com/p/28325366" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/28325366</a> （侵删）</p><h4><span id="43-神一样的论文翻译软件copytranslator">4.3 神一样的论文翻译软件——Copytranslator</span></h4><p>可能阅读文章的各位有不少是和我一样的英语菜鸡，时不时需要大段翻译英文论文，google翻译是真的好用，然鹅论文里每行后面都有一个回车就很蠢。</p><p>我在刚开始是粘贴到word文档，然后用空字符全部替换掉回车，再粘贴到google翻译。现在想想真是好难。</p><p>copytranslator，可以自动去掉空格，可以自动粘贴，甚至可以自动复制，，可以自己选翻译的引擎。。</p><p>反正翻译论文小能手就是它了！~</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586172273280.png" alt="1586172273280"></p><h3><span id="5-chrome插件">5. chrome插件</span></h3><h4><span id="51-忍不住下载但是很少使用的webtime-tracker">5.1 忍不住下载但是很少使用的——Webtime Tracker</span></h4><p>放着玩，可以看你浏览器在哪个网站花的时间多。</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170505543.png" alt="1586170505543"></p><h4><span id="52-github小帮手sourcegraph">5.2 Github小帮手——Sourcegraph</span></h4><p>github在线阅读代码插件，比在原github页面跳转快的多。</p><p>例如打开github的一个代码库</p><p>你想仔细的阅读其中的几个代码文件，甚至想比较着来看，你就可以点击这个插件，然后出现一个这个：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170690302.png" alt="1586170690302"></p><p>点击后就跳转到插件所在的网站：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586170739733.png" alt="1586170739733"></p><p>我相信看到这里你就会去添加插件了。</p><h4><span id="53-为拖延症晚期拔掉氧气管onetab">5.3 为拖延症晚期拔掉氧气管——OneTab</span></h4><p>因为你手头上的工作所以打开着很多网站，但是要先去干别的工作开别的网页，或者需要临时关闭电脑，或者要关闭一下浏览器去开别的？那这个插件很适合你</p><p>现在一堆的baidu.com等待你去看：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171115733.png" alt="1586171115733"></p><p>点击插件会全部关闭，等到下次打开的时候：</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171140126.png" alt="1586171140126"></p><p>是不是感觉进一步加剧了你的拖延症哈哈哈哈</p><h4><span id="54-不要登录真的相信我版页面双语对照翻译插件彩云小译">5.4 不要登录真的相信我版页面双语对照翻译插件——彩云小译</span></h4><p>chorme的页面翻译不知道你使用过没有呢？将页面内的文字全部翻译一下翻译成别的语言，之前的语言就没了。</p><p>而彩云小译是将页面保留，在下面或者旁边添加翻译，是不是感觉很贴心？</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171286930.png" alt="1586171286930"></p><p>补充一句，不要登录，不然使用次数会变的有限 /真哭笑不得</p><h4><span id="55-不知道好不好用反正下载就完事了广告拦截器adguard">5.5 不知道好不好用反正下载就完事了广告拦截器——ADGuard</span></h4><p>浏览器广告拦截器，不知道具体拦截了多少，不过周围人用的挺多的</p><p><img src="/2020/04/06/yu-yan-gong-ju-ji-zhu-deng-wen-dang/ge-ren-sheng-chan-li-gong-ju-tui-jian/1586171344275.png" alt="1586171344275"></p><p>本来还想推荐一些常用网站的，但是瞅了一眼可推荐的还挺多的，下次再说吧（拖延症又犯了吗）</p><p>大家有什么好用的软件，欢迎安利，可以补充到评论区~ </p><p>也非常感谢曾经安利过我工具的小伙伴，如果觉得有用，可以点个赞同~</p>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 个人生产力工具推荐 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督学习</title>
      <link href="/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/"/>
      <url>/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-聚类">1 聚类</span></h2><ul><li>聚类定义：根据数据中样本与样本之间的距离或相似度，将样本划分为若干组／类／簇</li><li>划分的原则：类内样本距离小、类间样本距离大</li><li>聚类类型<ol><li>基于划分的聚类（无嵌套）</li><li>层次聚类（嵌套）</li></ol></li><li><p>聚类分析的“三要素”</p><ol><li>如何定义样本点之间的“远近”：使用相似性/距离函数</li><li>如何评价聚类出来的簇的质量</li><li>如何获得聚类的簇</li></ol></li><li><p>距离度量函数</p><ol><li><p>闵可夫斯基（Minkowski）距离：p为1时为曼哈顿距离，p为2时为欧氏距离</p><script type="math/tex; mode=display">\operatorname{dist}\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\left(\sum_{d=1}^{D}\left|x_{i d}-x_{j d}\right|^{p}\right)^{1 / p}</script></li><li><p>余弦相似度(cosine similarity)</p><script type="math/tex; mode=display">s\left(\mathbf{x}_{i}, \mathbf{x}_{j}\right)=\frac{\sum_{d=1}^{D} x_{i d} x_{j d}}{\sqrt{\sum_{d=1}^{D} x_{i d}^{2}} \sqrt{\sum_{d=1}^{D} x_{j d}^{2}}}=\frac{\mathbf{x}_{i}^{T} \mathbf{x}_{j}}{\left\|\mathbf{x}_{i}\right\|\left\|\mathbf{x}_{j}\right\|}</script></li><li><p>相关系数（Pearson系数）</p></li><li>杰卡德相似系数(Jaccard)</li></ol></li><li><p>聚类性能评价指标</p><ol><li>外部评价法(external criterion) ：聚类结果与参考结果有多相近</li><li>内部评价法(internal criterion)：聚类的本质特点（无参考结果）。簇内相似度越高，聚类质量越好；簇间相似度越低，聚类质量越好。</li></ol></li><li><p>经典聚类算法</p><ol><li>K均值聚类（K-means）</li><li>高斯混合模型和最大期望算法（Gaussian Mixture Models and Expectation- Maximization Algorithm）</li><li>层次聚类</li><li>基于密度聚类</li></ol></li></ul><h2><span id="2-k均值聚类k-means">2 K均值聚类（K-means）</span></h2><ul><li>算法流程</li></ul><p><img src="/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/1578206576314.png" alt="1578206576314" style="zoom:50%;"></p><ul><li>运行示意</li></ul><p><img src="/2020/01/05/ji-qi-xue-xi/wu-jian-du-xue-xi/1578206607286.png" alt="1578206607286" style="zoom:50%;"></p><ul><li><p>一些细节</p><ul><li><p>如何划分节点：使用欧式距离进行距离度量，每个节点都划分到最近的那个质心的簇中</p></li><li><p>优化目标（损失函数）</p><script type="math/tex; mode=display">\quad J=\sum_{i=1}^{N} \sum_{k=1}^{K} r_{i k}\left\|\mathbf{x}_{i}-\boldsymbol{\mu}_{k}\right\|^{2}  \\ 从属度:r_{i,k} ∈ {0,1}  \\ u_k是类中心点</script></li></ul></li><li><p>K-Means的局限性<br>K-Means假定簇为球形且每个簇的概率相等。</p><p>当簇具有不同的尺寸、密度、簇非球形时，结果不理想；<br>离群点的影响</p></li></ul><h2><span id="3-em算法与高斯混合模型">3 EM算法与高斯混合模型</span></h2><h3><span id="31-em算法">3.1 EM算法</span></h3><ul><li><p>EM算法流程</p><ul><li>输入：观测变量数据Y，隐变量数据Z，联合分布$P(Y,Z | \theta )$，条件分布 $P(Y| Z , \theta )$；</li><li>输出：模型参数 $\theta$</li></ul><ol><li><p>选择模型参数的初始值$\theta _{0}$，开始迭代</p></li><li><p>E步，计算在模型参数固定为$\theta _{0}$的情况下的<strong>观测数据的概率</strong>：</p><script type="math/tex; mode=display">\begin{aligned}Q\left(\theta, \theta^{(i)}\right) &=E_{Z}\left[\log P(Y, Z | \theta) | Y, \theta^{(i)}\right] \\&=\sum_{Z} \log P(Y, Z | \theta) P\left(Z | Y, \theta^{(i)}\right)\end{aligned}</script></li><li><p>M步，当前观测数据概率下，通过极大似然估计来估计参数</p><script type="math/tex; mode=display">\theta^{(i+1)}=\arg \max _{\theta} Q\left(\theta, \theta^{(i)}\right)</script></li><li><p>重复2、3，直到收敛</p></li></ol></li></ul><h3><span id="32-高斯混合模型">3.2 高斯混合模型</span></h3><h2><span id="4-层次聚类">4 层次聚类</span></h2><h2><span id="5-基于密度的聚类">5 基于密度的聚类</span></h2><ul><li>DBSCAN( density-based spatial clustering ofapplication with noise）</li></ul><h2><span id="reference">Reference</span></h2><pre><code>《模式识别与机器学习》  ——from UCAS</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 无监督学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>激活函数</title>
      <link href="/2019/12/24/shen-du-xue-xi/ji-huo-han-shu/"/>
      <url>/2019/12/24/shen-du-xue-xi/ji-huo-han-shu/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h3><span id="1-为什么需要激活函数">1 为什么需要激活函数</span></h3><p>如果不使用非线性激活函数，那么每一层输出都是上层输入的<strong>线性组合</strong></p><p>此时无论网络有多少层，其整体也将是线性的</p><h3><span id="2-常用的激活函数">2 常用的激活函数</span></h3><h4><span id="21-sigmoid">2.1 sigmoid</span></h4><p><img src="/2019/12/24/shen-du-xue-xi/ji-huo-han-shu/00630Defly1g2x34jlnrrj306g0590st.jpg" alt="image" style="zoom: 67%;"></p><script type="math/tex; mode=display">\begin{aligned} a &=g(x)=\frac{1}{1+e^{-x}} \\ g(x)^{\prime} &=\frac{d}{d x} g(z)=\alpha(1-\alpha) \end{aligned}</script><h4><span id="22-tanh双曲正切">2.2 tanh(双曲正切)</span></h4><p><img src="/2019/12/24/shen-du-xue-xi/ji-huo-han-shu/00630Defly1g2x355gdkij306k04q0sr.jpg" alt="image" style="zoom:67%;"></p><p>相当于sigmoid向下平移和伸缩变形</p><script type="math/tex; mode=display">\begin{array}{l}{a=g(x)=\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}} \\ {g(x)^{\prime}=\frac{d}{d z} g(x)=1-(\tanh (x))^{2}}\end{array}</script><h4><span id="23-relu">2.3 Relu</span></h4><p><img src="/2019/12/24/shen-du-xue-xi/ji-huo-han-shu/00630Defly1g2x3f01a0gj306d04xmx7.jpg" alt="00630Defly1g2x3f01a0gj306d04xmx7"></p><ul><li>一定程度上缓解了梯度问题：其导数始终为一个常数</li><li><strong>计算速度非常快：</strong> 求导不涉及浮点运算，所以速度更快</li><li><strong>减缓过拟合：</strong> <code>ReLU</code> 在负半区的输出为 0，不会产生梯度/不会被训练，造成了网络的稀疏性——<strong>稀疏激活</strong>， 这有助于减少参数的相互依赖，缓解过拟合问题的发生</li></ul><h3><span id="24-softmax">2.4 softmax</span></h3><p>softmax将所有的输入归一化，多用于多分类问题</p><script type="math/tex; mode=display">P(i) = \frac{e^{a_i}}{\sum_{k=1}^T e^{a_k}} \in [0,1]</script><p>softmax的损失：</p><script type="math/tex; mode=display">L = - \sum_{j=1}^T y_j \, log \, s_j</script><h3><span id="3-如何选择激活函数">3 如何选择激活函数</span></h3><ul><li>经验：如果是二分类0-1的问题：sigmoid，其余选Relu</li><li>一般隐层采用Relu， 有时也要试试 tanh</li></ul><h3><span id="4-常见问题">4 常见问题</span></h3><p>为何 tanh 比 sigmoid 收敛快？导数值域大</p><script type="math/tex; mode=display">tanh^{'}(x)=1-tanh(x)^{2}\in (0,1) \\sigmoid^{'}(x)=sigmoid(x)*(1-sigmoid(x))\in (0,\frac{1}{4}]</script>]]></content>
      
      
      <categories>
          
          <category> deep-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 激活函数 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>nlp中的预训练</title>
      <link href="/2019/12/14/zi-ran-yu-yan-chu-li-ji-chu/nlp-zhong-de-yu-xun-lian/"/>
      <url>/2019/12/14/zi-ran-yu-yan-chu-li-ji-chu/nlp-zhong-de-yu-xun-lian/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h3><span id="1-图像领域的预训练">1 图像领域的预训练</span></h3><ul><li><p>图像领域（CNN系列模型）预训练的步骤：</p><ol><li>先用某个训练集合、在任务A上进行预训练，并保存参数；</li><li>采取相同的网络结构处理任务B，并在浅层结构中加载A任务学习好的参数，其它高层结构参数仍然随机初始化</li><li>之后我们用B任务的训练数据来训练网络，有两种做法：<ul><li>浅层加载的参数在训练B不更新，称为Frozen;</li><li>浅层加载的参数在训练B更新，称为Fine-Tuning</li></ul></li></ol></li><li><p>为什么有效：</p><ul><li>不同层级的CNN学到不同层级的特征，浅层特征的通用性更广，高层特征与任务更相关</li></ul></li></ul><h3><span id="2-word2vec-amp-glove">2 word2vec &amp; glove</span></h3><p>NLP领域的预训练最开始依靠的是word embedding，其中最知名的是word2vec，后来的glove在word2vec上进行改进。</p><p>word2vec就是训练了一个语言模型，其副产物（得到的参数矩阵）可以作为词的分布式表示</p><p>glove的改进：损失函数不同，引入了词语的共现信息（构建了共现矩阵）</p><ul><li>word embedding的缺陷：多义词</li></ul><h3><span id="3-elmo解决多义词问题">3 ELMO：解决多义词问题</span></h3><p>​        ELMO是“Embedding from Language Models”的简称</p><p>​        论文：《Deep contextualized word representation》</p><p>​        原理本质：<strong>先预训练一个word embedding</strong>；下游任务的实际使用时，<strong>使用上下文单词的语义去调整</strong>（参数是不可训练的）</p><p>​        结构：双层双向的LSTM进行编码，语言模型作为训练任务</p><p>​        缺点：LSTM特征抽取能力弱，双向特征融合的方式——拼接的方式融合特征能力较弱</p><h3><span id="6-gpt">6 GPT：</span></h3><p>​        GPT是“Generative Pre-Training”的简称</p><p>​        两个大改进：特征抽取器使用了<strong>transformer</strong>；用于下游任务时，GPT<strong>参数可训练</strong></p><p>​        只使用上文预测下一个单词，抛开了下文（只是单向）</p><h3><span id="7-bert">7 Bert</span></h3><p>​        GPT中的单向改为双向</p><p>​        NLP四大任务：序列标注、文本分类、文本关系推断（两个输入）、文本生成</p><p>​        普适性强：适用于以上任务</p><p>BERT的关键：<strong>Transformer</strong>作为特征抽取器，<strong>双向</strong>语言模型，预训练是一个<strong>多任务的训练</strong>，更巨大的语料</p><p>如何改造双向语言模型：</p><ol><li>随机挖掉15%单词，预测被挖掉的单词。被挖掉的单词，80%被替换为MASK，10%替换为另一个单词，10%原地不动</li><li>句子关系预测，真正相连的句子和随机组合的两个句子</li></ol><p>BERT的输入：三个embedding：位置、单词、句子</p><h3><span id="references">References</span></h3><p><a href="https://zhuanlan.zhihu.com/p/49271699" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/49271699</a></p>]]></content>
      
      
      <categories>
          
          <category> nlp基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nlp中的预训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Reasoning about Entailment with Neural Attention》简析</title>
      <link href="/2019/12/13/nlp-research/reasoning-about-entailment-with-neural-attention-jian-xi/"/>
      <url>/2019/12/13/nlp-research/reasoning-about-entailment-with-neural-attention-jian-xi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-介绍">1 介绍</span></h2><p>ESIM是一个超强的文本蕴含任务模型。</p><p>文本蕴含任务就是，给定两个句子，第一个句子是前提——premise，第二个句子是假设——hypothesis，判断两个句子构成的关系——蕴含、不相干、矛盾。本质上是一个句子级的文本分类问题</p><h2><span id="2-网络结构">2 网络结构</span></h2><h3><span id="21-整体结构">2.1 整体结构</span></h3><p>​        模型的输入是premise和hypothesis。</p><p>​        论文使用两个LSTM（A）对两个句子进行编码，其中第二个LSTM使用的是第一个LSTM的末状态。</p><p>​        然后论文使用了两个Attention对编码后的信息进行处理，一种是Attention（B），用于图中$h_9$对$c_1，c_2,..,c_5$的注意力；一种是Attenion（C），用于图中$h_7,h_8,h_9$分别对$c_1，c_2,..,c_5$的注意力。</p><p>​        最后将以上信息进行整合，softmax分类，交叉熵损失。</p><p><img src="/2019/12/13/nlp-research/reasoning-about-entailment-with-neural-attention-jian-xi/1576248174645.png" alt="1576248174645"></p><h3><span id="22-lstm">2.2 LSTM</span></h3><h3><span id="23-attention">2.3 Attention</span></h3><h3><span id="24-word-by-word-attention">2.4 word-by-word Attention</span></h3>]]></content>
      
      
      <categories>
          
          <category> nlp_Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Reasoning about Entailment with Neural Attention》简析 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF（转载）</title>
      <link href="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/tf-idf-zhuan/"/>
      <url>/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/tf-idf-zhuan/</url>
      
        <content type="html"><![CDATA[<hr><p>[TOC]</p><p>TF-IDF是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。</p><h2><span id="1-定义">1 定义</span></h2><p>一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章.</p><ul><li><p><strong>TF:</strong> Term Frequency, 表示词频。 一个给定的词在该文章中出现的次数。</p><script type="math/tex; mode=display">TF = \frac{\text{某个词在文章中的出现次数}}{\text{文章的总词数}}  \\</script></li><li><p><strong>IDF:</strong> Inverse Document Frequency, 表示逆文档频率。如果包含词条 t 的文档越少, IDF越大，则说明词条具有很好的类别区分能力。</p></li></ul><script type="math/tex; mode=display">IDF = log(\frac{语料库的文档总数}{包含该词的文档数+1})  \\</script><ul><li><strong>TF-IDF：</strong>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语<script type="math/tex; mode=display">\text{TF-IDF} = TF \times IDF</script></li></ul><h2><span id="2-例子">2 例子</span></h2><p>假设现在有一篇文章， 文章中包含 10000 个词组， 其中，”贵州” 出现100次，”的” 出现500次，那么我们可以计算得到这几个词的 TF(词频) 值：</p><script type="math/tex; mode=display">TF(贵州) = 100 / 10000 = 0.01 \\TF(的) = 500 / 10000 = 0.05</script><p>现在语料库中有 1000 篇文章， 其中，包含 “贵州” 的有 99 篇， 包含 “的” 的有 899 篇， 则它们的 IDF 值计算为：</p><script type="math/tex; mode=display">IDF(贵州) = log(1000 / (99+1)) = 1.000 \\IDF(的) = log(1000 / (899+1)) = 0.046</script><h2><span id="3-优缺点">3 优缺点</span></h2><ul><li>优点简单快速，而且容易理解。</li><li>缺点是有时候用词频来衡量文章中的一个词的重要性不够全面，有时候重要的词出现的可能不够多，而且这种计算无法体现位置信息，无法体现词在上下文的重要性。</li></ul><p>from</p><pre><code>https://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> nlp基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TF-IDF（转载） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>seq2seq（转载）</title>
      <link href="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/seq2seq-zhuan/"/>
      <url>/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/seq2seq-zhuan/</url>
      
        <content type="html"><![CDATA[<h2><span id="目录">目录</span></h2><ul><li><a href="#1-什么是seq2seq">1. 什么是seq2seq</a></li><li><a href="#2-编码器">2. 编码器</a></li><li><a href="#3-解码器">3. 解码器</a></li><li><a href="#4-训练模型">4. 训练模型</a></li><li><a href="#5-seq2seq模型预测">5. seq2seq模型预测</a><ul><li><a href="#51-贪婪搜索">5.1 贪婪搜索</a></li><li><a href="#52-穷举搜索">5.2 穷举搜索</a></li><li><a href="#53-束搜索">5.3 束搜索</a></li></ul></li><li><a href="#6-bleu得分">6. Bleu得分</a></li><li><a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.5%20seq2seq/seq2seq.ipynb" target="_blank" rel="noopener">7. 代码实现</a></li><li><a href="#8-参考文献">8. 参考文献</a></li></ul><h2><span id="1-什么是seq2seq">1. 什么是seq2seq</span></h2><p>在⾃然语⾔处理的很多应⽤中，输⼊和输出都可以是不定⻓序列。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，例如：</p><p>英语输⼊：“They”、“are”、“watching”、“.” </p><p>法语输出：“Ils”、“regardent”、“.” </p><p>当输⼊和输出都是不定⻓序列时，我们可以使⽤编码器—解码器（encoder-decoder）或者seq2seq模型。<strong>序列到序列模型，简称seq2seq模型。这两个模型本质上都⽤到了两个循环神经⽹络，分别叫做编码器和解码器。编码器⽤来分析输⼊序列，解码器⽤来⽣成输出序列。两 个循环神经网络是共同训练的。</strong></p><p>下图描述了使⽤编码器—解码器将上述英语句⼦翻译成法语句⼦的⼀种⽅法。在训练数据集中，我们可以在每个句⼦后附上特殊符号“\<eos>”（end of sequence）以表⽰序列的终⽌。编码器每个时间步的输⼊依次为英语句⼦中的单词、标点和特殊符号“\<eos>”。下图中使⽤了编码器在 最终时间步的隐藏状态作为输⼊句⼦的表征或编码信息。解码器在各个时间步中使⽤输⼊句⼦的 编码信息和上个时间步的输出以及隐藏状态作为输⼊。我们希望解码器在各个时间步能正确依次 输出翻译后的法语单词、标点和特殊符号“\<eos>”。需要注意的是，解码器在最初时间步的输⼊ ⽤到了⼀个表⽰序列开始的特殊符号“<bos>”（beginning of sequence）。 </bos></eos></eos></eos></p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_11-10-4.png" alt></p><h2><span id="2-编码器">2. 编码器</span></h2><p>编码器的作⽤是把⼀个不定⻓的输⼊序列变换成⼀个定⻓的背景变量 c，并在该背景变量中编码输⼊序列信息。常⽤的编码器是循环神经⽹络。</p><p>让我们考虑批量⼤小为1的时序数据样本。假设输⼊序列是 x1, . . . , xT，例如 xi 是输⼊句⼦中的第 i 个词。在时间步 t，循环神经⽹络将输⼊ xt 的特征向量 xt 和上个时间步的隐藏状态 <img src="https://latex.codecogs.com/gif.latex?h_{t-1}" alt>变换为当前时间步的隐藏状态ht。我们可以⽤函数 f 表达循环神经⽹络隐藏层的变换：</p><p><img src="https://latex.codecogs.com/gif.latex?h_t=f(x_t,h_{t-1}" alt>)</p><p>接下来，编码器通过⾃定义函数 q 将各个时间步的隐藏状态变换为背景变量：</p><p><img src="https://latex.codecogs.com/gif.latex?c=q(h_1,...,h_T" alt>)</p><p>例如，当选择 <em>q</em>(<strong><em>h</em></strong>1<em>, . . . ,</em> <strong><em>h*</em></strong>T<em> ) = **</em>h<strong><em>*T</em> 时，背景变量是输⼊序列最终时间步的隐藏状态*</strong>h<em>**</em>T*。</p><p>以上描述的编码器是⼀个单向的循环神经⽹络，每个时间步的隐藏状态只取决于该时间步及之前的输⼊⼦序列。我们也可以使⽤双向循环神经⽹络构造编码器。在这种情况下，编码器每个时间步的隐藏状态同时取决于该时间步之前和之后的⼦序列（包括当前时间步的输⼊），并编码了整个序列的信息。</p><h2><span id="3-解码器">3. 解码器</span></h2><p>刚刚已经介绍，编码器输出的背景变量 c 编码了整个输⼊序列 x1, . . . , xT 的信息。给定训练样本中的输出序列 y1, y2, . . . , yT′ ，对每个时间步 t′（符号与输⼊序列或编码器的时间步 t 有区别），解码器输出 yt′ 的条件概率将基于之前的输出序列<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-39-17.png" alt>和背景变量 c，即：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-45-12.png" alt></p><p>为此，我们可以使⽤另⼀个循环神经⽹络作为解码器。在输出序列的时间步 t′，解码器将上⼀时间步的输出 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-46-6.png" alt>以及背景变量 c 作为输⼊，并将它们与上⼀时间步的隐藏状态  <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-46-54.png" alt>变换为当前时间步的隐藏状态st′。因此，我们可以⽤函数 g 表达解码器隐藏层的变换：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-47-36.png" alt></p><p>有了解码器的隐藏状态后，我们可以使⽤⾃定义的输出层和softmax运算来计算 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-49-45.png" alt>，例如，基于当XQ前时间步的解码器隐藏状态 st′、上⼀时间步的输出<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-46-54.png" alt>以及背景变量 c 来计算当前时间步输出 yt′ 的概率分布。</p><h2><span id="4-训练模型">4. 训练模型</span></h2><p>根据最⼤似然估计，我们可以最⼤化输出序列基于输⼊序列的条件概率：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-51-22.png" alt></p><p>并得到该输出序列的损失：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-52-25.png" alt></p><p>在模型训练中，所有输出序列损失的均值通常作为需要最小化的损失函数。在上图所描述的模型预测中，我们需要将解码器在上⼀个时间步的输出作为当前时间步的输⼊。与此不同，在训练中我们也可以将标签序列（训练集的真实输出序列）在上⼀个时间步的标签作为解码器在当前时间步的输⼊。这叫作强制教学（teacher forcing）。</p><h2><span id="5-seq2seq模型预测">5. seq2seq模型预测</span></h2><p>以上介绍了如何训练输⼊和输出均为不定⻓序列的编码器—解码器。本节我们介绍如何使⽤编码器—解码器来预测不定⻓的序列。</p><p>在准备训练数据集时，我们通常会在样本的输⼊序列和输出序列后面分别附上⼀个特殊符号“\<eos>”表⽰序列的终⽌。我们在接下来的讨论中也将沿⽤上⼀节的全部数学符号。为了便于讨论，假设解码器的输出是⼀段⽂本序列。设输出⽂本词典Y（包含特殊符号“\<eos>”）的⼤小为|Y|，输出序列的最⼤⻓度为T′。所有可能的输出序列⼀共有 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-53-47.png" alt>种。这些输出序列中所有特殊符号“\<eos>”后⾯的⼦序列将被舍弃。</eos></eos></eos></p><h3><span id="51-贪婪搜索">5.1 贪婪搜索</span></h3><p>贪婪搜索（greedy search）。对于输出序列任⼀时间步t′，我们从|Y|个词中搜索出条件概率最⼤的词：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-55-26.png" alt></p><p>作为输出。⼀旦搜索出“\<eos>”符号，或者输出序列⻓度已经达到了最⼤⻓度T′，便完成输出。我们在描述解码器时提到，基于输⼊序列⽣成输出序列的条件概率是<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-56-6.png" alt>。我们将该条件概率最⼤的输出序列称为最优输出序列。而贪婪搜索的主要问题是不能保证得到最优输出序列。</eos></p><p>下⾯来看⼀个例⼦。假设输出词典⾥⾯有“A”“B”“C”和“\<eos>”这4个词。下图中每个时间步<br>下的4个数字分别代表了该时间步⽣成“A”“B”“C”和“\<eos>”这4个词的条件概率。在每个时间步，贪婪搜索选取条件概率最⼤的词。因此，图10.9中将⽣成输出序列“A”“B”“C”“\<eos>”。该输出序列的条件概率是0.5 × 0.4 × 0.4 × 0.6 = 0.048。</eos></eos></eos></p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_13-44-5.png" alt></p><p>接下来，观察下面演⽰的例⼦。与上图中不同，在时间步2中选取了条件概率第⼆⼤的词“C”<br>。由于时间步3所基于的时间步1和2的输出⼦序列由上图中的“A”“B”变为了下图中的“A”“C”，下图中时间步3⽣成各个词的条件概率发⽣了变化。我们选取条件概率最⼤的词“B”。此时时间步4所基于的前3个时间步的输出⼦序列为“A”“C”“B”，与上图中的“A”“B”“C”不同。因此，下图中时间步4⽣成各个词的条件概率也与上图中的不同。我们发现，此时的输出序列“A”“C”“B”“\<eos>”的条件概率是0.5 × 0.3 × 0.6 × 0.6 = 0.054，⼤于贪婪搜索得到的输出序列的条件概率。因此，贪婪搜索得到的输出序列“A”“B”“C”“\<eos>”并⾮最优输出序列。</eos></eos></p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_13-47-56.png" alt></p><h3><span id="52-穷举搜索">5.2 穷举搜索</span></h3><p>如果⽬标是得到最优输出序列，我们可以考虑穷举搜索（exhaustive search）：穷举所有可能的输出序列，输出条件概率最⼤的序列。</p><p>虽然穷举搜索可以得到最优输出序列，但它的计算开销<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-59-0.png" alt>很容易过⼤。例如，当|Y| =10000且T′ = 10时，我们将评估 <img src="https://latex.codecogs.com/gif.latex?10000^{10}=10^{40}" alt>个序列：这⼏乎不可能完成。而贪婪搜索的计算开销是 <img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_15-59-0.png" alt>，通常显著小于穷举搜索的计算开销。例如，当|Y| = 10000且T′ = 10时，我们只需评估<img src="https://latex.codecogs.com/gif.latex?10000*10=10^5" alt>个序列。</p><h3><span id="53-束搜索">5.3 束搜索</span></h3><p>束搜索（beam search）是对贪婪搜索的⼀个改进算法。它有⼀个束宽（beam size）超参数。我们将它设为 k。在时间步 1 时，选取当前时间步条件概率最⼤的 k 个词，分别组成 k 个候选输出序列的⾸词。在之后的每个时间步，基于上个时间步的 k 个候选输出序列，从 k |Y| 个可能的输出序列中选取条件概率最⼤的 k 个，作为该时间步的候选输出序列。最终，我们从各个时间步的候选输出序列中筛选出包含特殊符号“\<eos>”的序列，并将它们中所有特殊符号“\<eos>”后⾯的⼦序列舍弃，得到最终候选输出序列的集合。</eos></eos></p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_14-0-5.png" alt></p><p>束宽为2，输出序列最⼤⻓度为3。候选输出序列有A、C、AB、CE、ABD和CED。我们将根据这6个序列得出最终候选输出序列的集合。在最终候选输出序列的集合中，我们取以下分数最⾼的序列作为输出序列：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_16-5-41.png" alt></p><p>其中 L 为最终候选序列⻓度，α ⼀般可选为0.75。分⺟上的 Lα 是为了惩罚较⻓序列在以上分数中较多的对数相加项。分析可知，束搜索的计算开销为<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_16-6-24.png" alt>。这介于贪婪搜索和穷举搜索的计算开销之间。此外，贪婪搜索可看作是束宽为 1 的束搜索。束搜索通过灵活的束宽 k 来权衡计算开销和搜索质量。</p><h2><span id="6-bleu得分">6. Bleu得分</span></h2><p>评价机器翻译结果通常使⽤BLEU（Bilingual Evaluation Understudy）(双语评估替补)。对于模型预测序列中任意的⼦序列，BLEU考察这个⼦序列是否出现在标签序列中。</p><p>具体来说，设词数为 n 的⼦序列的精度为 pn。它是预测序列与标签序列匹配词数为 n 的⼦序列的数量与预测序列中词数为 n 的⼦序列的数量之⽐。举个例⼦，假设标签序列为A、B、C、D、E、F，预测序列为A、B、B、C、D，那么：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_16-7-11.png" alt></p><p>预测序列一元词组：A/B/C/D，都在标签序列里存在，所以P1=4/5，以此类推，p2 = 3/4, p3 = 1/3, p4 = 0。设 <img src="https://latex.codecogs.com/gif.latex?len_{label},len_{pred}" alt>分别为标签序列和预测序列的词数，那么，BLEU的定义为：</p><p><img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_16-8-52.png" alt></p><p>其中 k 是我们希望匹配的⼦序列的最⼤词数。可以看到当预测序列和标签序列完全⼀致时，BLEU为1。</p><p>因为匹配较⻓⼦序列⽐匹配较短⼦序列更难，BLEU对匹配较⻓⼦序列的精度赋予了更⼤权重。例如，当 pn 固定在0.5时，随着n的增⼤，<img src="https://gitee.com/kkweishe/images/raw/master/ML/2019-8-29_16-10-11.png" alt>。另外，模型预测较短序列往往会得到较⾼pn 值。因此，上式中连乘项前⾯的系数是为了惩罚较短的输出而设的。举个例⼦，当k = 2时，假设标签序列为A、B、C、D、E、F，而预测序列为A、 B。虽然p1 = p2 = 1，但惩罚系数exp(1-6/2) ≈ 0.14，因此BLEU也接近0.14。</p><h2><span id="7-代码实现">7. 代码实现</span></h2><p><a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/NLP/16.5%20seq2seq/seq2seq.ipynb" target="_blank" rel="noopener">TensorFlow  seq2seq的基本实现</a></p><h2><span id="8-转载自">8. 转载自：</span></h2><p><a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>]]></content>
      
      
      <categories>
          
          <category> nlp基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seq2seq（转载） </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2vec</title>
      <link href="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/"/>
      <url>/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-one-hot-vector">1 One-hot vector</span></h2><p>将每个单词用N维的长向量表示，N是一个很大的数字。</p><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/v2-4e2374fcb759404e1c16f56662093c9d_b.jpg" alt="img"></p><p>缺点是：</p><ol><li>每个单词都是单独的，无法计算单词之间的相似性，因为他们的乘积为0</li><li>并且维度较高，计算成本较大</li></ol><h2><span id="2-基于svd奇异值分解的词向量表示方法">2 基于SVD奇异值分解的词向量表示方法</span></h2><p>通过包含若干句话的语料库：</p><ol><li>I enjoy flying. </li><li>I like NLP. </li><li>I like deep learning</li></ol><p>我们可以得到一个共现矩阵，矩阵的各个数值代表词前后所跟其他词汇的频率统计：</p><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/v2-c1864b26d102c8c63bf6bac88e769250_b.jpg" alt="img"></p><p>我们可以把这个矩阵进行处理（降维），得到维度较小的词向量矩阵。</p><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/v2-1e6a5951b66d4f78ae26fef9b8b7e5ca_b.jpg" alt="img"></p><p>最终得到的矩阵USV，U就是我们想要得到的词向量。</p><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/v2-6eb166d15516ea5bf488c357e88a0dda_b.jpg" alt="img"></p><p>它达到了降维的目的，有效的编码语义和语法信息，U的V个行向量就分别是各个单词的词向量了。</p><p>奇异值分解（Singular Value Decomposition）是线性代数中一种重要的矩阵分解，是特征分解（方阵分解）在任意矩阵上的推广，最大目的是数据的降维。</p><p>但是它依然有很多的问题：</p><ol><li>矩阵维度变化频繁（语料库的词数改变）</li><li>矩阵过于稀疏</li><li>矩阵维度依然很大</li><li>计算SVD的效率问题</li></ol><h2><span id="3-word2vec">3 Word2vec</span></h2><p>通过神经网络，把ont-hot转换成低维的能体现相似性词向量的方法。</p><p>有两种算法，原理都差不太多：</p><ol><li>Skip-grams (SG) ：通过当前词预测上下文</li><li>Continuous Bag of Words (CBOW)：通过上下文预测当前词</li></ol><h3><span id="31-skip-grams-sg">3.1 Skip-grams (SG)</span></h3><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/1575882450737.png" alt="1575882450737" style="zoom: 50%;"></p><ol><li><strong>输入层：</strong>输入当前词的one-hot 向量$x_{k}$。词表大小为V，则ont-hot向量的维度为V</li><li><strong>输入层到隐藏层：</strong>第一个目标是训练出V行N列的矩阵W。 N是要训练出来的词向量的长度。这个矩阵就代表着最终训练得到的中心词的词向量矩阵。通过W与$x_k$的相乘，这个词的ont-hot转化为向量。</li><li><strong>隐藏层：</strong>Skip-gram的隐藏层不做任何处理，输入即输出：$h_i$</li><li><strong>隐藏层到输出层：</strong>第二个目标是训练出V行N列的矩阵W’。 通过隐层的输出与W’的点积，得到进行下一步</li><li><strong>输出层：</strong>通过隐层的输出$h_i$与W’的点积，得到一个V*1的向量，每一行的值代表着输入的中心词与矩阵W’中各个词的相似度。通过softmax将这个值转化成概率，损失函数则是真实概率与输出概率的差值，当概率越接近，得到的W和W’结果就越好。</li></ol><p>得到的W和W’就是我们将one-hot转化成的新的词向量矩阵。</p><h3><span id="32-cbow">3.2 CBOW</span></h3><p><strong>与skip-gram相反，CBOW</strong>通过上下文信息预测中心词wt。</p><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/1575883141332.png" alt="1575883141332" style="zoom: 50%;"></p><ol><li><strong>输入层：</strong>输入若干上下文词的one-hot 向量$x_{1k},x_{2k},…,x_{Ck}$。词表大小为V，则ont-hot向量的维度为V</li><li><strong>输入层到隐藏层：</strong>第一个目标是训练出V行N列的矩阵W。 N是要训练出来的词向量的长度。这个矩阵就代表着最终训练得到的中心词的词向量矩阵。通过W与$x_k$的相乘，这些词的ont-hot转化为向量。</li><li><strong>隐藏层：</strong>CBOW的隐藏层是词袋模型，得到$h_{i}$</li><li><strong>隐藏层到输出层：</strong>第二个目标是训练出V行N列的矩阵W’。 通过隐层的输出与W’的点积，得到进行下一步</li><li><strong>输出层：</strong>通过隐层的输出$h_i$与W’的点积，得到一个V*1的向量，每一行的值代表着输入的中心词与矩阵W’中各个词的相似度。通过softmax将这个值转化成概率，损失函数则是真实概率与输出概率的差值，当概率越接近，得到的W和W’结果就越好。</li></ol><p>word2Vec 模型训练带来了两个词嵌入矩阵 ， 分别代表中心词和上下文矩阵 。 上下文词嵌入会在训练后被丢弃，但保留了中心词词嵌入矩阵。</p><h3><span id="33-hierarchical-softmax">3.3 Hierarchical Softmax</span></h3><p>Skip-gram和cbow在输出层到隐藏层每次分别更新1行、C行的权重，但在隐藏层到输出层要更新所有权重，效率不行。</p><p>层次Softmax直接丢掉了第二个矩阵W’，直接将隐藏层的输出$h_{i}$直接处理：</p><p>根据词表构建一个哈夫曼树，之前要对V个词计算相似度，现在只需要最多计算$log(V)$次线就可以得到</p><p><img src="/2019/12/08/zi-ran-yu-yan-chu-li-ji-chu/word2vec/hs.png" alt="hs"></p><h3><span id="34-negative-sampling">3.4 Negative Sampling</span></h3><p>在 Word2Vec 中， 对于输出层来说，我每一个输出节点都要预测词表中所有词在当前位置的概率，在动辄几万甚至几十万大的词表中，用softmax 计算真的十分困难。 </p><p>但我们的目的不在于训练一个精准的语言模型，而只是为了训练得到语言模型的副产物-词向量，那么我们可不可以把输出压缩呢，将几万的输出压缩到几十程度，这计算量是成几何倍数的下降。</p><p>负采样的思路很简单，<strong>不直接让模型从整个词表中找最可能的词，而是直接给定这个词（正例）和几个随机采样的噪声词（负例），然后模型能够从这几个词中找到正确的词，就算达到目的了。</strong></p><p>那么如何对负例进行采样呢？作者直接使用<strong>基于词频的权重分布</strong>来获得概率分布进行抽样：</p><script type="math/tex; mode=display">weight(w) = \frac{count(w)^{0.75}}{\sum_u count(w)^{0.75}}</script><p>然后选取权重最大的一些词语作为负采样的样本</p><h2><span id="reference">reference</span></h2><p><a href="http://cs224d.stanford.edu/lecture_notes/notes1.pdf" target="_blank" rel="noopener">http://cs224d.stanford.edu/lecture_notes/notes1.pdf</a></p><p><a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a></p>]]></content>
      
      
      <categories>
          
          <category> nlp基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Word2vec </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率图模型-CRF</title>
      <link href="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/"/>
      <url>/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-马尔可夫随机场mrf">1 马尔可夫随机场MRF</span></h2><p>马尔可夫随机场是典型的马尔可夫网，即一个无向图模型。</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image001.png" alt="img"></p><p>马尔可夫随机场表示的随机变量之间具有马尔可夫性</p><ol><li>成对马尔可夫性：给定Yo的条件下，Yu和Yv条件独立</li></ol><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image006.png" alt="https://images2015.cnblogs.com/blog/1008922/201705/1008922-20170528162455422-1812780274.png" style="zoom:33%;"></p><ol><li>局部马尔可夫性</li></ol><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image008.png" alt="https://images2015.cnblogs.com/blog/1008922/201705/1008922-20170528160105610-150035802.png" style="zoom: 50%;"></p><ol><li>全局马尔可夫性</li></ol><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/clip_image010.png" alt="https://images2015.cnblogs.com/blog/1008922/201705/1008922-20170528162514578-985158227.png" style="zoom: 50%;"></p><h2><span id="2-条件随机场crf">2 条件随机场CRF</span></h2><p>条件随机场是给定随机变量X，输出随机变量Y的马尔可夫随机场，一种判别式无向图模型。</p><p>CRF的特点是假设输出随机变量构成马尔可夫随机场。</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing-crf/1575706608437.png" alt="1575706608437" style="zoom:50%;"></p><h2><span id="3-条件随机场的参数化形式">3 条件随机场的参数化形式</span></h2><p>设P(Y|X)为线性链条件随机场，则在随机变量X取值为x的条件下，随机变量Y取值为y的条件概率具有如下形式：</p><script type="math/tex; mode=display">\begin{aligned} P(y | x) &=\frac{1}{Z(x)} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \\ Z(x) &=\sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}(y, x) \end{aligned}</script><p>条件随机场定义条件概率的方式与隐马尔可夫模型类似，但是需要定义特征函数$f_k$</p><h2><span id="4-hmm与crf的区别">4 HMM与CRF的区别</span></h2><p> 一个是生成模型，一个是判别模型</p><p>用于标注问题的生成模型 and 由输入对输出进行预测的判别模型</p><p>HMM是一个特殊的CRF，CRF是HMM的一般化的形式</p>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率图模型-CRF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>概率图模型</title>
      <link href="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/"/>
      <url>/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-概率图模型">1 概率图模型</span></h2><p>机器学习最重要的任务，是根据一些己观察到的证据（例如训练样本）来对感兴趣的未知变量（例如类别标记）进行估计和推测。</p><p>而概率图模型就是概率图模型（probabilistic graphical model）是一类用图来表达变量相关关系的概率模型。</p><p>它以图为表示工具，最常见的是用一个结点表示一个或一组随机变量，结点之间的边表示变量间的概率相关关系，即”变量关系图”。</p><ul><li>节点表示随机变量/状态，边表示概率关系<ul><li>有向概率图模型或贝叶斯网络：因果关系</li><li>无向图模型或马尔科夫随机场：关联关系</li></ul></li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578149027920.png" alt="1578149027920" style="zoom: 50%;"></p><p>第一类是使用有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网(Bayesiannetwork)</p><p>第二类是使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网(Markovnetwork)</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578149164966.png" alt="1578149164966" style="zoom:50%;"></p><h2><span id="2-有向概率图模型贝叶斯网">2 有向概率图模型(贝叶斯网)</span></h2><ul><li>有向概率图模型的例子<br>隐马尔科夫模型、卡尔曼滤波、因子分析、概率主成分分析、独立成分分析、混合高斯、转换成分分析、概率专家系统、Sigmoid 信念网络、层次化混合专家</li></ul><h3><span id="21-有向概率图模型贝叶斯网">2.1 有向概率图模型(贝叶斯网)</span></h3><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153210482.png" alt="1578153210482" style="zoom: 67%;"></p><ul><li><p>概率分布</p><script type="math/tex; mode=display">p\left(x_{1}, x_{2}, x_{3}, x_{4}, x_{5}, x_{6}\right)=p\left(x_{1}\right) p\left(x_{2} | x_{1}\right) p\left(x_{3} | x_{1}\right) p\left(x_{4} | x_{2}\right) p\left(x_{5} | x_{3}\right) p\left(x_{6} | x_{2}, x_{5}\right)</script></li><li><p>贝叶斯网的表示<br>贝叶斯网使用一系列变量间的“局部”关系“紧凑”地表示联合概率分布</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153439973.png" alt="1578153439973" style="zoom: 50%;"></p></li><li><p>条件独立：$X_{4} \perp\left\{X_{1}, X_{3}\right\} | X_{2}$</p><script type="math/tex; mode=display">p\left(x_{4} | x_{1}, x_{2}, x_{3}\right)=p\left(x_{4} | x_{2}\right)</script><p> 其他的条件独立陈述：给定X2和X3，X1和X6独立</p></li></ul><h3><span id="22-三种经典有向图条件独立">2.2 三种经典有向图，条件独立</span></h3><ul><li>三种经典有向图，注意其中的条件独立证明</li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153674350.png" alt="1578153674350"></p><ul><li>通过贝叶斯球算法进行条件独立检验<ul><li>定义几个术语，描述贝叶斯球在一个节点上的动作：<ul><li><strong>通过（pass through）</strong>：从当前节点的父节点方向过来的球，可以访问当前节点的任意子节点。（父-&gt;子）从当前节点的子节点方向过来的球，可以访问当前节点的任意父节点。（子-&gt; 父）</li><li><strong>反弹（bounce back）</strong>：从当前节点的父节点方向过来的球，可以访问当前节点的任意父节点。（父-&gt; 父）从当前节点的子节点方向过来的球，可以访问当前节点的任意子节点。（子-&gt; 子）</li><li><strong>截止（block）</strong>：当前节点阻止贝叶斯球继续运动。</li></ul></li><li>贝叶斯球算法(规则)：<ul><li>假设在贝叶斯网络中，有一个按一定规则运动的球。已知中间节点（或节点集合）Z，如果球不能由节点X出发到达节点Y（或者由Y到X），则称X和Y关于Z独立。</li></ul></li><li>规则<ul><li>未知节点：总能使贝叶斯球通过，同时还可以反弹从其子节点方向来的球。（父-&gt; 子）|（子-&gt; 父/子）</li><li>已知节点：反弹从其父节点方向过来的球，截止从其子节点方向过来的球。（父-&gt; 父）|（子-&gt;“截止”）</li></ul></li><li></li></ul></li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578153956080.png" alt="1578153956080" style="zoom: 50%;"></p><h2><span id="3-无向概率图模型马尔科夫随机场">3 无向概率图模型(马尔科夫随机场)</span></h2><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578154032413.png" alt="1578154032413" style="zoom:50%;"></p><ul><li>无向概率图模型的概率分布<ul><li>有向图：利用“局部” 参数（条件概率）去表示联合概率</li><li>无向图：放弃条件概率，失去局部概率表示，保持独立地任意地选择这些函数的能力，保证所有重要的联合表示可以表示为局部函数的积</li></ul></li><li>局部函数的定义域<ul><li>团（全连通无向图）、极大团（局部最大的团）、势函数（非负实值函数，一种无约束形式）</li></ul></li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/1578154236713.png" alt="1578154236713" style="zoom:50%;"></p><h2><span id="4-生成式模型与称判别式模型">4 生成式模型与称判别式模型</span></h2><p>判别式模型（DiscriminativeModel）是直接学习决策函数f(x)或者直接求得条件概率分布p(y|x;θ)。常见的判别式模型有线性回归模型、支持向量机SVM等。</p><p><strong>对条件分布进行建模。</strong></p><p>生成式模型（GenerativeModel）由数据学习联合概率分布p(x,y)，然后通过贝叶斯公式来求得p(yi|x)，然后选取使得p(yi|x)最大的yi。例如朴素贝叶斯模型和隐马尔可夫模型。</p><p><strong>对联合分布进行建模。</strong></p><h2><span id="5-自然语言处理中概率图模型的演变">5 自然语言处理中概率图模型的演变</span></h2><p>横向：由点到线（序列结构）、到面（图结构）。</p><p>纵向：在一定条件下生成式模型（genmodel）转变为判别式模型（discriminativemodel）</p><p><img src="/2019/12/07/ji-qi-xue-xi/gai-lu-tu-mo-xing/clip_image002.jpg" alt="img"></p><h2><span id="6-典型的概率图模型">6 典型的概率图模型</span></h2><p>HMM、CRF等见其他文章。</p><h2><span id="reference">Reference</span></h2><pre><code>《模式识别与机器学习》课件 —— from UCAS《统计学习方法》 李航</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率图模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>集成学习</title>
      <link href="/2019/12/07/ji-qi-xue-xi/ji-cheng-xue-xi/"/>
      <url>/2019/12/07/ji-qi-xue-xi/ji-cheng-xue-xi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-集成学习">1 集成学习</span></h2><p>目前的集成学习有两大类：</p><ol><li>个体学习器之间不存在强依赖关系，可以同时生成、并行计算</li><li>个体学习器之间存在强依赖关系，必须串行生成</li></ol><p>1的代表：Bagging、随机森林</p><p>2的代表：Boosting、AdaBoost</p><h2><span id="2-bagging">2 Bagging</span></h2><ul><li>自助采样法  Bootstrap sample: <ul><li>对原始数据进行有放回的随机采样，m个样本的数据集中，有放回的采样m个样本；</li><li>等价于给每个样本点赋予不同权重</li></ul></li><li>思想：<ol><li>基于自助采样法（bootstrap sample）构造T个样本集</li><li>基于每个采样集训练T个基学习器，然后将这些基学习器结合</li><li>对分类任务采用投票法，对回归任务采用平均法</li></ol></li><li>Bagging可以降低模型方差，但不改变模型偏差</li></ul><h2><span id="3-随机森林ensemble-learning">3 随机森林（Ensemble Learning）</span></h2><p>随机森林以决策树为基学习器。是 Bagging 的优化版本。</p><ul><li><p>其包含的思想在于</p><ul><li>随机选择一部分特征</li><li>随机选择一部分样本</li></ul></li><li><p>构建过程：</p><ol><li>基于自助采样法（bootstrap sample）构造多个样本集</li><li>随机选取特征集合，特征数远小于原特征集合；</li><li>每棵树都尽最大程度的生长，并且没有剪枝过程；</li><li>进行投票决策</li></ol></li></ul><h2><span id="4-boosting">4 Boosting</span></h2><p>Boosting是将弱学习器提升为强学习器的算法</p><ul><li>Boosting的工作机制<ol><li>从初始训练集中学习一个基学习器</li><li>学习下一个基学习器，使其能帮助第一个基学习器</li><li>重复进行，最终组合所有的弱学习器</li></ol></li></ul><h2><span id="5-adaboost">5 AdaBoost</span></h2><ul><li><p>AdaBoost的思想</p><ul><li>在弱学习器失败的样本上学习第二个弱学习器</li><li>学习第二个弱学习器的方式是：<strong>样本重加权</strong><br>– 分对的样本，其权重减小<br>– 分错的样本，其权重增大</li></ul></li><li><p><strong>AdaBoost的学习过程</strong><br>有训练数据集   $\mathrm{T}=\left\{\left(\mathrm{x}_{1},\mathrm{y}_{1}\right),\left(\mathrm{x}_{2},  \mathrm{y}_{2}\right), \ldots,\left(\mathrm{x}_{\mathrm{N}}, \mathrm{y}_{\mathrm{N}}\right)\right\}$</p><ol><li><p>初始化训练数据的权值分布</p><script type="math/tex; mode=display">D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N</script></li><li><p>根据当前训练数据的权值分布<strong>学习到一个新的基学习器$G_{m}$</strong></p><script type="math/tex; mode=display">G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}</script></li><li><p><strong>更新训练数据的权值分布</strong></p></li><li><p>重复步骤2、3，得到多个基学习器</p></li><li><p>构建基本分类器的线性组合，并得到最终分类器</p><script type="math/tex; mode=display">G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)  \\ f(x)=\sum_{m=1}^{M} \alpha_{m} G_{m}(x)</script></li></ol></li></ul><ul><li><p>第3步中<strong>为基学习器$G_{m+1}$更新权值分布</strong>的步骤</p><ol><li><p>计算基学习器在训练集上的分类误差率$e_{m}$</p><script type="math/tex; mode=display">e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)</script></li><li><p>根据分类误差率得到系数$\alpha_{m}$，这个系数权重也是最终分类器组合时的权重：</p><script type="math/tex; mode=display">\alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}</script></li><li><p>更新训练数据集的权值分布</p></li></ol></li></ul><script type="math/tex; mode=display">\begin{array}{c}{D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)} \\{w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N}\end{array}</script><ul><li>伪代码描述</li></ul><p><img src="/2019/12/07/ji-qi-xue-xi/ji-cheng-xue-xi/1578318702125.png" alt="1578318702125" style="zoom: 50%;"></p><h2><span id="gbdt">GBDT</span></h2><h2><span id="xgboost">XGBoost</span></h2><h2><span id="reference">reference：</span></h2><pre><code>《统计学习方法》UCAS《模式识别与机器学习》课件《机器学习》——西瓜书https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 集成学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CNN and RNNs</title>
      <link href="/2019/12/07/shen-du-xue-xi/cnn-rnn/"/>
      <url>/2019/12/07/shen-du-xue-xi/cnn-rnn/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-cnn">1 CNN</span></h2><p>卷积神经网络：是一种专门用来处理具有类似网格结构数据的神经网络。</p><h3><span id="11-卷积神经网络结构">1.1 卷积神经网络结构</span></h3><div class="table-container"><table><thead><tr><th>CNN层次结构</th><th>作用</th></tr></thead><tbody><tr><td>输入层</td><td>卷积网络的<strong>原始输入</strong>，可以是原始或预处理后的像素矩阵</td></tr><tr><td>卷积层</td><td>参数共享、局部连接，利用平移不变性<strong>从全局特征图提取局部特征</strong></td></tr><tr><td>激活层</td><td>将卷积层的输出结果进行<strong>非线性映射</strong>，首选 Relu</td></tr><tr><td>池，化层</td><td><strong>进一步筛选特征</strong>，可以有效<strong>减少</strong>后续网络层次所需的<strong>参数量</strong></td></tr><tr><td>全连接层</td><td>用于把该层之前提取到的<strong>特征综合起来</strong>。</td></tr></tbody></table></div><h3><span id="12-卷积层的内容">1.2 卷积层的内容</span></h3><p><img src="/2019/12/07/shen-du-xue-xi/cnn-rnn/1.gif" alt="1"></p><p>原理上其实是对两个矩阵进行<strong>点乘求和</strong>的数学操作</p><p><strong>参数</strong>：batch * h * w * channel</p><ul><li><p>feature map：特征图的尺寸</p></li><li><p>kernel_size：卷积核的尺寸       </p></li><li><p>stride：步长</p></li><li><p>padding：填充</p></li><li><p>channel：通道数，包括输入和输出通道数</p></li><li><p><strong>尺寸计算公式</strong>：</p><script type="math/tex; mode=display">\text {output}=\left\lfloor\frac{(\text {input}-\text {kernel}+2 * \text {pad})}{\text {stride}}\right\rfloor+ 1</script></li></ul><h3><span id="13-常见问题">1.3 常见问题</span></h3><ul><li>卷积和池化的区别</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">卷积层</th><th style="text-align:center">池化层</th></tr></thead><tbody><tr><td style="text-align:center">结构</td><td style="text-align:center">零填充时输出维度不变，而通道数改变</td><td style="text-align:center">输出维度会降低，通道数不变</td></tr><tr><td style="text-align:center">稳定性</td><td style="text-align:center">输入特征发生细微改变时，输出结果会改变</td><td style="text-align:center">感受域内的细微变化不影响输出结果</td></tr><tr><td style="text-align:center">作用</td><td style="text-align:center">感受域内提取局部关联特征</td><td style="text-align:center">感受域内提取泛化特征，降低维度</td></tr><tr><td style="text-align:center">参数量</td><td style="text-align:center">与卷积核尺寸、卷积核个数相关</td><td style="text-align:center">不引入额外参数</td></tr></tbody></table></div><ul><li><p>为什么需要padding</p><ol><li>防止图像变小</li><li>防止边缘像素点关注过少</li></ol></li><li><p>为什么卷积核都是奇数尺寸</p><ol><li>保证像素点中心位置，避免位置信息偏移</li><li>填充边缘时能保证两边都能填充，原矩阵依然对称</li></ol></li><li><p>CNN的特性有哪些作用</p><ol><li>稀疏交互：卷积核尺度远小于输入的尺度。每个输出神经网络仅与前一层神经元存在连接权重</li></ol><blockquote><ul><li>提高模型的统计效率：原本一幅图像只能提供少量特征，现在每一块像素区域都可以提供一部分特征</li><li>使得参数大量减少，优化的时间复杂度也会减小几个数量级，过拟合情况也得到改善。 </li><li>稀疏交互的意义在于，<strong>先从局部的特征入手，再将局部特征组合起来形成更复杂和抽象的特征</strong>。</li></ul></blockquote><ol><li>参数共享：参数共享指的是<strong>同一个模型的不同模块中使用相同的参数</strong>。参数共享的意义在于使得卷积层具有<strong>平移等特性</strong>。</li></ol><blockquote><ul><li>权重共享一定程度上能增强参数之间的联系，获得更好的<strong>共性特征</strong>。</li><li>很大程度上降低了网络的参数，<strong>节省计算量和计算所需内存</strong>。</li><li>权重共享能起到<strong>很好正则的作用</strong>。正则化的目的是为了降低模型复杂度，防止过拟合，而权重共享则正好降低了模型的参数和复杂度。</li></ul></blockquote><ol><li>平移不变性：（局部）平移不变性是一个很有用的性质，尤其是当我们关心某个特征<strong>是否出现</strong>而不关心它出现的具体位置时。平移不变性是由于参数共享 和池化 所带来的。</li></ol></li><li><p>CNN有什么不足</p><ol><li>信息损失问题</li><li>忽略了位置信息</li></ol></li><li>1乘1卷积核的作用<ol><li>特征图大小不变，但是通道数可以改变，减小参数量</li><li>增加非线性，后面有激活函数</li><li>可以用于实现跨通道的信息交互，例如这一层有1乘1卷积核和3乘3卷积核同时存在</li></ol></li></ul><h2><span id="2-rnn">2 RNN</span></h2><p>循环神经网络（Recurrent Neural Network）是一种递归神经网络（Recursive Neural Network）。</p><p>其以序列数据为输入，在序列的演进方向进行递归且所有节点按链式连接的。<strong>将序列输入翻译成定长向量</strong></p><p><img src="/2019/12/07/shen-du-xue-xi/cnn-rnn/clipboard-1575710987473.png" alt="clipboard-1575710987473"></p><p>R、O决定网络类型，普通的RNN中：</p><script type="math/tex; mode=display">s_i=tanh(W_xx_i+W_ss_{i-1}+b)\\y_i=W_ys_i+b</script><h2><span id="3-lstm">3 LSTM</span></h2><ul><li><p>LSTM解决的问题：</p><ol><li>梯度消失，使得 RNN 很难有效地训练 。</li><li>RNN 难以捕捉到长距离依赖信息。</li></ol></li><li><p>LSTM的构成：输入门 i、遗忘门 f、输出门 o</p></li></ul><script type="math/tex; mode=display">\begin{aligned} c_{j} &=f \odot c_{j-1}+i \odot z \\ h_{j} &=o \odot \tanh \left(c_{j}\right) \\ i &=\sigma\left(x_{j} W^{x i}+h_{j-1} W^{h i}+b_i\right) \\ f &=\sigma\left(x_{j} W^{x f}+h_{j-1} W^{h f}+b_f\right) \\ o &=\sigma\left(x_{j} W^{x o}+h_{j-1} W^{h o}+b_o\right) \\ z &=\tanh \left(x_{j} W^{x z}+h_{j-1} W^{x z}+b_z\right)  \end{aligned}</script><ul><li><ul><li><strong>时刻 j 的状态</strong>：两个向量组成：<strong>记忆状态$c_j$ 和 隐藏状态$h_j$</strong></li><li><strong>记忆状态$c_j$ ：</strong>由经过遗忘门的上一个记忆状态$c_{j-1}$ 和经过输入门的更新候选项$z$</li><li><strong>更新候选项$z$：</strong>由当前输入$x_j$和前一个状态 $h_{j-1}$ 加一个tanh决定</li><li><strong>输入门 i、遗忘门 f、输出门 o：</strong>门的值由当前输入$x_j$和前一个状态 $h_{j-1}$ 加一个sigmoid决定</li></ul></li></ul><p><img src="/2019/12/07/shen-du-xue-xi/cnn-rnn/clipboard.png" alt="clipboard"></p><h2><span id="4-lstm如何解决rnn梯度消失问题">4 LSTM如何解决RNN梯度消失问题</span></h2><h3><span id="41-rnn为什么会出现梯度消失问题">4.1 RNN为什么会出现梯度消失问题</span></h3><p>RNN单元有三个w参数矩阵，分别控制输入$x_i$，上一个状态$S_{i-1}$，以及输出$y_i$</p><script type="math/tex; mode=display">S_i=tanh(W_xx_i+W_ss_{i-1}+b)\\O_i=W_ys_i+b</script><ul><li>前向传播过程（假设序列长度为3：）</li></ul><script type="math/tex; mode=display">\begin{array}{l}{S_{1}=W_{x} X_{1}+W_{s} S_{0}+b_{1}} \\ {S_{2}=W_{x} X_{2}+W_{s} S_{1}+b_{1}} \\ {S_{3}=W_{x} X_{3}+W_{s} S_{2}+b_{1} \\  O_{3}=W_{o} S_{3}+b_{2}}\end{array}</script><ul><li>反向传播（假设损失为$L_3$）</li></ul><script type="math/tex; mode=display">\begin{array}{l}{\frac{\partial L_{3}}{\partial W_{0}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial W_{o}}} \\ {\frac{\partial L_{3}}{\partial W_{x}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{x}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{x}}} \\ {\frac{\partial L_{3}}{\partial W_{s}}=\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial W_{s}}+\frac{\partial L_{3}}{\partial O_{3}} \frac{\partial O_{3}}{\partial S_{3}} \frac{\partial S_{3}}{\partial S_{2}} \frac{\partial S_{2}}{\partial S_{1}} \frac{\partial S_{1}}{\partial W_{s}}}\end{array}</script><ul><li>任意时刻对$w_s,w_x$的偏导为：</li></ul><script type="math/tex; mode=display">\frac{\partial L_{t}}{\partial W_{x}}=\sum_{k=0}^{t} \frac{\partial L_{t}}{\partial O_{t}} \frac{\partial O_{t}}{\partial S_{t}}\left(\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}\right) \frac{\partial S_{k}}{\partial W_{x}}</script><ul><li>由于tanh的累乘极限为0，则序列较长时梯度消失出现</li></ul><h4><span id="42-lstm是怎么解决的">4.2 LSTM是怎么解决的</span></h4><p>LSTM与RNN本质是相同的，其公式为：</p><script type="math/tex; mode=display">\begin{aligned} c_{j} &=f \odot c_{j-1}+i \odot z \\ h_{j} &=o \odot \tanh \left(c_{j}\right) \\ i &=\sigma\left(x_{j} W^{x i}+h_{j-1} W^{h i}+b_i\right) \\ f &=\sigma\left(x_{j} W^{x f}+h_{j-1} W^{h f}+b_f\right) \\ o &=\sigma\left(x_{j} W^{x o}+h_{j-1} W^{h o}+b_o\right) \\ z &=\tanh \left(x_{j} W^{x z}+h_{j-1} W^{x z}+b_z\right)  \end{aligned}</script><p>RNN，LSTM可以抽象为下图：</p><p><img src="/2019/12/07/shen-du-xue-xi/cnn-rnn/1470684-20190512212754183-21935751.png" alt="img"></p><p><img src="/2019/12/07/shen-du-xue-xi/cnn-rnn/1470684-20190512212754481-298118564.png" alt="img"></p><p>根据其思想，其本质上是：</p><script type="math/tex; mode=display">s_j=tanh(f \odot s_{j-1} + i \odot x)</script><p>其反向传播公式也是如同RNN的形式，限制其梯度消失的因素中：</p><script type="math/tex; mode=display">\prod_{j=k+1}^{t} \frac{\partial S_{j}}{\partial S_{j-1}}  = \prod_{j=k+1}^{t} hanh'\sigma(y)</script><script type="math/tex; mode=display">hanh'W_s$$的图像如下，大部分的数值要么0要么1：<img src="CNN&RNN/1576391115887.png" alt="1576391115887" style="zoom:50%;" />则反向传播的结果中，相当于一部分保留一部分消失。**即解决的本质上是：RNN中导致梯度消失的项，使其约等于0或1**## 5 GRUGRU是一种LSTM的替代方案。GRU 也基于门机制，但是总体上使用了更少的门并且网络不再额外给出记忆状态</script><p>\begin{aligned} s_{j}=R_{\mathrm{GRU}}\left(s_{j-1}, x_{j}\right) &amp;=(1-z) \odot x_{j-1}+z \odot \tilde{s_{j}} \\ z &amp;=\sigma\left(x_{j} W^{xz}+s_{j-1} W^{s z}\right) \\ r &amp;=\sigma\left(x_{j} W^{xr}+s_{j-1} W^{s r}\right) \\ \tilde{s_{j}} &amp;=\tanh \left(x_{j} W^{x_{j}}+\left(r \odot s_{j-1}\right) W^{xg}\right) \\ y_{j}=O_{\mathrm{GRU}}\left(s_{j}\right) &amp;=s_{j} \end{aligned}</p><p>$$</p><ul><li><ul><li>更新门z：由前一时刻的状态和输入决定</li><li>重置门r：由前一时刻的状态和输入决定</li></ul></li><li><ul><li>s_j~ ：由重置门r、输入、前一个状态决定</li><li>当前状态s_j：由更新门、输入、s_j~决定</li></ul></li></ul><h2><span id="reference">Reference</span></h2><p>《统计学习方法》</p><p>《深度学习》 （花书）</p><p><a href="https://github.com/NLP-LOVE/ML-NLP" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP</a><br><a href="https://github.com/htfhxx/NLPer-Interview" target="_blank" rel="noopener">https://github.com/htfhxx/NLPer-Interview</a><br><a href="https://www.cnblogs.com/jins-note/p/10853788.html" target="_blank" rel="noopener">https://www.cnblogs.com/jins-note/p/10853788.html</a><br><a href="https://zhuanlan.zhihu.com/p/44163528" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/44163528</a></p>]]></content>
      
      
      <categories>
          
          <category> deep-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN and RNNs </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DL优化方法</title>
      <link href="/2019/12/07/shen-du-xue-xi/dl-you-hua-fang-fa/"/>
      <url>/2019/12/07/shen-du-xue-xi/dl-you-hua-fang-fa/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-防止欠拟合-过拟合的方法">1 防止欠拟合、过拟合的方法</span></h2><ul><li><strong>欠拟合：</strong>模型⽆法得到较低的训练误差。</li><li><strong>过拟合：</strong>是模型的训练误差远小于它在测试数据集上的误差。</li></ul><p><img src="/2019/12/07/shen-du-xue-xi/dl-you-hua-fang-fa/2019-8-18_21-7-0.png" style="zoom:50%;"></p><h3><span id="11-增大训练集规模">1.1 增大训练集规模</span></h3><h3><span id="12-dropout">1.2 Dropout</span></h3><ul><li>Dropout的思想是：在每次训练过程中随机地忽略一些神经元。</li><li>为什么Dropout能解决过拟合<br>整个dropout过程就相当于： 对很多个不同的神经网络取平均，相当于Bagging的一种近似</li></ul><h3><span id="13-正则化">1.3 正则化</span></h3><p>在模型损失函数中添加惩罚项，使学出的模型参数值较小，是应对过拟合的常⽤⼿段。</p><h4><span id="131-l1正则化">1.3.1 L1正则化</span></h4><p>L1正则化项：向量的1范数</p><script type="math/tex; mode=display">正则化项： \Omega(\theta) = ||w||_1 =  \sum_i |w_i| \\目标函数： \tilde{J}(w;X,y) = \alpha ||w||_1  + J(w;X,y)  \\梯度： \nabla_w \tilde{J}(w;X,y) = \alpha sign(w) + \nabla_w J(w;X,y) \\</script><p>L1 正则化使得权重值可能被减少到0。 因此，L1对于压缩模型很有用。</p><h4><span id="132-l2正则化">1.3.2 L2正则化</span></h4><p>L2正则化项：向量的2范数（元素平方和相加再开放</p><script type="math/tex; mode=display">正则化项： \Omega(\theta) = \frac{1}{2} ||w||_2^2  = \frac{1}{2}w^Tw \\目标函数： \tilde{J}(w;X,y) = \frac{\alpha}{2}w^Tw  + J(w;X,y)  \\梯度： \nabla_w \tilde{J}(w;X,y) = \alpha w + \nabla_w J(w;X,y) \\梯度更新 ： w \leftarrow (1- \epsilon \alpha) w - \epsilon \nabla_w J(w;X,y)</script><p>L2正则化又称权重衰减。因为其导致权重<strong>趋向于0</strong>（但不全是0）。</p><h4><span id="132-关于正则化的一些问题">1.3.2 关于正则化的一些问题</span></h4><ul><li><p>只对权重做惩罚，不包含偏置<br>精确拟合偏置所需的数据通常比拟合权重少得多，因此偏置拟合程度较好。<br>且正则化偏置参数可能会导致明显的欠拟合。</p></li><li><p>L1正则化与L2正则化的异同<br>相同点：限制参数的规模，使模型偏好于权值较小的目标函数，防止过拟合。<br>不同点：</p><ol><li>L1 正则化可以产生<strong>稀疏权值矩阵</strong>，可以用于特征选择；L2 趋向于生成参数值很小的矩阵</li><li>L1 适用于特征之间有关联的情况； L2 适用于特征之间没有关联的情况</li></ol></li><li><p>为什么L1正则化可以产生稀疏值，而L2不会？</p></li></ul><p>  <img src="/2019/12/07/shen-du-xue-xi/dl-you-hua-fang-fa/L2.png" alt="L2"><img src="/2019/12/07/shen-du-xue-xi/dl-you-hua-fang-fa/L1.png" alt="L1"></p><p>  如下图所示：加入的正则化惩罚项可以看成是对原损失函数的约束，尽可能使惩罚项最小。</p><p>  彩色的圈是原损失函数的梯度线，在同一个圈上的梯度相同。<br>  可以得知，选择L1惩罚项时，容易出现很多的“尖”，在这些“尖”上L1惩罚项最小，导致了部分参数为0</p><ul><li>为什么L1和L2正则化可以防止过拟合<br>L1 &amp; L2 正则化会使模型偏好于更小的权值，更小的权值意味着更低的模型复杂度，有助于提高模型的泛化能力。</li></ul><h3><span id="14-early-stoping">1.4 Early stoping</span></h3><p>提前终止指的是<strong>当验证集的误差连续 <code>n</code>次递增时停止训练</strong>。</p><h2><span id="2-梯度下降方法">2 梯度下降方法</span></h2><h3><span id="21-基础梯度下降法">2.1 基础梯度下降法</span></h3><ul><li><p>标准梯度下降</p><script type="math/tex; mode=display">\theta = \theta + \eta \cdot  \nabla_\theta J(\theta)</script></li><li><p>mini-batch 梯度下降</p><script type="math/tex; mode=display">\theta = \theta + \eta \cdot  \nabla_\theta J(\theta; x^{(i:i+n)}; y^{(i:i+n)})</script></li><li><p>随机梯度下降</p><script type="math/tex; mode=display">\theta = \theta + \eta \cdot  \nabla_\theta J(\theta; x^{(i)}; y^{(i)}) \,\,\, \eta 为学习率</script></li><li><p>优缺点：</p><ol><li>标准梯度下降需要计算所有样本梯度，更新速度慢；梯度方向一致且更新次数少，容易陷入局部最小值</li><li>随机梯度下降是mini-batch的特殊，不能达到最优解，不容易陷入局部最小</li></ol></li></ul><h3><span id="22-动量梯度下降法">2.2 动量梯度下降法</span></h3><ul><li><p>梯度的指数加权平均值$v_t$:</p><script type="math/tex; mode=display">v_t = \gamma v_{t-1} + \eta \nabla_\theta J(\theta)  \\ \theta = \theta - v_t \\\gamma: \text{加权系数，常取值为0.9}</script></li><li><p>使用指数加权平均值来更新梯度：本质是通过增加动量来减少随机，增加梯度的稳定性。</p></li><li><p>其中的指数加权平均值本质上考虑了前几次的梯度，不至于在”下坡“过程中因为遇到暂时的”上坡“而改变方向</p></li></ul><h3><span id="23-自适应学习率优化算法">2.3  自适应学习率优化算法</span></h3><ul><li>Adagrad</li><li>Adadelta</li><li>RMSprop</li><li>Adam</li></ul><p>ps：看到这里脑壳痛，先放放</p><h2><span id="3-归一化方法">3 归一化方法</span></h2><h3><span id="31-普通的数据归一化方法">3.1 普通的数据归一化方法</span></h3><ul><li><p>Min-max 归一化：</p><script type="math/tex; mode=display">x^* = \frac{x -min } {max - min}</script></li><li><p>Zero-mean 归一化：均值为0，标准差为1的标准正态分布</p><script type="math/tex; mode=display">x^* = \frac{x- \mu }{\sigma}</script></li></ul><h3><span id="32-batch-normalization">3.2 Batch Normalization</span></h3><p>假设一个 batch 为 m 个输入 $B = \{x_{1}, \cdots, x_m\}$ , Batch Normalization在这 m 个数据之间做归一化， 并学习参数 $\gamma , \beta$：</p><script type="math/tex; mode=display">\mu_B \leftarrow \frac{1}{m} \sum_{i=1}^{m} x_i    \\\sigma_B^2 \leftarrow \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2  \\\hat{x}_i  \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \\y_i \leftarrow \gamma \hat{x}_i + \beta \equiv BN_{\gamma, \beta}{(x_i)}</script><h3><span id="33-layer-normalization">3.3 Layer Normalization</span></h3><p>不同于 BN， 其在层内进行 Normalization。即直接对隐层单元的输出做 Normalization。</p><script type="math/tex; mode=display">u^l = \frac{1}{H} \sum_{i=1}^H a_i^l \\\sigma^l = \sqrt{\frac{1}{H}\sum_{i=1}^H(a_i^l - u^l)^2} \\\hat{a}_i^l  \leftarrow \frac{a_i^l - \mu^l}{\sqrt{\sigma_l^2 + \epsilon}} \\y_i^l \leftarrow \gamma \, \hat{a}_i^l + \beta \equiv LN_{\gamma, \beta}{(a_i^l)}</script><p>reference：</p><pre><code>《统计学习方法》https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> deep-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DL优化方法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention</title>
      <link href="/2019/12/07/zi-ran-yu-yan-chu-li-ji-chu/attention/"/>
      <url>/2019/12/07/zi-ran-yu-yan-chu-li-ji-chu/attention/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-basic-attention">1 Basic Attention</span></h2><h3><span id="11-attention-定义">1.1 Attention 定义</span></h3><p><img src="/2019/12/07/zi-ran-yu-yan-chu-li-ji-chu/attention/006gOeiSly1g0tf397umyj30kn08uq34.jpg" alt="006gOeiSly1g0tf397umyj30kn08uq34"></p><p>对于Attention机制的整个计算过程，可以总结为以下三个过程：</p><ul><li><strong>socre 函数：</strong> 根据 Query 与 Key 计算两者之间的相似性或相关性， 即 socre 的计算。</li><li><strong>注意力权重计算：</strong>通过一个softmax来对值进行归一化处理获得注意力权重值， 即$a_{i,j}$ 的计算。</li><li><strong>加权求和生成注意力值：</strong>通过注意力权重值对value进行加权求和， 即 $c_i$ 的计算。</li></ul><script type="math/tex; mode=display">\alpha_{i,j} = \frac{e^{score(Query, Key(j))}}{\sum_{k=1}^t e^{score(Query, Key(k))}} \\c_i= \sum_{i=1}^n = \alpha_{i,j} h_j</script><h3><span id="12-score函数的选择">1.2 Score函数的选择</span></h3><p>常见的方式主要有以下三种：</p><ul><li>求点积：学习快，适合向量再同一空间中，如 Transformer 。</li></ul><script type="math/tex; mode=display">score(Query, Key(j)) = Query \cdot Key(j)</script><ul><li>Cosine 相似性</li></ul><script type="math/tex; mode=display">score(Query, Key(j)) = \frac{Query \cdot Key(j)}{||Query|| \cdot ||Key(j)||}</script><ul><li>MLP网络</li></ul><script type="math/tex; mode=display">score(Query, Key(j)) = MLP(Query,  Key(j)) \\general: score(Query, Key(j)) = Query \, W \, Key(j) \\concat: score(Query, key(j)) = W \, [Query;Key(j) ]</script><p>一般情况下，采用MLP网络更加灵活一些，且可以适当的扩展层以及改变网络结构，这对于一些任务来说是很有帮助的。</p><h3><span id="13-self-attention">1.3 Self-Attention</span></h3><p>Self-Attention 的本质就是<strong>自己注意自己</strong>， 粗暴点来说，就是，<strong>Q，K，V是一样的</strong>，即：</p><script type="math/tex; mode=display">Attention \, value = Attention(W_QX,W_KX,W_VX)</script><p>它的内部含义是对序列本身做 Attention，来获得序列内部的联系。 </p><h2><span id="2-seq2seqattention">2 Seq2Seq+Attention</span></h2><ul><li>传统的Seq2Seq<script type="math/tex; mode=display">\begin{array}{c}{s_{i}=f\left(y_{i-1}, s_{i-1}, c\right)} \\ {p\left(y_{i} | y_{1}, y_{2} \ldots y_{i-1}\right)=g\left(y_{i-1}, s_{i}, c\right)}\end{array}</script>$s_{i-1}$是decoder中的上一个隐状态；$y_{i-1}$是decoder中上一个预测得到的输出，在下一步进行输入；c是encoder得到的编码信息</li></ul><p><img src="/2019/12/07/zi-ran-yu-yan-chu-li-ji-chu/attention/%E5%9F%BA%E7%A1%80%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84/11525720-15378fee9fe62013.webp" alt="img" style="zoom: 80%;"></p><ul><li><p>加入Attention的Seq2Seq：<strong>原公式中encoder的编码信息c变成了$c_{i}$</strong></p><script type="math/tex; mode=display">{p\left(y_{i} | y_{1}, y_{2}, \ldots, y_{i-1}\right)=g\left(y_{i-1}, s_{i-1}, c_{i}\right)}</script><script type="math/tex; mode=display">- 其中，对encoder得到的编码信息做了处理：语境权重加入到encoder的隐藏向量得到编码信息</script><script type="math/tex; mode=display">c_{i}=\sum_{j=1}^{T_{x}} \alpha_{i j} h_{j}</script></li></ul><script type="math/tex; mode=display">- 语境权重是decoder中的上一个隐状态与所有的encoder中的隐藏向量计算得到（例如内积）</script><script type="math/tex; mode=display">\begin{aligned} \alpha_{i j}=& \frac{\exp \left(e_{i j}\right)}{\sum_{k=1}^{T_{x}} \exp \left(e_{i k}\right)} \\ e_{i j}=& a\left(s_{i-1}, h_{j}\right) \end{aligned}</script>]]></content>
      
      
      <categories>
          
          <category> nlp基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归与逻辑回归</title>
      <link href="/2019/12/05/ji-qi-xue-xi/xian-xing-hui-gui-yu-luo-ji-hui-gui/"/>
      <url>/2019/12/05/ji-qi-xue-xi/xian-xing-hui-gui-yu-luo-ji-hui-gui/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1><span id="1-线性回归">1 线性回归</span></h1><h2><span id="11-简介">1.1 简介</span></h2><p>简单来说，线性回归算法就是<strong>找到一条直线（一元线性回归）或一个超平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。</strong></p><script type="math/tex; mode=display">y = w_0x_0 + \cdots  + w_px_p + b = wx+b</script><h2><span id="12-如何计算">1.2 如何计算</span></h2><ul><li><p>损失函数为：</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2, \qquad  \\</script></li><li><p>利用<strong>梯度下降法</strong>找到最小值点，也就是最小误差，最后把 w 和 b 给求出来</p></li></ul><h2><span id="13-过拟合如何解决">1.3 过拟合如何解决</span></h2><ul><li><p>使用岭回归（加入L2正则项）</p><script type="math/tex; mode=display">\hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i w_i^2</script></li><li><p>使用Lasso回归（加入L1正则项）</p><script type="math/tex; mode=display">\hat{h}_{\theta}(x) = h_{\theta}(x) + \lambda \sum_i |w_i|</script></li><li><p>使用场景</p><ul><li>只要数据线性相关，用LinearRegression拟合的不是很好，<strong>需要正则化</strong>，可以考虑使用岭回归</li><li>如果输入特征的维度很高，而且是稀疏线性关系的话， 岭回归就不太合适,考虑使用Lasso回归。</li><li><strong>L1正则化(Lasso回归)可以使得一些特征的系数变小,甚至还使一些绝对值较小的系数直接变为0</strong>，从而增强模型的泛化能力 。</li></ul></li></ul><h1><span id="2-逻辑回归">2 逻辑回归</span></h1><h2><span id="21-简介">2.1 简介</span></h2><p>logistic回归用于解决的是分类问题，<strong>其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。</strong></p><p>也就是说，logistic 回归不是对所有数据点进行拟合，而是要对<strong>数据之间的分界线</strong>进行拟合。</p><ul><li>表达式是：</li></ul><script type="math/tex; mode=display">h_\theta(x) = sigmoid(\theta^T X)  = \frac{1}{1 + e^{-\theta^T X}}</script><ul><li>其中，sigmoid函数的参数就是线性回归的结果</li></ul><p><img src="/2019/12/05/ji-qi-xue-xi/xian-xing-hui-gui-yu-luo-ji-hui-gui/00630Defly1g4pvk2ctatj30cw0b63yq.jpg" alt="image" style="zoom: 50%;"></p><h2><span id="22-如何求解">2.2 如何求解</span></h2><ul><li><p>线性回归的拟合函数本质上是对 <strong>输出变量 y 的拟合</strong>， 而逻辑回归的拟合函数是对 <strong>label 为1的样本的概率的拟合</strong>。</p></li><li><p>线性回归其参数计算方式为<strong>最小二乘法</strong>， 逻辑回归其参数更新方式为<strong>极大似然估计</strong>。</p></li><li><p>逻辑回归满足伯努利分布：</p><script type="math/tex; mode=display">P(Y=1|x; \theta) = h_{\theta}(x) \\P(Y=0|x; \theta)  = 1 - h_{\theta}(x) \\p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}</script></li><li><p>逻辑回归的极大似然函数是：</p><script type="math/tex; mode=display">\begin{align}L(\theta) &= \prod_{i=1}^m p(y^{(i)}|x(i);\theta) \\ &= \prod_{i=1}^m  (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}\\\end{align}</script></li><li><p>对数似然函数为：</p><script type="math/tex; mode=display">\begin{align}L(\theta) &= log L(\theta ) \\&= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))\end{align}</script></li></ul><ul><li><p>求解梯度：</p><script type="math/tex; mode=display">\begin{align}\frac{\partial L(\theta)}{\partial \theta_j} &= (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} \\&= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} \\&= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j \\&= [y - h_{\theta} (x)]x_j \\\end{align}</script></li><li><p>优化参数：</p><script type="math/tex; mode=display">\begin{align}\theta_j &= \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} \\&= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}   \end{align}</script></li></ul><h2><span id="23-如何实现多分类">2.3 如何实现多分类</span></h2><ul><li><strong>方式1：</strong> softmax。修改逻辑回归的损失函数，将sigmoid改为softmax函数构造模型从而解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。</li><li><strong>方式2：</strong> 根据每个类别都建立一个二分类器。本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。</li></ul><p>若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。</p><h2><span id="24-逻辑回归特征的离散化">2.4 逻辑回归特征的离散化</span></h2><p>很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型</p><p>LR 为何要对特征进行离散化</p><ul><li><strong>非线性。</strong> 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； </li><li><strong>速度快。</strong> 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展</li><li><strong>鲁棒性。</strong> 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄&gt;30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；</li><li><strong>方便交叉与特征组合</strong>： 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。</li><li><strong>稳定性：</strong> 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。</li><li><strong>简化模型：</strong> 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。</li></ul><h1><span id="reference">Reference</span></h1><pre><code>《统计学习方法》《机器学习》——西瓜书https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 线性回归与逻辑回归 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法题易用写法汇总</title>
      <link href="/2019/12/04/yu-yan-gong-ju-ji-zhu-deng-wen-dang/suan-fa-ti-yi-yong-xie-fa-hui-zong/"/>
      <url>/2019/12/04/yu-yan-gong-ju-ji-zhu-deng-wen-dang/suan-fa-ti-yi-yong-xie-fa-hui-zong/</url>
      
        <content type="html"><![CDATA[<h2><span id="二分查找">二分查找</span></h2><p>leetcode704：<a href="https://leetcode.com/problems/sqrtx/submissions/" target="_blank" rel="noopener">https://leetcode.com/problems/sqrtx/submissions/</a></p><pre><code>class Solution {public:    int search(vector&lt;int&gt;&amp; nums, int target) {        if(nums.empty()==true)            return -1;        int left=0,right=nums.size()-1;        while(left&lt;right){    //不再需要考虑退出循环时返回left还是right            int middle=left+(right-left)/2;   //得到的始终是左中位数；left+(right-left+1)/2是右中位数   //(left+right)/2; 会有整型溢出问题                     // if(nums[middle]==target)  //只夹逼，不判断，效率且适用性高            //     return middle;            if(nums[middle]&lt;target)  //注意如果是从这里收缩就不要带等于号（不收缩掉middle）                left=middle+1;            else   //只需要判断在左边还是右边就好了                   right=middle;  //因为取的是左中位数，所以要从左边界收缩（例如只有两个数字[1,2]，target=3，从右边界收缩会在1处死循环）        }        if(nums[left]==target) //因为临界条件中left==right时没有判断，因此在这里需要判断            return left;        return -1;    }};</code></pre><p>判断是否左收缩还是右收缩，可以分析只有两个数字的极限情况<br>例如leetcode69中，求sqrt(x)的整数解，需要右收缩：只有两个数字时：[2,3]要取2的时候右收缩<br>收缩过程要收缩掉middle<br><a href="https://leetcode.com/problems/sqrtx/submissions/" target="_blank" rel="noopener">https://leetcode.com/problems/sqrtx/submissions/</a></p><h2><span id="reference">Reference</span></h2><pre><code>https://mp.weixin.qq.com/s?__biz=MzUyNjQxNjYyMg==&amp;mid=2247486644&amp;idx=1&amp;sn=a4c5e9aad51a42fceeb13543b80c22a3&amp;chksm=fa0e6335cd79ea23a15452bf3c4195f99c6a70fc10273870dda2f4f0db46543e429189affd8d&amp;mpshare=1&amp;scene=23&amp;srcid=&amp;sharer_sharetime=1574858020671&amp;sharer_shareid=59332ea7c33ee752808701f0287171ae#rd</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法题易用写法汇总 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树</title>
      <link href="/2019/12/02/ji-qi-xue-xi/jue-ce-shu/"/>
      <url>/2019/12/02/ji-qi-xue-xi/jue-ce-shu/</url>
      
        <content type="html"><![CDATA[<h1><span id="决策树">决策树</span></h1><p>[TOC]</p><h2><span id="1-简介">1 简介</span></h2><p>决策树是一个分而治之的递归过程。 </p><ul><li>开始，构建根节点，将所有训练数据都放在根节点。</li><li>然后，<strong>选择一个最优特征</strong>，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。</li><li>如果子集未分类完毕，则在子集中选择一个最优特征，继续进行划分，直到所有训练数据子集都被正确分类或没有合适的特征为止。</li></ul><ol><li>构建根节点；2. 选择最优特征并划分子集；3. 继续划分直到所有训练数据基本划分正确。</li></ol><p>决策树学习通常包括3个步骤：<strong>特征选择、决策树的生成和决策树的修剪。</strong></p><h2><span id="2-特征选择">2 特征选择</span></h2><p><strong>选择最优划分属性</strong>是决策树的关键。</p><p>几种常见的划分属性的策略：ID3、C4.5、CART。</p><h3><span id="21-id3-c45">2.1 ID3、C4.5</span></h3><ul><li>熵：随机变量的不确定性（熵越大不确定性越大），用于度量样本的集合纯度。</li></ul><script type="math/tex; mode=display">H(X)=-\sum_{i=1}^{n} p_{i} \log p_{i}</script><ul><li>条件熵：在随机变量X的条件下，随机变量Y的不确定性</li></ul><script type="math/tex; mode=display">H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)</script><ul><li>信息增益：<strong>数据集D的经验熵$D(D)$</strong>与<strong>特征A下的数据集D的经验条件熵</strong>之差</li></ul><script type="math/tex; mode=display">g(D, A)=H(D)-H(D | A)</script><ul><li><p>信息增益的算法</p><ul><li><p>输入：数据集D和特征A<br>输出：特征A对数据集D的信息增益$g(D, A)$</p><ol><li>计算数据集D的经验熵H(D)： </li></ol><script type="math/tex; mode=display">H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}</script><ol><li>计算特征A对数据集D的经验条件熵H(D|A)：</li></ol><script type="math/tex; mode=display">H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D |} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}</script><ol><li>计算信息增益： $g(D, A)=H(D)-H(D | A)$</li></ol></li></ul></li><li><p>信息增益比：</p></li></ul><script type="math/tex; mode=display">g_{R}(D, A)=\frac{g(D, A)}{H(D)}</script><ul><li>信息增益与信息增益比的协调<br>信息增益对<strong>取值数目较多的属性</strong>有偏好(分支多，划分后的信息增益大)<br>增益比对取值数目较少的属性有所偏好，于是：<pre><code>    c4.5**先从侯选属性中找出信息增益高于平均水平的，再从中选择增益率最高的**</code></pre></li></ul><h3><span id="22-cartclassification-and-regression-tree">2.2 CART（Classification And Regression Tree)</span></h3><ul><li><p>基尼指数：表示集合 D 的不确定性</p><script type="math/tex; mode=display">\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}</script></li><li><p>给定样本的基尼指数</p><script type="math/tex; mode=display">\operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}</script></li><li><p>如果样本集合 $D$ 根据特征 $A$ 是否取一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 两部分， 那么在特征 A 的条件下， 集合 D 的基尼系数定义为：</p><script type="math/tex; mode=display">\operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)</script></li></ul><h2><span id="3-决策树的生成">3 决策树的生成</span></h2><h3><span id="31-id3-c45决策树的生成">3.1 ID3、C4.5决策树的生成</span></h3><ul><li>输入：训练数据集D，特征集A，信息增益（比）的阈值；<br>输出：决策树T。<ol><li>计算特征集中各个特征对数据集D的信息增益（比），选择信息增益较大的特征$A_g$</li><li>如果特征$A_g$的信息增益（比）小于阈值，则置为单节点的树</li><li>否则：对特征$A_g$的各个取值划分子集作为其子树</li><li>递归这一过程（数据集为空或者特征集为空的情况返回树）</li></ol></li></ul><p>​                                                                         </p><h3><span id="32-cart决策树的生成">3.2 CART决策树的生成</span></h3><ul><li><p>输入：训练数据集D，特征集A，终止条件；<br>输出：决策树T</p><ol><li>从根节点开始，对节点计算现有特征的基尼系数。对于每一个特征，以及特征的每一种取值，根据 “是” 与 “否” 计算划分过后的基尼系数</li><li>选择基尼指数最小的<strong>特征及其对应的取值</strong>作为<strong>最优特征和最优切分点</strong>。然后根据最优特征和最优切分点，将本节点的数据集二分，生成两个子节点。</li><li>对两个字节点递归地调用上述步骤。</li></ol><p><strong>终止条件</strong>一般是：节点中的样本个数小于阈值，样本中的基尼指数小于阈值，没有更多特征</p></li></ul><p>尽可能多的划分，然后进行后剪枝</p><h2><span id="4-决策树的剪枝处理">4 决策树的剪枝处理</span></h2><p>防止过拟合并减少训练时间和测试时间</p><p>剪枝通过：极小化决策树整体的损失函数</p><ul><li><p>决策树整理的损失函数：包括信息增益与模型规模</p><script type="math/tex; mode=display">C_{\alpha}(T)=C(T)+\alpha|T|</script></li><li><p>剪枝过程</p><ol><li>计算每个节点的经验熵（基尼指数）</li><li>从叶子节点自底向上递归，回缩到对应的父节点上，计算损失函数的变化</li><li>损失变小则剪枝</li></ol></li></ul><h2><span id="5-三种决策树的区别">5 三种决策树的区别</span></h2><p>特征选择方式不同：信息增益、信息增益率、基尼指数</p><p>其中，信息增益偏向属性取值多的分支进行划分，信息增益率偏向属性取值少的分支进行划分，CAR树既可以回归也可以分类</p><h2><span id="6-面试常问">6 面试常问</span></h2><p>递归的终止条件是什么呢？</p><ol><li>直到每个叶子节点都<strong>只有一种类型</strong>时停止，（这种方式很容易过拟合） </li><li>当叶子节点的<strong>样本个数</strong>小于一定的阈值</li><li><p>节点的<strong>信息增益</strong>（或基尼系数）小于预定阈值    <strong>推荐</strong></p></li><li><p>没有更多特征</p></li></ol><p>为什么信息增益偏向属性取值多的分支？</p><ul><li>当特征取值较多时， 根据此特征划分得到的子集纯度提升的更多（对比取值较少的特征）</li></ul><h2><span id="reference">Reference：</span></h2><pre><code>《统计学习方法》《机器学习》——西瓜书https://github.com/NLP-LOVE/ML-NLPhttps://github.com/htfhxx/NLPer-Interview</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 决策树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SVM</title>
      <link href="/2019/11/28/ji-qi-xue-xi/svm-de-jian-ji-xiang-zheng-li/"/>
      <url>/2019/11/28/ji-qi-xue-xi/svm-de-jian-ji-xiang-zheng-li/</url>
      
        <content type="html"><![CDATA[<h1><span id="svm">SVM</span></h1><p>[TOC]</p><h2><span id="1-整体概览">1 整体概览</span></h2><ul><li>什么是SVM<br>SVM 是一种二分类模型。它的基本思想是在特征空间中寻找间隔最大的分离超平面使数据二分类</li></ul><ul><li>线性可分支持向量机——硬间隔支持向量机</li><li>线性支持向量机——软间隔支持向量机</li><li>非线性支持向量机——核技巧</li></ul><h2><span id="2-线性可分支持向量机">2 线性可分支持向量机</span></h2><h3><span id="21-函数间隔与几何间隔">2.1 函数间隔与几何间隔</span></h3><ul><li>分离超平面：$w^{<em>} \cdot x+b^{</em>}=0$</li><li>点到超平面的距离：正的一侧：$\gamma_{i}=\frac{1}{|w|}(w \cdot x_i +b) $      负的一侧：$\gamma_{i}=-\frac{1}{|w|}(w \cdot x_i +b) $      </li><li>由于$w \cdot x+b$与类标记y的符号是否一致能代表分类正确，定义：<ul><li>超平面对于<strong>样本点</strong>$(x_i,y_i)$的<strong>函数间隔</strong>：$\hat{\gamma}_{i}=y_{i}\left(w \cdot x_{i}+b\right)$</li><li>超平面对于<strong>数据集</strong>的函数间隔：$\hat{\gamma}=\min  \hat{\gamma}_{i}$</li><li>超平面对于<strong>样本点</strong>$(x_i,y_i)$的<strong>几何间隔</strong>：$\gamma_{i}=y_i\frac{1}{|w|}\left(w \cdot x_{i}+b\right)$</li><li>超平面对于<strong>数据集</strong>的几何间隔：$\gamma=min{ \gamma_{i}}$</li></ul></li></ul><h3><span id="22-间隔最大化">2.2 间隔最大化</span></h3><ul><li><p>最大间隔分离超平面可以表示为约束优化问题：</p><ul><li>最大间隔求分离超平面</li></ul><script type="math/tex; mode=display">\max _{w, b}\gamma</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(\frac{w}{\|w\|} \cdot x_{i}+\frac{b}{\|w\|}\right) \geqslant \gamma, \quad i=1,2, \cdots, N</script><ul><li><p>将几何间隔改为函数间隔：</p><script type="math/tex; mode=display">\max _{w, b} \frac{\hat{\gamma}}{\|w\|}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant \hat{\gamma}, \quad i=1,2, \cdots, N</script></li><li><p><strong>将函数间隔固定为1，最大化改为最小化（得到凸二次规划问题）</strong></p><script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2}</script></li></ul></li></ul><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right)\geqslant 1, \quad i=1,2, \cdots, N</script><h3><span id="23-学习的对偶算法">2.3 学习的对偶算法</span></h3><ul><li><p>拉格朗日对偶问题</p><ul><li><p>转化为最小约束的问题：</p><script type="math/tex; mode=display">\min _{w, b} \frac{1}{2}\|w\|^{2}</script><script type="math/tex; mode=display">\text { s.t. } \quad y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1, \quad i=1,2, \cdots, N</script></li><li><p><strong>拉格朗日乘子法（带约束的问题转为无约束的问题）</strong>，这也是<strong>损失函数</strong><br>在这里，对于系数$\alpha$，非支持向量的系数为0，支持向量的系数大于0</p></li><li><script type="math/tex; mode=display">L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(w \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i}</script></li></ul></li></ul><ul><li><p>对偶问题求解 —— 求$\min_{w,b} L(w,b,a)$  - 原始问题的对偶问题极大极小问题，后面两部分始终小于0</p><ul><li><p>对w、b求偏导：</p><script type="math/tex; mode=display">\begin{array}{l}{\nabla_{w} L(w, b, \alpha)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0} \\ {\nabla_{b} L(w, b, \alpha)=\sum_{i=1}^{N} \alpha_{i} y_{i}=0}\end{array}</script><p>得到：</p><script type="math/tex; mode=display">\begin{array}{l}{w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}} \\ {\sum_{i=1}^{N} \alpha_{i} y_{i}=0}\end{array}</script></li><li><p>将其带入拉格朗日函数</p><script type="math/tex; mode=display">\begin{aligned} L(w, b, \alpha) &=\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i} y_{i}\left(\left(\sum_{j=1}^{N} \alpha_{j} y_{j} x_{j}\right) \cdot x_{i}+b\right)+\sum_{i=1}^{N} \alpha_{i} \\ &=-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i} \end{aligned}</script></li></ul></li><li><p>对偶问题求解 —— 求$\min_{w,b} L(w,b,a)$对a的极大</p><ul><li><p><strong>对偶问题为</strong></p><script type="math/tex; mode=display">\begin{array}{ll}{\max _{\alpha}} & {-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script></li><li><p><strong>极大转换成极小</strong></p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{\alpha}} & {\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)-\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {\alpha_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script></li><li><p>假设有解：$\mathrm{a}^{<em>}=\left(\alpha_{1}^{</em>}, \alpha_{2}^{<em>}, \ldots, \alpha_{N}^{</em>}\right)^{\mathrm{T}}$，则最优化的解为：</p><script type="math/tex; mode=display">\begin{array}{c}{w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}} \\ {b^{*}=y_{j}-\sum_{i=1}^{N} \alpha_{i}^{*} y_{i}\left(x_{i} \cdot x_{j}\right)}\end{array}</script></li><li><p>并可以证明满足KKT的条件。</p></li><li><p>补充：极小问题满足二次规划问题，通过代入数据集后求解a从而求得w和b</p></li></ul></li></ul><h2><span id="3-线性不可分支持向量机与软间隔">3 线性(不可分)支持向量机与软间隔</span></h2><h3><span id="31-引入软间隔后的问题">3.1 引入软间隔后的问题</span></h3><ul><li><p>引入松弛变量后的约束</p><script type="math/tex; mode=display">y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}</script></li><li><p>引入松弛变量后的目标函数</p><script type="math/tex; mode=display">\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}</script></li><li><p>线性不可分的学习问题变为如下凸二次规划问题：</p><script type="math/tex; mode=display">\begin{array}{cl}{\min _{w, b, \xi}} & {\frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}} \\ {\text { s.t. }} & {y_{i}\left(w \cdot x_{i}+b\right) \geqslant 1-\xi_{i}, \quad i=1,2, \cdots, N} \\ {} & {\xi_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script></li></ul><h3><span id="32-对偶问题与求解">3.2 对偶问题与求解</span></h3><ul><li><p>拉格朗日函数为</p><script type="math/tex; mode=display">L(w, b, \xi, \alpha, \mu) \equiv \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{N} \xi_{i}-\sum_{i=1}^{N} \alpha_{i}\left(y_{i}\left(w \cdot x_{i}+b\right)-1+\xi_{i}\right)-\sum_{i=1}^{N} \mu_{i} \xi_{i}</script></li><li><p>对偶问题求解 —— 求$\min_{w,b,\xi} L(w, b, \xi, \alpha, \mu)$</p><ul><li><p>对各变量的偏导数</p><script type="math/tex; mode=display">\begin{array}{l}{\nabla_{w} L(w, b, \xi, \alpha, \mu)=w-\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}=0} \\ {\nabla_{b} L(w, b, \xi, \alpha, \mu)=-\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {\nabla_{\xi_{i}} L(w, b, \xi, \alpha, \mu)=C-\alpha_{i}-\mu_{i}=0}\end{array}</script></li><li><p>偏导数为0的结果</p><script type="math/tex; mode=display">\begin{array}{c}{w=\sum_{i=1}^{N} \alpha_{i} y_{i} x_{i}} \\ {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {C-\alpha_{i}-\mu_{i}=0}\end{array}</script></li></ul></li><li><p>对偶问题求解 —— 对$\min_{w,b,\xi} L(w, b, \xi, \alpha, \mu)$求a的极大</p></li></ul><script type="math/tex; mode=display">\begin{array}{ll}{\max _{\alpha}} & {-\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i} \alpha_{j} y_{i} y_{j}\left(x_{i} \cdot x_{j}\right)+\sum_{i=1}^{N} \alpha_{i}} \\ {\text { s.t. }} & {\sum_{i=1}^{N} \alpha_{i} y_{i}=0} \\ {} & {C-\alpha_{i}-\mu_{i}=0} \\ {} & {\alpha_{i} \geqslant 0} \\ {} & {\mu_{i} \geqslant 0, \quad i=1,2, \cdots, N}\end{array}</script><ul><li>利用等式约束消去u，最终的结果</li></ul><script type="math/tex; mode=display">\begin{array}{c}{w^{*}=\sum_{i=1}^{N} \alpha_{i}^{*} y_{i} x_{i}} \\ {b^{*}=y_{j}-\sum_{i=1}^{N} y_{i} \alpha_{i}^{*}\left(x_{i} \cdot x_{j}\right)}\end{array}</script><ul><li>并可以证明满足KKT的条件。</li></ul><h2><span id="4-非线性支持向量机与核函数">4 非线性支持向量机与核函数</span></h2><h3><span id="41-核函数">4.1 核函数</span></h3><ul><li>非线性带来高维转换（高维空间更容易线性可分）</li><li>对偶表示带来内积</li><li>核函数的定义</li></ul><script type="math/tex; mode=display">\phi(x): \mathcal{X} \rightarrow \mathcal{H}</script><script type="math/tex; mode=display">K(x, z)=\phi(x) \cdot \phi(z)</script><h3><span id="42-正定核">4.2 正定核</span></h3><ul><li>定义映射并构成向量空间S</li><li>在S上定义内积，构成内积空间</li><li>将S完备化构成希尔伯特空间</li></ul><h3><span id="43-常用的核函数">4.3 常用的核函数</span></h3><ul><li>多项式核函数</li><li>高斯核函数</li><li>字符串核函数</li></ul><h2><span id="reference">reference：</span></h2><pre><code>《统计学习方法》《机器学习》——西瓜书Bilibili白板推导</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器学习/概率图模型-HMM</title>
      <link href="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/"/>
      <url>/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-马尔可夫模型">1 马尔可夫模型</span></h2><h3><span id="11-概念导入">1.1 概念导入</span></h3><p>在某段时间内，交通信号灯的颜色变化序列是：红色 - 黄色 - 绿色 - 红色。</p><p>在某个星期天气的变化状态序列：晴朗 - 多云 - 雨天。</p><p>像交通信号灯一样，某一个状态只由前一个状态决定，这就是一个一阶马尔可夫模型。而像天气这样，天气状态间的转移仅依赖于前n天天气的状态，即状态间的转移仅依赖于前n个状态的过程。这个过程就称为<strong>n阶马尔科夫模型</strong>。</p><p>不通俗的讲，马尔可夫模型（Markovmodel）描述了一类重要的随机过程，随机过程又称随机函数，是随时间而随机变化的过程。</p><h3><span id="12-马尔可夫模型定义">1.2 马尔可夫模型定义</span></h3><p>存在一类重要的随机过程：如果一个系统有N个状态$S_1$,$S_2$,$S_3$..$S_N$ 随着时间的推移，该系统从某一状态转移到另一状态。如果用$q_t$ 表示系统在时间t的状态变量，那么 t 时刻的状态取值为$S_j$(1&lt;=j&lt;=N)的概率取决于前t-1 个时刻(1, 2, …, t-1)的状态，该概率为：</p><script type="math/tex; mode=display">p\left(q_{t}=S_{j} | q_{t-1}=S_{i}, q_{t-2}=S_{k}, \cdots\right)</script><ol><li><strong>假设一：</strong>如果在特定情况下，系统在时间t 的状态只与其在时间t-1 的状态相关，则该系统构成一个<strong>离散的一阶马尔可夫链</strong>：</li></ol><script type="math/tex; mode=display">p\left(q_{t}=S_{j} | q_{t-1}=S_{i}, q_{t-2}=S_{k}, \cdots\right)=p\left(q_{t}=S_{j} | q_{t-1}=S_{i}\right)</script><ol><li><strong>假设二：</strong>如果只考虑独立于时间t的随机过程，状态与时间无关，那么<script type="math/tex; mode=display">p\left(q_{t}=S_{j} | q_{t-1}=S_{i}\right)=a_{i j}, \quad 1 \leq i, j \leq N</script>即：t时刻状态的概率取决于前t-1 个时刻(1, 2, …, t-1)的状态,且状态的转换与时间无关，则<strong>该随机过程</strong>就是<strong>马尔可夫模型</strong>。</li></ol><h2><span id="2-隐马尔可夫模型">2 隐马尔可夫模型</span></h2><h3><span id="21-概念导入">2.1 概念导入</span></h3><p>在马尔可夫模型中，每个状态代表了一个可观察的事件，所以，马尔可夫模型有时又称作可视马尔可夫模型（visibleMarkovmodel，VMM），这在某种程度上限制了模型的适应性。</p><p>对于盲人来说也许不能够直接获取到天气的观察情况，但是他可以通过触摸树叶通过树叶的干燥程度判断天气的状态。于是天气就是一个隐藏的状态，树叶的干燥程度是一个可观察的状态，于是我们就有了两组状态，一个是不可观察、隐藏的状态（天气），一个是可观察的状态（树叶），我们希望设计一种算法，在不能够直接观察天气的情况下，通过树叶和马尔可夫假设来预测天气。</p><p>以此为例，一个一阶的马尔可夫过程描述：  </p><p> <img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/clip_image004-1573140263848.jpg" alt="clip_image004" style="zoom:67%;"></p><p>在隐马尔可夫模型（HMM）中，我们<strong>不知道模型具体的状态序列</strong>，<strong>只知道状态转移的概率</strong>，即模型的状态转换过程是不可观察的。</p><p>因此，该模型是一个<strong>双重随机过程</strong>，包括<strong>模型的状态转换</strong>和<strong>特定状态下可观察事件的随机</strong>。</p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1.png" alt="1" style="zoom: 50%;"></p><h3><span id="22-hmm的组成">2.2 HMM的组成</span></h3><p>例如，N个袋子，每个袋子中有M种不同颜色的球。选择一个袋子，取出一个球，得到球的颜色。</p><ol><li>状态数为N(袋子的数量)</li><li>每个状态可能的符号数M(不同颜色球的数目)</li><li>状态转移概率矩阵A＝$a_{ij}$(从一只袋子(状态Si) 转向另一只袋子(状态Sj ) 取球的概率)</li><li>从状态Sj 观察到某一特定符号vk 的概率分布矩阵为：$B=\betaj(k)$  (从第j个袋子中取出第k种颜色的球的概率)</li><li>初始状态的概率分布为：$\pi=\pi_{i}$</li></ol><p><strong>一般将一个隐马尔可夫模型记为：$λ=[π, A,B]$</strong></p><p>需要确定以下三方面内容（三要素）：</p><ol><li>初始状态概率π：模型在初始时刻各状态出现的概率，通常记为$π=(π_1,π_2,…,π_N)$，$π_i$表示模型的初始状态为$S_i$的概率.</li><li>状态转移概率A：模型在各个状态间转换的概率，通常记为矩阵A[$a_{ij}$]，其中$a_{ij}$表示在任意时刻t，若状态为Si，则在下一时刻状态为Sj的概率.</li><li>输出观测概率B：模型根据当前状态获得各个观测值的概率通常记为矩阵<br>B=[$(\beta{ij})$]。其中，$\beta{ij}$表示在任意时刻t，若状态为$S_j$，则观测值$O_j$被获取的概率.</li></ol><p>相对于马尔可夫模型，隐马尔可夫只是多了一个各状态的观测概率</p><p>给定隐马尔可夫模型  $λ=[A, B, π]$，它按如下过程产生观测序列   ${X_1,X_2，…,X_n}$:</p><p>(1)  设置t=1，并根据初始状态概率π选择初始状态$Y_1$;</p><p>(2)  根据状态值和输出观测概率B选择观测变量取值$X_t$ ;</p><p>(3)  根据状态值和状态转移矩阵A转移模型状态，即确定$Y_{t+1}$;</p><h2><span id="3-三个问题">3 三个问题</span></h2><p>一旦一个系统可以作为HMM被描述，就可以用来解决三个基本问题。</p><p><strong>1.</strong>  <strong>评估（Evaluation）</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，求某个观察序列的概率$p(x| \lambda)$。</p><p>例如：给定一个天气的隐马尔可夫模型，包括第一天的天气概率分布，天气转移概率矩阵，特定天气下树叶的湿度概率分布。<strong>求第一天湿度为1，第二天湿度为2，第三天湿度为3的概率。</strong></p><p><strong>2.</strong>  <strong>解码 or 推断（ Decoding）</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，以及某个观察序列，求得天气的序列y。</p><p>例如：给定一个天气的隐马尔可夫模型，包括第一天的天气概率分布，天气转移概率矩阵，特定天气下树叶的湿度概率分布。并且已知第一天湿度为1，第二天湿度为2，第三天湿度为3。<strong>求得这三天的天气情况</strong>。</p><p> 即：发现“最优”状态序列能够“最好地解释”观察序列</p><p><strong>3.</strong>  <strong>学习（Learning）</strong></p><p>给定一个观察序列，得到一个隐马尔可夫模型$\lambda=[π, A,B]$。</p><p>例如：已知第一天湿度为1，第二天湿度为2，第三天湿度为3。<strong>求得一个天气的隐马尔可夫模型</strong>，包括第一天的天气，天气转移概率矩阵，特定天气下树叶的湿度概率分布。</p><h3><span id="31-前向算法">3.1 前向算法</span></h3><p><strong>对于评估问题（Evaluation）</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，求某个观察序列的概率。</p><p>例如：给定一个天气的隐马尔可夫模型，包括第一天的天气概率分布，天气转移概率矩阵，特定天气下树叶的湿度概率分布。<strong>求第一天湿度为1，第二天湿度为2，第三天湿度为3的概率。</strong></p><p><strong>思路一：找到所有状态序列，得到各状态概率，得到每种状态概率对应的观察概率，求和。</strong> </p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/clip_image006.png" alt="clip_image006" style="zoom:67%;"></p><p>即：找到每一个可能的隐藏状态，并且将这些隐藏状态下的观察序列概率相加。</p><p>对于上面那个（天气）例子，将有3^3 = 27种不同的天气序列可能性，因此，观察序列的概率是：Pr(dry,damp,soggy | HMM) = Pr(dry,damp,soggy | sunny,sunny,sunny) + Pr(dry,damp,soggy | sunny,sunny ,cloudy) + Pr(dry,damp,soggy | sunny,sunny ,rainy) + . . . . Pr(dry,damp,soggy | rainy,rainy ,rainy)</p><p>用这种方式计算观察序列概率极为昂贵，特别对于大的模型或较长的序列，因此我们可以利用这些概率的时间不变性来减少问题的复杂度。</p><p><strong>思路二：采用动态规划——前向算法</strong></p><p> 基本思想：定义前向变量$α_t(i)$  ：t时刻状态为$S_i$且观察状态为$O_t$ 的概率</p><script type="math/tex; mode=display">\alpha_{t}(i)=p\left(O_{1} O_{2} \cdots \underline{O_{t}}, q_{t}=S_{i} | \lambda\right)</script><p> 如果可以高效地计算$α_t(i)$，就可以高效地求得$p(O|\lambda)$。</p><p> <img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571903774343.png" alt="1571903774343" style="zoom:50%;"></p><p>蓝色部分是：从t时刻的各个状态$S_i$得到t时刻的观察状态$O_t$的概率</p><p>黑色部分是：从t时刻的各个状态$S_i$得到t+1时刻的各个状态</p><p> 即：</p><ol><li>初始化：$\alpha_{1}(i)=\pi_{i} \beta_{i}\left(O_{1}\right), \quad 1 \leq i \leq N$</li><li>循环计算：$\alpha_{t+1}(j)=\left[\sum_{i=1}^{N} \alpha_{t}(i) a_{i j}\right] \times \beta{j}\left(O_{t+1}\right), \quad 1 \leq t \leq T-1$</li><li>结束，输出：$p(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$</li></ol><h3><span id="32-后向算法">3.2 后向算法</span></h3><p>定义后向变量$\beta_t(i)$是在给定了模型 $\lambda=[π, A,B]$和假定在时间t 状态为$S_i$的条件下，模型输出<br>观察序列$O_{t+1} O_{t+2} \cdots {O_{T}}$ 的概率：</p><script type="math/tex; mode=display">\beta_{t}(i)=p\left(O_{t+1} O_{t+2} \cdots O_{T} | q_{t}=S_{i}, \mu\right)</script><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571904725005.png" alt="1571904725005" style="zoom:50%;"></p><p>蓝色部分是：从t+1时刻的各个状态$S_i$得到t+1时刻的观察状态$O_t$的概率</p><p>黑色部分是：从t+1时刻的各个状态$S_i$得到t时刻的各个状态</p><p> 即：</p><ol><li>初始化：$\beta_{T}(i)=1, \quad 1 \leq i \leq N$</li><li>循环计算：$\beta_{t}(i)=\sum_{j=1}^{N} a_{i j} \beta{j}\left(O_{t+1}\right) \times \beta_{t+1}(j), \quad T-1 \geq t \geq 1, \quad 1 \leq i \leq N$</li><li>结束，输出：$p(O | \lambda)=\sum_{i=1}^{N} \beta_{1}(i) \times \pi_{i} \times \beta{i}\left(O_{1}\right)$</li></ol><h3><span id="33-viterbi-搜索算法">3.3 Viterbi 搜索算法</span></h3><p><strong>对于解码问题（ Decoding）</strong></p><p>给定HMM，即$\lambda=[π, A,B]$，以及某个观察序列，求得天气的序列。</p><p>例如，给定一个天气的隐马尔可夫模型，包括第一天的天气，天气转移概率矩阵，特定天气下树叶的湿度概率分布。并且已知第一天湿度为1，第二天湿度为2，第三天湿度为3。求得这三天的天气情况。</p><p> 即：发现“最优”状态序列能够“最好地解释”观察序列</p><h4><span id="331-如何理解最优的状态序列">3.3.1 如何理解“最优”的状态序列？</span></h4><p>解释(1)：</p><p>状态序列中的每个状态都单独地具有概率，对于每个时刻$t(1 \leq t \leq T)$, 寻找$q_t$ 使得$\gamma_{t}(i)=p\left(q_{t}=S_{i} | O, \lambda\right)$最大。——<strong>近似算法</strong></p><p>问题：每一个状态单独最优不一定使整体的状态序列最优，两个最优的状态之间的转移概率可能为0</p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905343398.png" alt="1571905343398" style="zoom:50%;"></p><p>解释(2)：在给定模型$\lambda$和观察序列O的条件下求概率最大的状态序列<strong>——Viterbi 算法: 动态搜索最优状态序</strong></p><script type="math/tex; mode=display">\widehat{Q}=\underset{Q}{\arg \max } p(Q | O, \mu)</script><h4><span id="332-viterbi-搜索算法过程">3.3.2 Viterbi 搜索算法过程</span></h4><p>搜索过程大概如下：</p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905575248.png" alt="1571905575248" style="zoom:50%;"></p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905516366.png" alt="1571905516366" style="zoom:50%;"></p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905527009.png" alt="1571905527009" style="zoom:50%;"></p><p><img src="/2019/11/28/ji-qi-xue-xi/gai-lu-tu-mo-xing-hmm/1571905543642.png" alt="1571905543642" style="zoom:50%;"></p><p>每个时刻都求得每个状态的概率，并求得最大概率对应的上一时刻的状态。</p><p><strong>算法描述</strong></p><ol><li><p>初始化：$\delta_{1}(i)=\pi_{i} \beta_{i}\left(O_{1}\right), \quad 1 \leq i \leq N$</p><p>概率最大的路径变量：$\psi_{1}(i)=0$</p></li><li><p>递推计算：$t$ 是计算的顺序数（计算第 t 个），$j$  是计算过程中的第 j 个状态</p><p>$\delta_{t}(j)=\max _{1 \leq i \leq N}\left[\delta_{t-1}(i) \cdot a_{i j}\right] \cdot \beta_{j}\left(O_{t}\right), \quad 2 \leq t \leq T, \quad 1 \leq j \leq N$</p><p>$\psi_{t}(j)=\underset{1 \leq i \leq N}{\arg \max }\left[\delta_{t-1}(i) \cdot a_{i j}\right] \cdot \beta_{j}\left(O_{t}\right), 2 \leq t \leq T, 1 \leq i \leq N$</p></li><li><p>结束：</p><p>$\widehat{Q}_{T}=\underset{1 \leq i \leq N}{\operatorname{argmax}}\left[\delta_{T}(i)\right], \quad \widehat{p}\left(\widehat{Q}_{T}\right)=\max _{1 \leq i \leq N} \delta_{T}(i)$</p></li><li><p>通过回溯得到路径（状态序列）：</p><p>$\widehat{q}_{t}=\psi_{t+1}\left(\widehat{q}_{t+1}\right), \quad t=T-1, T-2, \cdots, 1$</p></li></ol><h3><span id="34-参数学习">3.4 参数学习</span></h3><p><strong>对于学习问题（Learning）</strong></p><p>给定一个观察序列，得到一个隐马尔可夫模型。</p><p>已知第一天湿度为1，第二天湿度为2，第三天湿度为3。求得一个天气的隐马尔可夫模型，包括第一天的天气，天气转移概率矩阵，特定天气下树叶的湿度概率分布。</p><p>如果产生观察序列O的状态已知(即存在大量标注的样本), 可以用最大似然估计来计算 $\lambda$ 的参数：Baum-Welch 算法(前向后向算法)描述</p><p>如果不存在大量标注的样本：EM算法—期望值最大化算法(Expectation-Maximization, EM) </p><h2><span id="4-hmm应用">4 HMM应用</span></h2><p>中文分词问题，从句子的序列标注问题解决。BMES</p><p>如果有标注语料，则问题的解决过程：</p><ol><li>计算初始状态概率分布（初始字符的BMES概率）</li><li>计算转移概率矩阵（BMES之间转移的概率）</li><li>计算输出概率矩阵（BMES转为字符的概率）</li><li>使用Viterbi算法解码</li></ol><p>如果没有标注语料，则问题的解决过程：</p><ol><li>获取词的个数</li><li>确定状态的个数</li><li>参数学习（利用EM迭代算法获取初始状态概率、状态转移概率和输出概率）</li><li>使用Viterbi算法解码</li></ol><h2><span id="5-reference">5 Reference</span></h2><p>宗成庆：《自然语言处理》讲义</p><p>《统计学习方法》</p><p>其他网络资料（侵删）</p>]]></content>
      
      
      <categories>
          
          <category> machine-learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 概率图模型-HMM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer的简洁向解释</title>
      <link href="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/"/>
      <url>/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p><img src="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574773800747.png" alt="1574773800747" style="zoom:67%;"></p><h2><span id="1-主要结构">1 主要结构</span></h2><p>​        <strong>1. 输入：Input Embedding——$(x_1,x_2,x_3,…,x_n)$ 进行Positional Encoding，投入Encoder；</strong></p><p>​        <strong>2. Encoder：编码处理后输出——$(z_1,z_2,z_3,…,z_n)$，并将其作为Decoder的输入；</strong></p><p>​        <strong>3. Decoder：进行解码处理；</strong></p><p>​        <strong>4. 输出：最终对Decoder的输出进行处理最终的概率$(y_1,y_2,y_3,…,y_m)$</strong></p><p>​        <strong>注意！</strong>Encoder和Decoder都是并行计算的N（论文取N=6）个相同结构的堆叠。</p><h2><span id="2-encoder">2 Encoder</span></h2><h3><span id="21-encoder整体">2.1 Encoder整体</span></h3><p><img src="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574779976637.png" alt="1574779976637" style="zoom:33%;"></p><p>​        Encoder部分有两个sub-layer，<strong>Multi-Head Attention</strong>和<strong>Feed Forward</strong>。上图的Add代表残差网络，由图可知在每个sub-layer都加入了残差项。其中的Norm是Layer Normalization。</p><p>​        Multi-Head Attention是本论文的核心，主要是self Attention；Feed Forward是一个简单的全连接前馈神经网络。</p><h3><span id="22-encoder的输入">2.2 Encoder的输入</span></h3><p>​        Encoder端每个大模块接收的输入是不一样的，第一个大模块(最底下的那个)接收的输入是输入序列的embedding，其余大模块接收的是其前一个大模块的输出，最后一个模块的输出作为整个Encoder端的输出。  </p><h3><span id="23-multi-head-attention">2.3 Multi-Head Attention</span></h3><p><img src="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574775307731.png" alt="1574775307731" style="zoom: 33%;"></p><p>​        </p><p>​        对于self-attention来讲，Q(Query), K(Key), V(Value)三个矩阵均来自同一输入。单个Multi-Head Attention层的输入进行处理得到QKV，通过线性变换输入到Scaled Dot-Product Attention，得到多组结果进行concat并加权后，作为Encoding后的结果。（QKV是什么见下一小节）</p><p>​        整个过程的公式：</p><script type="math/tex; mode=display">\begin{aligned} \text { MultiHead }(Q, K, V) &\left.=\text { Concat (head }_{1}, \ldots, \text { head }_{\mathrm{h}}\right) W^{O} \\ \text { where head }_{\mathrm{i}} &=\text { Attention }\left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right) \end{aligned}</script><p>​        </p><h3><span id="24-scaled-dot-product-attention">2.4 Scaled Dot-Product Attention</span></h3><p><img src="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574777556699.png" alt="1574777556699" style="zoom:33%;"></p><p>​        Scaled Dot-Product Attention是Transformer较为重要的核心部分，整个过程：</p><script type="math/tex; mode=display">\text { Attention }(Q, K, V)=\operatorname{softmax}\left(\frac{Q K^{T}}{\sqrt{d_{k}}}\right) V</script><ol><li>Q和K计算相似度，此处使用点积的方法；</li><li>防止结果过大，除以${\sqrt{d_{k}}}$，其中${\sqrt{d_{k}}}$是K的维度；</li><li>经过一个Mask操作； Q，K长度是不定时，进行补齐操作，将补齐的数据设置为负无穷</li><li>进行softmax归一化得到Q和K的Attention；</li><li>Attention与V相乘，得到self-Attention的结果。</li></ol><p>​        这里的QKV论文里没有详细展开，现有的博客文章很少提到，本文引用的第二篇博客中有段话写的很好。Attention机制中，将Source中看做是由一系列的(Key,Value)对构成，此时给定某元素Query，计算Query和各个Key的相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</p><p><img src="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/att7.png" alt="image" style="zoom:33%;"></p><p>​        而对于本文的self-Attention来说，key、value和query都是其本身，也就是上一层的输出，作为下一层的输入。</p><h2><span id="3-decoder">3 Decoder</span></h2><p>​        Decoder部分有三个sub-layer，<strong>Masked Multi-head Attention</strong>、<strong>Encoder-Decoder Attention</strong>和<strong>Feed Forward</strong>。上图的Add和Norm与Encoder部分一致，分别代表残差网络和Layer Normalization。</p><p>​        decoder相对encoder，有两个不同的地方，一是第一级的Masked Multi-head，二是第二级中Multi-Head Attention的输入。</p><p><img src="/2019/11/28/zi-ran-yu-yan-chu-li-ji-chu/transformer-de-jian-ji-xiang-jie-shi/1574780008243.png" alt="1574780008243" style="zoom:33%;"></p><p>​    </p><h3><span id="31-masked-multi-head-attention">3.1 Masked Multi-head Attention</span></h3><p>​        Masked Multi-head是decoder的第一级decoder，其key, query, value均来自前一层decoder的输出。</p><p>​        <strong>但其加入了Mask操作，因为翻译过程我们当前还并不知道下一个输出哪个词语。</strong></p><p>​        训练过程中，因为在实现中无法每次动态的输入，就一次性把目标序列通通输入第一个大模块中，然后在Multi-head Attention中对序列进行mask即可。</p><p>​        在测试过程中，先生成第一个位置的输出，第二次预测时，再将其加入输入序列，以此类推直至预测结束。</p><h3><span id="32-encoder-decoder-attention-attention">3.2 Encoder-Decoder attention Attention</span></h3><p>​        Decoder中第二级decoder是Multi-Head Attention，论文中也称为”encoder-decoder attention”。它不仅接受来自前一级的输出，还接收encoder的输出。</p><p>​        它的query来自于之前一级的decoder层的输出，但其key和value来自于encoder的输出，这使得decoder的每一个位置都可以attend到输入序列的每一个位置。</p><p>​        总结一下，k和v的来源总是相同的，Q在encoder及第一级decoder中与K，V来源相同，在encoder-decoder attention layer中与K,V来源不同。</p><h2><span id="4-其他细节">4 其他细节</span></h2><h3><span id="41-position-wise-feed-forward-networks">4.1 Position-wise Feed-Forward Networks</span></h3><p>​        除了Attention子层之外，其他子层都包含了一个全连接的前馈网络。包括两个线性变换，中间有一个ReLU：</p><script type="math/tex; mode=display">\mathrm{FFN}(x)=\max \left(0, x W_{1}+b_{1}\right) W_{2}+b_{2}</script><h3><span id="42-positional-encoding">4.2 Positional Encoding</span></h3><p>​        模型结构本质上是忽视了数据的序列信息，因此为了充分利用序列信息，将tokens的相对和绝对位置编码进输入Embedding。</p><p>​        论文中使用sine and cosine的方式计算出来（称其效果与训练出来的比较接近），且结果与embedding维度相同。然后将Positional Encoding与embedding 进行加和。</p><script type="math/tex; mode=display">\begin{array}{c}{P E_{(p o s, 2 i)}=\sin \left(\text {pos} / 10000^{2 i / d_{\text {madel }}}\right)} \\ {P E_{(\text {pos}, 2 i+1)}=\cos \left(\text {pos} / 10000^{2 i / d_{\text {madit }}}\right)}\end{array}</script><h2><span id="5-常见问题总结">5 常见问题总结</span></h2><ul><li>Transformer的结构是什么样的？<br>Transformer本身还是一个典型的encoder-decoder模型：<ol><li>Encoder由N(原论文中<strong>N=6</strong>)个相同的大模块堆叠而成，其中每个大模块又由<strong>两个sub-layer</strong>构成，这两个子模块分别为Multi-Head Attention模块，以及一个前馈神经网络模块；</li><li>Decoder端同样由N个相同的大模块堆叠而成，其中每个大模块则由<strong>三个sub-layer</strong>构成，这三个子模块分别为Masked Multi-Head Attention模块，本质也是Multi-Head Attention的encoder-decoder attention模块，以及一个前馈神经网络模块；  </li><li>在所有sub-layer中，都附加了残差网络和layer normalization。</li></ol></li><li><p>Multi-Head Attention的具体结构？<br>Multi-Head Attention的结果是QKV进行不同的变换后，再进行self-Attention后进行concat得到的。</p></li><li><p>self-Attention具体内容？<br>Q和K计算相似度，此处使用点积的方法；防止结果过大，除以${\sqrt{d_{k}}}$，其中${\sqrt{d_{k}}}$是K的维度；进行softmax归一化得到Q和K的Attention；Attention与V相乘，得到self-Attention的结果。</p></li><li>Self-Attention为什么work？<br>self-attention的特点在于<strong>无视词(token)之间的距离直接计算依赖关系，从而能够学习到序列的内部结构</strong></li><li>Transformer相比于RNN/LSTM，有什么优势？为什么？<br>RNN由于序列依赖的原因，并行计算能力比较差；Transformer的特征提取能力比RNN系列强。</li><li>Transformer与Seq2Seq相比？<br>seq2seq最大的问题在于<strong>将Encoder端的所有信息压缩到一个固定长度的向量中</strong>，并将其进行解码。在输入信息较长时，会损失大量的信息。而且这样整体交给Decoder进行编码，Decoder很难关注到真正要关注的信息。</li><li>Transformer是如何加入词序信息的？<br>模型结构本质上是忽视了数据的序列信息，因此为了充分利用序列信息，将tokens的相对和绝对位置编码进输入Embedding。论文中使用sine and cosine的方式计算出来，且结果与embedding维度相同。然后将Positional Encoding与embedding 进行加和。</li></ul><h2><span id="6-参考文章">6 参考文章</span></h2><p><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">https://arxiv.org/abs/1706.03762</a></p><p><a href="https://zhuanlan.zhihu.com/p/47063917" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/47063917</a></p><p><a href="https://www.cnblogs.com/huangyc/p/10409626.html" target="_blank" rel="noopener">从Encoder-Decoder(Seq2Seq)理解Attention的本质</a></p><p><a href="https://www.nowcoder.com/discuss/258321?type=0&amp;order=0&amp;pos=19&amp;page=1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/258321?type=0&amp;order=0&amp;pos=19&amp;page=1</a></p>]]></content>
      
      
      <categories>
          
          <category> nlp基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer的简洁向解释 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++常用API</title>
      <link href="/2019/09/03/yu-yan-gong-ju-ji-zhu-deng-wen-dang/c-chang-yong-api/"/>
      <url>/2019/09/03/yu-yan-gong-ju-ji-zhu-deng-wen-dang/c-chang-yong-api/</url>
      
        <content type="html"><![CDATA[<p>arrary</p><pre><code>int number[5];//= {1,2,3,4,5};    //数组初始化int *number=new int[5];        //数组初始化swap(number[i],number[j]);</code></pre><p>include<algorithm></algorithm></p><pre><code>sort(array,array+length);     //0-(length-1)数组排序bool cmp(const int &amp;a,const int &amp;b) return a&gt;b;   //没有官方封装的快sort(array,array+length,cmp);  //0-(length-1)数组排序-降序</code></pre><h1><span id="include">include<math></math></span></h1><pre><code>fabs(number); //绝对值,abs只针对整数sqt(number); //开方</code></pre><h1><span id="include">include<string></string></span></h1><pre><code>str.size();  str.length();memset(a,0,sizeof(a));string subS=str.substr(begin_index,length);  //获取子串string subS=str.substr(begin_index);</code></pre><h1><span id="include">include<vector></vector></span></h1><pre><code>//初始化vectorvector&lt;int&gt; v;  vector&lt;int&gt; v(n+1,-1); //n+1个数，初始化-1vector&lt;int&gt;({1,2,3,4});//直接加入元素vector&lt;int&gt;(); //返回一个空的容器vector&lt;vector&lt;int&gt;&gt; v;vector&lt;vector&lt;int&gt;&gt; v(n,vector&lt;int&gt;(n,0));v.size();    v[0].size();v.empty();  v.push_back(x);v.pop_back(x);  v.clear(); //清空元素，但不回收空间swap(v[i],v[j]);sort(result.begin(),result.end());  //排列</code></pre><h1><span id="include">include<stack></stack></span></h1><pre><code>stack&lt;int&gt; s;s.empty();s.push(value);s.top();s.pop();</code></pre><h1><span id="include">include<queue></queue></span></h1><pre><code>queue&lt;int&gt; q;q.empty();q.push(value);q.front();s.pop();</code></pre><h1><span id="include">include<map></map></span></h1><pre><code>map&lt;int,int&gt; m;   //初始化m[key]=val;     //赋值m.erase(key);   m.erase(iter);   //去掉-key或指针for(auto iter=m.begin();iter!=m.end();iter++){    cout &lt;&lt; iter-&gt;first &lt;&lt; &quot; : &quot; &lt;&lt; iter-&gt;second &lt;&lt; endl; //注意是&quot;-&gt;&quot;不是&quot;.&quot;&quot;}//map中的元素按照key顺序排列，在对顺序有要求的问题中使用map，查找速度上慢于哈希实现的unordered_map</code></pre><h1><span id="include">include<priority_queue></priority_queue></span></h1><pre><code>priority_queue&lt;int&gt; Q;  //大顶堆priority_queue&lt;int,vector&lt;int&gt;,less&lt;int&gt;&gt; Q;  //大顶堆priority_queue&lt;int,vector&lt;int&gt;,greater&lt;int&gt;&gt; Q;//小顶堆priority_queue&lt;pair&lt;int,int&gt;&gt; pQ;    //pairpQ.push(make_pair(value1,value2));Q.top();Q.empty();Q.size();Q.pop();</code></pre><h1><span id="include">include<set></set></span></h1><pre><code>unordered_set&lt;int&gt; hash;unordered_set &lt;int&gt; hash(_vector.begin(), _vector.end());hash.find(val)!=hash.end();</code></pre><h1><span id="include">include<list></list></span></h1><pre><code>list&lt;int&gt; l;l.begin();  l.end();l.size();list&lt;int&gt;::iterator current=l.begin();l.erase(current);l.push_back(val);</code></pre><h1><span id="include-双向队列">include<deque>  //双向队列</deque></span></h1><pre><code>deque&lt;int&gt; q;q.push_back(value);q.empty();q.front();q.back();s.pop();</code></pre><h1><span id="include-自动排序的set">include<multiset>  //自动排序的set</multiset></span></h1><pre><code>multiset&lt;int, greater&lt;int&gt; &gt; leastNums;  //从大到小排序leastNums.end();  //最后一个元素之后的迭代器，不是最后一个元素leastNums.insert(val);leastNums.erase(least.begin());result=vector&lt;int&gt; (leastNumbers.begin(),leastNumbers.end());</code></pre>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++常用API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>保研的科普与准备攻略</title>
      <link href="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/"/>
      <url>/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>此文章适用于希望保研的大一大二大三学弟学妹们，尤其是刚入学的新生</p><p>文章的开头，对帮助过我的非常nice的学长学姐老师们表达感谢。</p><p>​       在我初高中，甚至更小的时候，我就听说了“考研”这个词，被父母和周围人灌输了考研可以获得更高的学历。而大学刚入学，就接触到了几个大我几届的学长，第一次听说了“保研”这个词。</p><p>​       现在基本所有的前985、211高校和一些重点本科高校，都会有教育部划拨的保送名额，在几年前，保研意味着你不需要考试，就可以留在“本校“读研，而随着最近几年政策的逐渐放开，“外推”成为了更普遍的保研出路。而我只针对当下的政策和形式进行解读，不排除今后保研政策改变的可能。记住，询问他人得到的结果远远不如带有印章的政策文件来的踏实。</p><p>​       保研，应该说是：推荐免试研究生。与考研作对比，考研需要初试（全国硕士研究生入学统一考试）和复试（考生在通过初试的基础上，对考生业务水平和实际能力的进一步考察）。而保研只需要通过学校对你的考核，不再需要全国统一的考试，就可以去目标学校读研。</p><p>​       </p><h2><span id="1-保研介绍">1 保研介绍</span></h2><h3><span id="11-保研名额的获取">1.1   保研名额的获取</span></h3><p>保研的第一步是获得本校的保研资格，而这个保研资格是根据学校制定的政策，同专业进行竞争得来得。每年教育部会给各大高校划拨保研名额，根据之前制定的保研政策，一些相对优秀的同学通过成绩或者竞赛等加权得来。</p><p>以东北大学15级学生为例，参照《东北大学学生手册》，东北大学关于推荐优秀应届本科毕业生免试攻读研究生工作办法。大致分为两种：具有突出竞赛奖项的、学业成绩（GPA）较高的。当然18级新生的保研政策已经修改，仔细研读文件即可，这里不再赘述。注意：不要道听途说，以政策文件为准。</p><h3><span id="12-外推过程">1.2 外推过程</span></h3><p>​       拿到了本校的保研名额，还需要意向学校的接收。最终保研的结果，是在9月末学信网上的填报系统为准，那些夏令营、九月推免都是为了让意向高校预录取你的过程。</p><p>​       大致的外推途径有：</p><ol><li>各高校某些学院举办的夏令营（大多在6、7月份）</li><li>各高校各大学院举办的预面试（九月中下旬，俗称九月推免）</li><li>一些独立实验室的招生（例如南大的lamda，哈工大SCIR等）</li><li>一些较差学校的补录（填报完系统之后，对于层次较高的院校基本不涉及）。</li></ol><p>这几个阶段如果被学校预录取，就坐等九月末填报系统就ok了。</p><h3><span id="13-夏令营和九月推免">1.3  夏令营和九月推免</span></h3><p>大多数夏令营一般是在6-7月份，计算机相关的夏令营大多在7月上旬和中旬。夏令营难度大、竞争极强。但是会有很多的机会拿到好导师好学校的offer。</p><p>九月推免竞争相对较少，对于举办过夏令营的高校和学院基本就是补补漏，好导师好方向已经没坑了；对于没有举办夏令营的高校和学院，竞争也是较大的。</p><p>对于计算机相关来说，夏令营和九月推免的考核大同小异，面试+机试（可能没有）+笔试（可能没有）。之后在准备那里会详述。</p><h2><span id="2-保研准备">2. 保研准备</span></h2><p>为了保送到较好的高校，早做准备是非常值得的。</p><h3><span id="21-本校的保研资格">2.1 本校的保研资格</span></h3><p>​       首先，为了拿到本校的保研资格，你需要好好考试、提高绩点、参加竞赛等等，尽自己的努力去拿到这个名额。</p><p>​       例如：排名尽可能的高、去争取学校认可的国家级比赛一等奖等等。做哪些取决于保研资格认定的需要。例如争取双优（如果不懂，参考学生手册）等等。</p><p>​       </p><h3><span id="22-外校的接收">2.2 外校的接收</span></h3><p>​       本校的保研资格，在政策上写的清清楚楚，但是如何拿到外校的资格，网上的信息就比较眼花缭乱了。</p><p>​       那么，在夏令营和九月推免期间哪些是体现自己竞争力的点呢？这里我以计算机相关学科为例（排名有先后）：</p><ol><li><p>学校背景。不管怎么样，本科背景是最重要的，但也是目前的你改变不了的。如果本科牌子够响就能争取到更多的机会，不够响，就只能通过自己的实力让导师刮目相看。</p></li><li><p>专业绩点排名比例。绩点排名是体现综合水平最直观的指标，是你跨入各大高校的门槛，也是考核时的重要参考依据之一。同一个高校不同专业排名的同学的去向是真的会差很多的。</p></li><li><p>科研&amp;项目。我把这项放在竞赛之前，是因为这一点会涉及到你的知识域，任何一个老师都会喜欢一个已经学会很多内容的学生，而不是需要从0开始的学生。如果能跟着实验室的老师发paper（论文）就更好了。</p></li><li><p>竞赛。竞赛放到后面，不是因为含金量小，是因为面试的时候可以说的内容较少。当然一块ACM金牌银牌这种含金量的比赛肯定是要放在3之前，但是一般的比赛例如数学建模、蓝桥杯等等含金量还是不如一些科研和竞赛的。其实换个角度讲，做一些能说的出来的比赛，例如一些算法大赛、kaggle和天池的数据竞赛等等也还是很有帮助的。</p></li><li><p>英语四六级。至少得过，不过六级会过不了很多学校的初筛，一些经管类的六级分数要求也是特高的。</p><p>最重要的是你的表达能力。如何将你的优秀展示出来，如何展示自己的亮点在众同学中脱颖而出，如果言语得体有礼貌并获得老师的好感，这才是最重要的。</p></li></ol><p>于是，我们可以看到，在拿到本校的保研名额后，如果想保送到较好的学校，你需要在大学的前三年，去：提高绩点、尽可能的参加学校实验室的科研or项目、尽可能的去参加省级国家级的比赛、提高四六级成绩甚至考个雅思托福等等。</p><h3><span id="23-临近保研的准备">2.3 临近保研的准备</span></h3><p>​       在前三年，不论是绩点、竞赛还是其他什么，该有的也都有了。但是很多同学都比较迷，自己的水平应该申请怎样的学校。</p><p>对我们人数较多的大专业而言，同专业学长学姐的去向非常有参考价值。我从学院官网和学长那里得来了他们的排名和去向，写几行代码merge了一下，得到如下：</p><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image002.jpg" alt="https://pic4.zhimg.com/80/v2-b06c07aedc1ceb5fb2504fcc25f3eb92_hd.jpg"></p><p>这些学长学姐很多都有很有含金量的竞赛、科研等等，因此也并不是排名越高去向越好，但是基本锁定了自己想去并且能去的高校区间。</p><p>院校定下来之后，就要考虑自己想读什么方向，这方面就根据自己的兴趣和行业发展去选择并大胆申请就好了。</p><p>​       在院校的选择上，有些人不惜牺牲学校档次追求导师水平，有些人追求名校光环不care导师和方向。在这些选择上，仁者见仁智者见智，追求自己想要的就好。当然，如果想要导师、方向、院校兼顾也未尝不可，提升自己的竞争力就ok了。</p><h3><span id="24-保研材料的准备">2.4 保研材料的准备</span></h3><p>我所做的准备如下：</p><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image002.jpg" alt="https://pic3.zhimg.com/80/v2-d2ea3a3f8287a1fe2fecc051d6e4a72a_hd.jpg"></p><ol><li>材料准备</li></ol><p>包括：</p><p>材料证明（成绩单、证书、排名证明、四六级成绩单、身份证等等，建议用扫描全能王app拍下来保存，打印也方面）、简历和个人陈述（简历很重要！）、夏令营要准备的材料（分各个学校，按照文件要求打包整理）、</p><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image004.jpg" alt="https://pic4.zhimg.com/80/v2-a4167cef57b96ff9a8aef5db3ec64308_hd.jpg"></p><ol><li>参考材料</li></ol><p>各个学校各大学院的招生简章和政策、同专业学长的排名和去向、各个学校各大学院历年招收的名单（用于参考）等</p><ol><li>联系老师</li></ol><p>各个相中的老师资料、跟每个老师要发的不同的邮件内容（在框架上加一些具体的信息针对不同的老师去发邮件）</p><ol><li>知识填坑</li></ol><p>参加夏令营前要复习的基础知识的整理、所做项目的整理等等</p><ol><li>网站收藏</li></ol><p><img src="/2019/08/20/jing-yan-fen-xiang/bao-yan-de-ke-pu-yu-zhun-bei-gong-lue/clip_image006.jpg" alt="https://pic4.zhimg.com/80/v2-4eaf2597a9dfad45e6967e8a392f7c6b_hd.jpg"></p><p>很直白了，自己看吧</p><p>其实，最最重要的还是你的简历。怎么更好的展示自己，怎么让你被导师吸引，怎么体现你的闪光点，怎么让你脱颖而出。基本缕清简历，就缕清了自己的竞争力。我的简历经过大大小小几十次改版，也慢慢变得完善。在此再次感谢帮助过我的学长学姐和老师。</p><p>保研，虽说是保送，但如果大学期间不好好努力，保研期间不好好准备，就会浪费掉很好的深造机会。</p><p>早点了解、早点准备总归是好事，但愿你们不会像我一样发出：“我要是早点开始准备就能balabala”这种感叹。</p><p>最后，预祝大家保研顺利，将来都能被理想的学校录取！~</p><p>再次感谢帮助我支持我的人！</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 保研的科普与准备攻略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>北理北航中科院计算所保研经验</title>
      <link href="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/"/>
      <url>/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-前言">1. 前言</span></h2><p>放在开头：</p><p>非常感谢传授经验耐心解惑无私的学长学姐们，站在你们的肩膀上我才能看的更远</p><p>非常感谢学院悉心教导指点迷津的老师们，是你们为我增添力量为我指引方向</p><p>非常感谢辅导员和教学办老师的指导和帮助，给你们添了不少麻烦</p><p>非常感谢默默支持我的家人，我因你们高兴而满足。 </p><p>写于2018年7月30日：</p><p>笔者大一转专业到软件工程专业、大二决心保研后排名由 50+名提升到 14 名，大三经过选拔进入软件学院大数据实验班。目前已拿到了北京理工大学计算机学院、北京航空航天大学计算机学院、中科院计算技术研究所的夏令营offer，南京大学得到了入营资格但是选择了放弃参加，没有意外和变故的话最终会选择中科院计算所读研。</p><p>因为这段时间夏令营刚刚结束，而九月末才会填报系统尘埃落定，又担心届时将夏令营的细节忘掉，故先记录下来，等最终确定再将之公开作为下一届学弟学妹的参考。</p><h2><span id="2-准备的文件">2. 准备的文件</span></h2><p>我所做的准备如下：</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image002.jpg" alt="clip_image002"></p><p><strong>（1）</strong>     <strong>材料准备</strong></p><p>包括：材料证明（成绩单、证书、排名证明、四六级成绩单、身份证等等，建议用扫描全能王app拍下来保存，打印也方面）、简历和个人陈述（简历很重要！）、夏令营要准备的材料（分各个学校，按照文件要求打包整理）、</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image004.jpg" alt="clip_image004"></p><p><strong>（2）</strong>     <strong>参考材料</strong></p><p>各个学校各大学院的招生简章和政策、同专业学长的排名和去向、各个学校各大学院历年招收的名单（用于参考）等等</p><p><strong>（3）</strong>     <strong>联系老师</strong></p><p>各个相中的老师资料、跟每个老师要发的不同的邮件内容（在框架上加一些具体的信息à针对不同的老师去发邮件）</p><p><strong>（4）</strong>     <strong>知识填坑</strong></p><p>参加夏令营前要复习的基础知识的整理、所做项目的整理等等</p><p><strong>（5）</strong>     <strong>网站收藏</strong></p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image006.jpg" alt="clip_image006"></p><p>很直白了，自己看吧。</p><p>其实，最最重要的还是你的简历。怎么更好的展示自己，怎么让你被导师吸引，怎么体现你的闪光点，怎么让你脱颖而出。基本缕清简历，就缕清了自己的竞争力。我的简历经过大大小小几十次改版，也慢慢变得完善。在此再次感谢帮助过我的学长学姐和老师。</p><h2><span id="3-夏令营申请和准备">3. 夏令营申请和准备</span></h2><p>​       很多同学都比较迷，自己的水平应该申请怎样的学校。</p><p>​       对我们人数较多的大专业而言，同专业学长学姐的去向非常有参考价值。我从学院官网和学长那里得来了他们的排名和去向，写几行代码merge了一下，得到如下：</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image008.jpg" alt="clip_image008"></p><p>​       这些学长学姐很多都有很有含金量的竞赛、科研等等，因此也并不是排名越高去向越好，但是基本锁定了自己想去并且能去的高校区间。</p><p>​       我的意中高校（京津）我大致作了个优先级排名，在实验室或者导师同等水平下按照如下次序选择，如果导师较强就视情况升档：</p><p>​       北大信科、北大叉院&gt;清华软院&gt;中科院计算所、自动化所=&gt;北航计算机&gt;北大软微&gt;人大信息&gt;软件所&gt;北理&gt;天大计算机&gt;中科院信工所&gt;南开计算机</p><p>刚开始我想着不管想不想去，能报的都报一下，到时候如果过了感觉很有成就感。但是思考再三，秉着不影响排名靠后的同学报名真正想去的学校，之前想报但是并不想去的学校就都没报。</p><p>​       除了南大报名最早流程简单就随手报了，其他的都是自己真正想去的学校。下图是我当时的报名、可见很惨。基本上是当时看不上的都没报，结果看的上的都过不了。</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image010.jpg" alt="clip_image010"></p><p>​       因为我很想去北京读研，基本全报的北京的高校，想着人大计算机不是很强，学姐说也不是很难，结果就想拿来保个底。结果人大是最先出来的，第一个就把我刷掉了，顿时怀疑人生，之后各大夏令营一个一个的把我刷掉，清北就算了，天大都没让入营也是很惨……</p><p>​       可以说，在各方的大肆宣传下，全民开始转计算机，尤其是人工智能大热，导致顶尖高校竞争十分激烈，数学、自动化、计算机、软件、电子通信等等等等专业都涌向计算机。这里举几个例子：自动化所4000+人（老师说的）报名入营240人入营，南大3000+人（忘了哪看的了）报名也是入营几百个、北航去年报名800+今年1345……</p><p>​       </p><p>但是很多院校里的不同实验室水平参差不齐，有的人追求就业就不太care导师只care方向，有的人追求好导师不惜降低学校档次，其实不太强的学校里一些老师带的学生并不比一些强校老师带出的学生差。只能说，人各有志，选择适合自己的就好。</p><h2><span id="4-关于联系老师">4. 关于联系老师</span></h2><p>​       有人说联系老师没啥用，也有人说联系老师很重要。不管有用没用，联系了就不吃亏。</p><p>​       我从四月末五月初就开始给我中意的老师发邮件，现在看来确实太早了，很多官回也有很多老师告诉我等开始招生再商量不迟，当然大多数还是没有回信</p><p>​       </p><p>分析一下联系老师的以下几个时间点</p><ol><li><p>夏令营报名还未开始。适用人群：大神。适用高校：顶尖的高校。作用：看看有没有老师对你感兴趣，进行提前考核预定牛导（晚了大牛就没名额了）。当然一般人是没用的（例如我/哭）。</p></li><li><p>夏令营报名前后：适用人群：所有人。适用高校：有把握入营的高校/非常没把握入营的高校。作用：前者是提前预定老师，后者是碰运气看看有没有老师帮你入营。</p></li><li><p>夏令营确定入营：使用人群：入营的人（/摊手）。适用高校：入营的学校（/摊手）。作用：提前预定老师，晚了就没名额了。</p></li></ol><p>忠告：</p><p>很多人疑问保底的学校要不要去联系老师，我的亲身经历告诉你：不要。因为如果不是运气太差或者太自满的话，每个人基本都能找到高于自己保底学校水准的高校。因此自己把握很大，但是有更大把握去更好的高校的，就不要再联系老师了，否则到时候还要拒绝徒增烦恼。</p><p>​       另外，如果有一些把握能去更想去的学校，就不要跟老师作出保证，即使你只有这一个offer，毕竟变故太多了，谁都无法预料。我们这届就有同学鸽了老师，虽然在7月份就跟老师说明了，但是老师也很生气找到了我们辅导员这里。虽然历年这样的情况很多，但是只能说，吸取教训与老师多真诚一些沟通吧。</p><h2><span id="5-北理工夏令营">5. 北理工夏令营</span></h2><p>​       北理是我参加的第一个夏令营。出通知第一天就报了名，于是就显示出了报名早的好处，在基友还没报的时候就收到了入营通知邮件。</p><p>入营难度：</p><p>北理很看重985-211，即使是985的跨专业且排名20+%的学生都能入营。报名人数未知，入营人数270+。</p><p>考核：</p><p>水到不能水的机试、水到不能水的面试。</p><p>​       机试分两场，根据基友描述的第二场和我做的第一场，感觉难度相当。机试环境是devc++，版本落后，无补全，无stl。写完告诉老师，老师让你输入样例。一道题三个样例，分值不同，满分一百。</p><p>​       我参加了第一场，要求两个小时，两道题。半个多小时做完，小半个小时在那测无数个样例，最后发现样例真的很简单/笑哭。第一题是输入一行字符串：“1,2,3,4,1,2,3”。输出第一个重复的数字，如果没有输出-1。三个样例分别是：0输出-1；1,2,1,2输出2；1,2,3,4输出-1。第二题是输入一串字母x例如abcd。设定一个函数f(x)，可以得到f(x)=bcda，则f(f(x))=cdab。字母x的长度为l=4，则最多可以嵌套四次对这个字符串处理。区分大小写。输出int类型的结果，表示在每次f(x)的过程中得到的结果与原字符串相同的次数。三个样例如下：aa   2；aAa  1；byebyebye   3；</p><p>​       </p><p>​       面试就是中文自我介绍，问问项目和个人情况。我是带着简历进去的，所以基本就是针对简历的某个项目讲了讲，一个人七八分钟的样子。很水。</p><p>北理一行除了拿到了第一个offer外，最大的收获是有机会和北理的老师聊了聊。</p><p>去夏令营之前联系的一个视觉的老师，他可能是人收满了，把我简历转给了他实验室另一个比较厉害的老师，这个老师联系了我。我到北理的当天，专程赶到学校问了我的情况，算是简单的面试，大体比较满意。但是他说要我确定去他那里读研，我就如实告诉他我可能会去试试别的高校。那时候对自己不够自信，但是可能老师觉得我基本不会来他这里了，基本就不太理睬我了。</p><p>​       然后就跟着基友去他联系的那个老师那里，晚上九点开始聊了将近俩小时，老师也没对我们进行考核，主要交流了研究生的学习生活和科研状况，可以看得出这个老师是真的一心搞科研的……这个老师真的很棒，除了让我们开拓了眼界，还让我们增长了自信，给我们提了很多建议，鼓励我们去更好的地方试试。当然她实力也很强，指导的学生也有顶会发表，对学生也很好，研究方向也很热。礼欣老师，很感谢她，强推一波。</p><p>综合评价一下北理： </p><p>首先：住宿很破，硕博公寓四人间上床下桌，很旧，两床之间特别窄，两人无法并行的那种。但是校园还不错，校园很大但是路很窄（毕竟北京寸土寸金），葱一样的树郁郁葱葱，有的道路自行车限行，治安很好，校园很干净，食堂很多。</p><p>​       2018年计算机学院和软件学院合并，计算机学院的牛导特别多，随便抓一个就一堆顶会。还有幸听了一个九篇顶会的博士和五篇顶会的硕士做了学习分享，真的很厉害。唯一的美中不足是生源不是很好跟清北华五没法比，但是学生最终这么优秀真的是很不容易了。建议不太在意学校牌子的，去北理找个牛导真的是很不错的选择。</p><h2><span id="6-北航夏令营">6. 北航夏令营</span></h2><p>​       当时一串夏令营都没过真的很绝望，到最后只剩北航没出。北航去年（2017）报名800+，入营300+（应该没记错，官网有具体数据可以仔细看看）。而今年（2018）竟然1345人报名，牛校很多，瞬间感觉自己凉了。</p><p>​       但是幸运的是，入了营，成为500+中的一个。那个时候很忐忑，北航过不了北京的高校也没有多少可以选择并且有希望进入了。于是比较积极的准备。</p><p>​       北航是机试+面试，满分300，历年是公布分数，前120优营，120-180候补营员。今年是直接公布了不到两百的优营。</p><p>​       机试环境看运气，有的只有vc6.0。有的有codeblocks和dev。传闻可以用stl。提交可以选c和c++。但是听闻c++不稳，容易出事故，所以就纯c写的，也间接导致机试很惨。机试两道题，每道题都跟阅读理解似的，一千字可能没有，五六百还是有的。题目不难，很基础但很麻烦，细节很多，样例给了一个很复杂的，基本这个样例过了后面的就没啥问题了。两个小时两道长长的题。</p><p>​       第一个小时做出来第一道，但是仔细比对和样例有一丢丢不一样。又调了一小时bug，结果还是没调出来。第二道题就把题目的样例输出了一下，一点没做，绝望的不行，晚上回沈阳的车票都买好了，最后退票还花了20块钱，真是心疼。感觉样例应该过了简单的几个，也可能老师也参考了代码。反正幸运的通过了。</p><p>​       北航面试分很多组，问什么看运气，我很幸运的排在第二天，第一天四处打听，很多组的流程：1-政治题（瞎扯）2-念一段英文，然后翻译中文/英文自我介绍 3-专业知识考察，操作系统计算机组成原理什么的 4-项目 5-瞎扯点家常。</p><p>​       我面试在第二天，第一天的时候听到消息，赶紧复习了一天的操作系统，最后也没问尴尬。我的流程，进去之后抽小纸条政治题，跟老师说一下题号，是XXX讲话上青年爱国、立志什么什么的，我说了题号之后一个老师问我：你知道这个XXX讲话是啥吗，我说：应该是习近平在某次会议上的谈话。然后老师都开始笑我，我才意识到说了一句废话。老师追问我具体是啥，我说不知道，老师很无奈的说那你谈谈感想吧，我就瞎扯balabala。然后开始抽纸条念英文，磕磕绊绊，念完后翻译，日常卡壳，自己都不知道翻的啥，但是看老师表情还不算特别烂吧。</p><p>​       然后，特别幸运的，这几个老师只问了我项目！！！最拿手的最能扯的项目。最后看起来扯得不错。老师开始唠嗑，甚至问我简历上自我评价那“做事认真极为可靠“为什么要加极为……翻白眼，最后老师问计时的老师时间，老师说还差几分钟，然后几个老师就比较和蔼的说没事，然后就随便抛了个话题，我接住之后继续扯。能看出来老师们比较满意，美滋滋。</p><p>​       然后下午就去天安门和博物馆玩去了，排队安检的过程接到了面试我的一个老师，说他的研究方向balabala，让我有兴趣明天去聊聊，我就知道面试稳了，看看机试会不会拉分了。当天晚上就得到名单了，赫然在列。</p><p>​       北航通过夏令营的同学会发一张导师意向表，最后一天找意向导师签字，九月底填系统就是这个老师收你了。虽然机试面试还算顺利，但是找老师让我吃尽了苦头。我相中了做视觉的一个老师，夏令营前两个月就先后给他发过三封邮件，结果依然没啥用，没要我，关键是中午才出结果告诉我已经确定学生了。这个时候我就慌了，因为觉得这个老师比较稳，就没联系别的老师。然后下午就一直不停给各个老师发邮件，直接上办公室找，结果耽误了黄金时间的我自然找不到太好的老师了，之前联系过的老师也都满了。五点就要交表了，结果我还没有老师，在四点的时候一个老师给我发邮件让我去另一个老师的办公室，让那个老师面试我，面试了我一下觉得我很满意，但是那个老师给我的印象特别不好，由于并不是这个老师招我，所以我也没太care，最终签了那个老师。当然最最后发现这个老师以前是和cxw一个组的……然后经过后面几天的思考还是觉得不太合适，夏令营没过几天就早早的拒绝了这个offer。</p><p>综合评价一下北航：</p><p>​       首先：校园环境相对北理还是不错的。住宿听说还不错，屋小了点但是还算新。校园和北理差不多，但是显得宽敞一点，综合来看，环境比北理强一档。</p><p>​       北航软件工程学科评估A+,计算机A。但是不代表软院比计院强，其实，这里的软件工程学科大部分是由计算机学院的软件工程学科评上去的，有个软工重点实验室比较强，所以要搞清楚概念。</p><p>​       北航有一点比较坑，就是虽然收的人多，但是很多都要外派到青岛、合肥、杭州等研究院学习一年，大概比例20-30%，很多人的意向好老师最后只剩了外地的名额。例如我联系的一个青千最后只剩了青岛的名额……当然果断的拒绝了。</p><p>​       还有一点，我接触到的几个老师感觉就像是雇佣你去做项目的，根本不是培养学生为目的，不让实习，补助还很少（学院700+导师400），感觉没有那种人文关怀，很不舒服。当然也可能是偶然，我接触的少。北航接的纵向项目很多都跟军工有关，很多人具体研究方向取决于他接到的项目。但是也有不少很好的实验室，偏学术，乐于培养学生，这样的就很不错。个人觉得实验室整体招生或者培养的比某个老师带一个组的氛围要舒服的多。</p><h2><span id="7-计算所">7. 计算所</span></h2><p>​       计算所真的是意外之喜。在只拿到北理offer，被拒了一堆（包括计算所也没有入营），只剩北航的时候，感觉很慌张，很忐忑，北航过不了就凉凉了。这个时候计算所的招生老师说没过的可以填问卷，如果实验室同意了可以去面试。</p><p>​       于是我就每个实验室（网数、智信、前瞻）联系了一个老师，看看哪个老师回我了就报哪个老师。结果到问卷截至日期快到了才收到智信实验室联系的老师的官方回复。于是问卷就填了智信，也没抱希望。</p><p>​       结果过了几天，网数负责招生的老师通知我去面试，如下图：</p><p><img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image012.jpg" alt="clip_image012"></p><p>​       邮件把整个过程都记录了下来：我联系的老师A并不了解招生事项à老师A询问老师B是否可以招夏令营没过的学生 à 老师B告诉老师A可以，告知负责招生的老师C就行à 老师A告知老师Cà老师C通知我去面试。</p><p>​       于是我依靠我的简历、依靠我详细的说明、依靠我诚恳的态度，通过我一直以为没用的提前联系老师的过程，为我挣到一个面试机会！<strong>感谢xx老师</strong>（为了不给老师的邮箱添加负担还是不写名字了）</p><p>​       </p><p>​       当然挣到了面试机会，还是得想办法通过。这个时候我强大的人品又一次登陆了！计算所入营的学生被要求只能填写一个实验室去面试，虽然也有很多学生去多个实验室面试，但是最热门的三个实验室网数、智信、前瞻基本只能报一个。于是我这个没入营的和其他入营的同学的差别就是那张几百块的饭卡和价值几百块的住宿了。</p><p>网数的面试学生听学长说去年是94进28，竞争还是蛮激烈的，不抱希望的我看到面试的只有65个人是相当激动的。网数是机试+面试，没有笔试所以不用太过于准备专业知识。       机试两个小时五道题。在自己准备的笔记本上编写，然后u盘拷贝代码（没错看起来很low）。这样看来应该是不会自动过样例，顶多手动编译一下。于是我依然秉着大学三年来充分展示自己的机会，多行注释+各种思路说明+整洁的代码，没做出的题也写了大概思路和可能的解法，不放弃任何展示自己独特的机会。题目各种类型都有，简单的hash、堆栈实现一个较为负责的符号计算器、dp问题/背包、DFS/BFS。我的水平勉强做出了不到三道题，其中一道差一个函数，return true并且加注释给凑过去了。后来才了解到机试不计权重，做出来两道算通过，蓝桥杯C/C++省二选手还是没啥问题的/滑稽。</p><p>面试的话，一个人10-25分钟，由于姓名排序，我机试完的那天下午就开始等待面试。刚开始每个同学都是15-25分钟，过了饭点后就快了，基本每个人10分钟，轮到我的时候不出意外的长了点15分钟（/笑）。</p><p>流程很简单：自我介绍+很深入的问项目。这我最拿手了，可是一群研究文本的老师追着我问视觉的项目我也是很迷。问完项目，有个老师问我数学怎么样，数学哪科好，虽然我数理不差，但是没复习，大一学的早忘光了。但是也不能说都不行吧，我就说最近看了点线代，很多都是大一学的，可能记得不太清楚。然后就问我线代的内容，我就是挨个章节扯：行列式、矩阵、向量、二次型balabala。二次项没想起来，想起来个二，然后旁边几个老师帮我打圆场23333。然后问了我线代的问题，都不是简单的概念而是偏理解，例如矩阵秩的意义是什么、伴随矩阵为啥有俩，正交矩阵有什么含义……于是脸不红心不跳的瞎扯一丢丢，扯一些绝对正确的大概念，然后再说细节不会，虽然没全答出来，但是看来那个老师也不是特不满意。到最后就开始问要不要读博balabala这种问题，我就balabala说，说因为目前做的都是偏应用的，不知道做学术能不能做下去，准备先读个硕看看，如果可以、会考虑读博（标准答案，但是也是实话）。然后表达了一下河北人对北京的偏爱，balabala多么想去计算所等等。感觉老师探你的底问你会不会来的时候，其实已经八成稳了。</p><p>忘了说了，面试时是一屋子老师面试，大概十三四个，感兴趣的就问你问题，最终能不能通过，老师们投票，通过半数就ok。美滋滋。</p><p>然后就胸有成竹但还是很忐忑的等了两天，等来了中科院计算技术研究所网络数据科学与技术实验室学硕的通知电话，我还专门录了音，每天睡前听一遍233333.</p><p>计算所综合评价：</p><p>​       计算所在一幢写字楼里，学生们在一间超大超开阔的实验室有各个隔开的工位，比北航看起来舒服多了。计算所的楼里环境特别好，很喜欢。虽然不是校园式的而是研究所性质的，但是整体比较舒服。</p><p>​       计算所的生源比北航还要好一档，入营名单就一水的985，而且是排名靠前的那些。优营名单绝大部分都是985，其他的都是没入营的211甚至双非，相信霸面的他们不会差到哪里。关键是有五个北大七个清华，想想就刺激。</p><p>​       计算所最热的实验室当属智信、网数、前瞻。其他的实验室不是差，而是方向不够热或者招生人数少，所以就没有那么火。研一在雁栖湖校区（北京郊区）听院士们上课，研二研三回所做项目/写论文。当然计算所补助也远远多于高校的平均水平，研一1500，研二将近3000（根据实验室有所差距）。学费硕士交8000返8000，博士交10000返13000。要不是直博风险太大，真的想读博去2333.</p><h2><span id="8-南大计算机">8. 南大计算机</span></h2><p>南大计算机也在计算所结果出来之后补录了我，南大今年竞争也异常激烈，据说3000+报名，入营几百个，基本上是暴力筛，除了学校背景、专业排名、英语水平、论文、比赛之外都不用填别的信息了。所以卡在5%外真的难受（虽然还是补录了），如果六级没过也很惨，下面这个哥们就是一个特别想去南大但是六级没过没入营，替他祈祷，希望九推能进。</p><p> <img src="/2019/07/30/jing-yan-fen-xiang/bei-li-bei-hang-zhong-ke-yuan-ji-suan-suo-bao-yan-jing-yan/clip_image014.jpg" alt="clip_image014"></p><p>​       最后因为在北京奔波了几天真的很累，离的又远，又秉着把机会留给真正想去的同学，就放弃了公费旅游住星级酒店的机会，哎可惜了，因为我觉得如果我去了还能再拿一个offer233333.</p><p>综合评价一下南大：</p><p>​       校园很棒（尤其是新校区，虽然在郊区），有幸去过老校区，感觉很不错。华五计算机，生源肯定不差，一水儿的985。Lamda是南大必须要提到的，周boss传闻很强，因为当时也没想去南方，所以也没报lamda，错过了收到lamda贼棒拒信（写的可好了，一点生不出被拒绝的难过）的机会。但是这种水平的高校，一般牌子都够了，要看导师和实验室的水平，这么大的学院导师水平参差不齐，一定要鉴别好，不要入坑！</p><p>​       </p><h2><span id="9-写在最后">9. 写在最后</span></h2><p>​       其实总结起来，不论是跟老师的交流、面试机会的争取、面试的通过全靠我的简历，展示了我的闪光点：动手能力强、学习能力强有上进心、项目多。其中那些项目真的真的很好的帮助了我。所以不论是竞赛、科研、实习经历等等，把亮点展示出来，是真的非常非常重要。当然，从大一开始到现在，再到将来，我也始终秉持着这一点，去争取更多的机会！</p><p>截止到现在，保研告一段落了，九月末才会填报系统尘埃落定，先记录下来，等最终确定再将之公开得瑟一波，希望一切顺利。</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 北理北航中科院计算所保研经验 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>来自良心学长的东北大学新生攻略</title>
      <link href="/2019/07/30/jing-yan-fen-xiang/lai-zi-liang-xin-xue-chang-de-dong-bei-da-xue-xin-sheng-gong-lue/"/>
      <url>/2019/07/30/jing-yan-fen-xiang/lai-zi-liang-xin-xue-chang-de-dong-bei-da-xue-xin-sheng-gong-lue/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>​       这两天学校又搭起了一排排的彩虹门，一个个略显稚嫩的新面孔踏进了这个校园，一如我当年踏进校园的模样，还记得三年前南湖二舍东门口的条幅上：“东北大学，梦想开始的地方”，是的，梦想开始的地方。</p><p>“倘若当时…就好了”大概是最遗憾但是却最不可能避免的想法了。而从我入学到现在，虽然有很多学长学姐老师们热心的帮助，但是也不免走了很多弯路。</p><p>最近比较闲，打算分享一些自身的经验、大学期间试过的坑、吃过的亏亦或是一些可喜的收获，给将要（由于拖延症的关系，这里改成：已经）入学的新生或者离毕业还有一段时间的学弟学妹一些建议，提供一些力所能及的帮助。</p><p>如果还有大家关心的问题，欢迎在评论区提出，如果我了解一些的话，会尽可能提出一些力所能及的建议。此文所有的观点都是个人的经历带来的感受，切不可模仿，倘若能起到参考的作用，那就很ok了，如果有不太对的地方，欢迎批评指正。</p><h1><span id="1-学习相关">1. 学习相关</span></h1><p>在我还没入学的时候，就认识了一个同专业的学姐，这个学姐帮助了我很多，最最重要的是教会了我大学里最重要的是学习，虽然在一定意义上有失偏颇，但是却令我受益匪浅。所以我打算先谈谈学习相关。</p><h2><span id="11-成绩相关攻略">1.1 成绩相关攻略</span></h2><h3><span id="111-gpa绩点">1.1.1 GPA/绩点</span></h3><p>东北大学的学生都免不了接触到GPA/绩点这个词，通过学分作为权重的方法进行加权平均得到一个5以内的小数，它代表了你的学习成绩。通常，GPA与你的奖学金、保研、专业分流、转专业、出国交流机会等等息息相关，一个学霸的重要评判标准就是他的绩点。</p><p>GPA是你所有课的成绩计算得来的，包括选修课，包括人文选修。有个叫学分的东西，代表这门课的权重，学分越高，占比越大。</p><h3><span id="112-培养计划">1.1.2 培养计划</span></h3><p>你可以登陆到东北大学教务处，有个叫培养计划的东西，显示了你大学的所有课程和学分，学分没有修够是不允许毕业的，课程类型例如学位课、鼓励选修课等等在上面都可以看到，这里不再赘述。</p><p>大部分学生到了大二才了解到培养计划这个东西，建议有心的学弟学妹早点看看，应该会有一点点帮助。</p><h3><span id="113-考试攻略">1.1.3 考试攻略</span></h3><p>​       想要奖学金，想要保研，必须要把GPA尽量提高，那么这就遇到一个问题，绩点怎么提，课怎么考，分怎么高？</p><p>​       在大学，已经不像高中，要天天学，天天做题。大学生，学的好，不一定考的高！如果你想学的好点，花费时间学习理论并加以动手实践。但是有很多课程并不能跟上时代的发展，你今后的学习和工作并不能用到它，那该怎么办呢？这就涉及到大学的学习和考试技巧了。</p><p>​       相信看完培养计划，你就可以把所有的课大致分成：学位课（专业课）、学位课（非专业课）、专业选修课、非专业选修课、实践课程。我会根据课程不同类型，给大家一些提高分数的建议。</p><p><strong>1.</strong> <strong>学位课（专业课）—平时分+考试分</strong></p><p>这就是我们大学期间最重要的一类课程了，这类课程的代表有通识课程之类的高等数学线性代数等，有跟专业联系紧密的—以计算机相关专业为例有：数据结构，计算机网络等。这类课程大部分来说以后是都能用到的，考研保研找工作乃至以后的基础专业技能等等，因此不仅要学的好，还要考的好。</p><p>学：老师教的好的话，跟着老师走就ok，老师教的不好的话，找点网课课外资料什么的，也能学的很好。</p><p>考：还是那句话，学的好不一定考的好。想考好，首先看老师有没有划范围，有没有历年题，有的话就使劲参考。平常的学习，一定不要脱离课本，例如高数，期末考的大部分课后题都有，这种课到最后复习再去刷同济数学题就不太明智了。其实最简单的方法就是：找个去年考的高的学长问问他怎么考的。这里再次感谢帮助过我的学长学姐。</p><p>平时分：学位课是有平时分的，根据老师要求好好做就行了，大部分情况可以参考学位课（非专业课）的情况。</p><p>​       想提高GPA，学的好，真的不如会考试。（可怜我用大一惨的不行的绩点才明白这个道理）但是千万别有：既然能想办法考的高，何必再学呢，的这种想法。鬼知道以后会不会用上呢，例如学妹找你教高数题你只应付了下考试当然教不了就错过了跟学妹接触的机会。</p><p><strong>2. </strong>学位课（非专业课）— 平时分+期末分or平时分+考试分**</p><p>这种课一般代表着毛概，近代史，思修这种政治课，俗称水课。老师讲的没意思的话就很少有人听，学分占的还挺多，最后分低了还很难受。那么怎么去提高这种课的分数呢。</p><p>这种课是老师给的，可能根据你的作业，根据你的考勤，课堂表现等等，十分的虚无缥缈，也折射出些许的不公平。很多学霸吐槽水课分都低，那我只能说你没有去争取，下面是争取的手段。</p><p>（1）      当学委。 学委跟老师的关系肯定很熟的，天天送作业，给的平时分和期末分低了也不好看。</p><p>（2）      每堂课都抢占前排+回答问题。 也是混脸熟的手段，但是容易吃力不讨好。</p><p>（3）      老师明确了的就尽量刷满平时分。 有的老师在第一节课就会声明平时分的给分方法，例如回答够几次问题等等，积极的满足要求就ok。</p><p>（4）      展现得分决心。以我为例，大一上的心理健康期末卷子，我写了慢慢当当一大篇（写的多）。大一下的思修期末卷子，写的多再加个设计的很好的封皮（作业独特出彩）。大一的游泳课在期末考试允许游两次但是大家都游一次的情况下，请求老师再游一次给了老师好的印象，最后分数也挺高。大二下的思政课老师布置拍校园图片的任务，我做了个ppt+视频，让老师记住了直接承诺给了优。其他的课程也都这样所以我的水课分数都挺高的。</p><p>所以大家看出来了吧，这种非考试的分数，只要你让老师看到你的用心，让老师看到你想拿高分的决心，你就可以拿高分。</p><p> <strong>3. </strong>专业选修课</p><p>这种课，一般都是在大二大三才会涉及，大部分情况下因为相对落后的培养计划就是为了补满学分才选的，最后的评分和考试也会很水（当然也分老师）。老师讲的好就听，讲的不好的话不如干点更有意义的事情，看书刷题（da you xi）等等，但是切记要尊重老师。</p><p>分数技巧也参考<strong>学位课（非专业课）的攻略，大同小异。</strong></p><p> <strong>4. </strong>非专业选修课</p><p>这种课其实挺少，例如人文选修课等，也参考<strong>学位课（非专业课）</strong></p><p><strong>5. </strong>实践课程</p><p>很多专业有实践课程，例如计算机相关就是去实验室做个小项目。大部分的给分是根据报告和答辩。报告尽量写多，写好，排版一定要好。关键是答辩，让老师记住你你的分数就不会太低。依然以我为例，我每次答辩，都是在答辩前尽可能完善，并加要求之外的功能（加界面加创新点）。在答辩的时候准备好：我的亮点or创新点，直观的呈现给老师；实现中的难点，卖惨嘛。换位思考一下，如果你是答辩的老师，你怎么从这么多学生里挑出做的好的，当时是表述清晰层次分明有亮点的。与水课的绩点类似，表达出来你得高分得决心就ok了，依靠这些，我得实践课分数基本没有太低得，大多都在专业前几。所以，要表现出来自己。</p><h2><span id="12-专业技能">1.2 专业技能</span></h2><p>只学了课程，是远远不够的。想要工作的时候被大厂看重，想要考研保研的时候脱颖而出，就需要提高自己课外的专业技能。这种玄学的东西，就自己查自己专业的吧。有学弟希望我谈谈假期的时间利用。其实我觉得不放假的时候利用好时间就非常可以了。当然假期的时候放松休息咸鱼，或者是旅行，或者是充充电学学习，或者是找个实习，只要充实，都是很ok的。</p><p>啥都不懂的话，先大致百度个一二三，然后找个同专业学长问问就ok了。</p><h2><span id="13-转专业">1.3   转专业</span></h2><p>很多新生入学就奔着转专业去，确实，东大的自动化很强，信息时代计算机也很火。高考后阴差阳错录取的专业现在有了更换的机会。其实东大的转专业政策算是很开明了，很感谢东大的政策。</p><p>转专业是个需要很慎重的事情，千万要考虑好自己以后想要的是什么，并且尽可能长远的考虑。这里有我17年写的帖子，很多信息已经失去了时效性，大家以准确的转专业政策为准。政策文件！政策文件！政策文件！通过政策文件你才可以得到最权威的消息，这比学长学姐那里听到的要准的多。</p><p><a href="http://tieba.baidu.com/p/5233701747?fid=44908" target="_blank" rel="noopener">http://tieba.baidu.com/p/5233701747?fid=44908</a></p><h2><span id="14-奖学金相关">1.4   奖学金相关</span></h2><p>入学时发的学生手册上有奖学金评定相关政策。我来大概说一下政策没说到的部分。</p><p>奖学金的评定是一年一评，在学年结束，下学年刚开始的时候评定。</p><p>​       奖学金大致有：国家奖学金8000，国家励志奖学金（限贫困生）5000，各种命名奖学金（大部分在4000及以下），校级一二三等奖学金分别是2k-1k-600。每个人可以评个一等或者二等或者三等，然后再加个国奖/国家励志/命名。实力强的话一万块到手。</p><p>评定的标准是智育分数和德育分数加权（每个学院的权重和组成不太一样，找学院政策）。智育分就是你的这学期的学位课的平均分。德育分就比较复杂了，包括很多项，这个可以在政策上看到。政策上看不到的是怎么去加这些分。一般是宿舍卫生会影响，志愿时长会加分，比赛会加分。专利论文等等等等。可以说德育分的影响比智育分要大。所以想拿奖学金，要注意提高德育分，根据不同学院的政策，相应的去做对应的事情即可。</p><h2><span id="15-学习生涯规划读研or就业">1.5   学习生涯规划（读研or就业）</span></h2><p>我是一个目的性很强的人，入学就决定要转专业，大二就想着保研。现在来看，这样的提早规划是非常有必要的一件事情。</p><p>一般东大本科的出路有：直接就业，保研，考研，出国，创业等。各种出路要准备的方面是截然不同的，大家的选择也会相同，关于自己以后要干什么要早点想好早做准备。</p><p>就业的话就相对不太需要care绩点，但需要尽量提高你的专业技能，尽量在大四之前找个实习，之后找个正式的工作。不太熟悉，就不多说了</p><p>保研需要前三年的成绩，或者是拿到国家级的奖项，现在的保研政策也有了改动，关于保研我写了挺多，单独放在了另外一篇：<a href="https://zhuanlan.zhihu.com/p/42530877。" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/42530877。</a></p><p>考研的大多是没有拿到保研资格，但是仍然打算深造一下，一般是初试+复试，大三下大四上就要准备考研，对于新生，能保研尽量保。考研相对费力，风险也较大，很辛苦，但也很让人佩服。</p><p>出国，不太懂，不说了。</p><p>有的同学不满意自己的专业，自暴自弃，沉迷游戏，这样是不对的。大学的出路有很多的，除了本专业本行业的就业，完全可以跨专业工作，跨专业保研or考研，甚至去转销售或者去创业（虽然我不太认可），都比自暴自弃的强。</p><p>如果不满意自己的专业，尽量大一好好学习，大一结束转专业，转专业的经历是不会被歧视的。转不成专业，可以考虑跨专业工作or考研or保研，例如很多机械的学长毕业去华为，例如每年都有跨专业读研的，早点准备其他专业的专业技能。想咸鱼翻身，什么时候都不晚。</p><h2><span id> </span></h2><h1><span id="2-新生建议">2 新生建议</span></h1><h2><span id="21-社团or学生组织">2.1 社团or学生组织</span></h2><p>相对于学习，其实新生们面临的第一个问题就是社团的问题了，估计在高中被老师忽悠大学的美好生活的时候肯定有丰富多彩的社团这项。除此之外，还有大量的关于加不加入学生会的争论。</p><p>社团和学生组织是不一样的。社团大多是兴趣或者学习社团，大家根据兴趣凑到一块，没有什么任务，只有举办的活动，不太功利性。而学生组织是学校的一些组织，例如校级学生会，院级学生会，科协，社联，志协，团委四大中心等等。这些学生组织等级分明，上到主任下到部员，他们大多是学校下派的老师招生完成一些学校指派的任务，办一些相对官方的活动。</p><p>对于社团，感兴趣加就是了，但是很多社团活跃度都不高，一年也没几次活动，活动多的也就全明星、轮滑社那么几个。所以不要抱有太大期望。</p><p>那么到底要不要加入学生组织呢。对于家里有矿的，或者是以后想创业的想转销售的，毕业想留校当导员之类的，肯定是需要学生工作经历的，那么肯定是要加的。对于家里没矿的，以后想从事本专业工作的，我的建议是：在不影响自己学习的前提下，尽可能的丰富自己的大学生活。</p><p>有的学生组织，上下级分明，支使你各种干活，熬夜做ppt，熬夜ps，熬夜赶稿子，这种就没有必要了，除了增长一些可以俗称的ppt技巧并没有什么作用，只会影响学习，得不偿失。围绕着导员工作的我个人认为也没有太大必要，靠导员倾斜资源不如提高自己的能力去自己争取。</p><p>有的学生组织，气氛融洽，经常举办活动，交流较多，工作不多，有利于身心健康，这种就非常适合，例如我大一待的能力拓展中心就很不错。其他的，我听过的志协一些部门也很不错，其他的就不了解了。</p><p>家里没矿的，还是学习比较重要。</p><h2><span id="22-电脑购买建议">2.2 电脑购买建议</span></h2><p> 新生们都面临着买电脑的问题，那么买什么样子的电脑好呢，听我一句劝，一定要搞明白自己的需求。</p><p>有专业要求的，比如需要显卡渲染图形的，或者是经常打单机游戏的，要堆配置，买厚重的游戏本，毕竟刚需。</p><p>没有专业要求的，一定要轻薄！轻薄！轻薄！直到现在我还背着我的又厚又重的笔记本跑来跑去，哎，有点后悔。LOL这种游戏一般轻薄的都hold的住，之前不怎么打游戏的买了电脑也不会怎么打游戏的，要好好学习所以更不能买游戏本，所以要买轻薄的！</p><p>怎么选呢，大部分都是小白or懒癌，那么推荐个公众号：笔吧评测室，历史消息里有各价位的游戏本or万金油or超级本的推荐，挑个顺眼的，遇到双十一就买了吧，省心省事。</p><p>​       </p><h2><span id="23-驾照相关">2.3 驾照相关</span></h2><p>​       在学校学车，看起来很美好，实则很苦逼。说的有空练车就行，但是临到考试前，就得天天请假去练了。然后考试只会在周一到周五，课多的还得请假，很麻烦。可能还会比家里贵，还很慢，所以跟家里比比再选择。</p><p>​       一般留校时间长的可以考虑在沈阳考，我在浑南报的八棵树的情况可以简单说说，大概三千几百的报名费，科二模拟费200，科三300。科二练车在3公里外的练车场，骑自行车15分钟，335也是10分钟。报名的不同教练待遇和态度也不太一样，我报名的那个教练还不错有需要的可以私戳我。</p><p>​       时间上，科一科四基本就是花个报名考试时间，科二需要练个两三周，练的差不多随时可以报名考试，科三练车需要4+4+2个小时，目前挺难预约，上次考试结束之后三个月大概才可以约上。所以时间还是挺长的。</p><p>​       如果课多的话，还是不建议在沈阳考驾照。</p><h2><span id="24-买书建议">2.4 买书建议</span></h2><p>​       买书啊，，，估计新生们用不上了。</p><p>​       买书分三种，从学校定，从学长定，自己买。</p><p>​       不缺钱就从学校定，不太麻烦，书质量也好</p><p>​       从学长那定，有缺书被坑的风险，书的质量也凑活，价格应该比学校便宜一点。</p><p>​       对于家里没矿不怕麻烦的，建议重要的参考书淘宝或者南湖小北门买新书，不重要的学完就扔的书直接找老乡学长要或者买学长的二手或者淘宝影印版淘宝二手都行。</p><h1><span id="3-大学技巧分享">3 大学技巧分享</span></h1><h2><span id="31-重要信息获取建议微信公众号网站群">3.1 重要信息获取建议（微信公众号，网站，群）</span></h2><p>​       大学期间我获得的最重要的能力就是获取信息的能力。信息不对称极大的造成了人和人的差距，这也是我写这篇的初衷。考研的时候你获得更多的考试信息，你就能做对更多的题，工作的时候你获取到更好的岗位信息并为之住呢比，毕业待遇就会比其他人好。所以获取信息是相当的重要。</p><p>​       那么怎么去提高获取信息的能力呢？摸索+请教。</p><p>​       怎么摸索呢？</p><p>官方政策要牢牢把握，比如你想获取奖学金，你就应该去找政策文件，了解怎么提高自己的德育分，比别人知道的多就可能比别人准备的多，所以就能占到更好的先机。至少自己学院官方和学校官网和教务处官网要摸索一遍吧，你会收获很多的。</p><p>还有呢？各种信息渠道：微信公众号，各种论坛—贴吧，知乎，简书，保研or考研论坛等等等等。简直太多了，比如公众号能快速的告诉我校车信息澡堂超市信息的更新等等。比如这篇文章你可以在贴吧精品贴看到等等。</p><p>​       怎么请教呢？</p><p>​       学长学姐。真的，我加了很多的学长学姐，不论是期末考试的攻略，还是转专业还是保研，很大程度上都多亏了他们的帮助，再次感谢他们。</p><p>​       </p><h2><span id="32-挣钱贷款">3.2 挣钱，贷款</span></h2><p>​       关于挣钱，我大一下的时候做了半年家教，也做过学校的勤工助学岗位，算是有点经验。</p><p>​       学校有个家教中心，非常ok，一般填好一点的申请表就会被家长看中去给人孩子做家教，小学初中高中不等。当时我教过两个，都是60元一小时，不多但是也不少。相对的我也付出了很多时间，一定程度上影响了学习，但是我并不后悔，毕竟丰富了经历很有意思，那段时间很有钱过的生活很舒服。</p><p>​       关于勤工助学岗位是只针对贫困生的，贫困生的政策在通知书里的小本本里可以看到，家里条件较为困难的可以填表+盖章到学校申请，会有助学金or可以申请励志奖学金。勤工助学岗位可以从官网进入勤工助学网里申请，不定时会有岗位空出来，一般一月10天，每天4小时，基本就是看门，摆车这种，不太花时间，在门岗那里学习所以不耽误干别的，对于家里条件较差的可以去申请。如果长时间没有反应，建议直接去找对应楼的楼长，很好说话的都。</p><p>对于贷款，千万不要校园贷！千万不要校园贷！千万不要校园贷！可以通过贫困生的身份进行校园地贷款（来校之后办理），也可以进行生源地贷款（来校之前办理），一般是8000一年，毕业前没有利息，毕业后n年还清就ok，像我已经通过贷款周转，通过奖学金和软件的奖励基金已经cover掉学费了。所以不要因为学费的事情担心。</p><h2><span id="33-怎么找女朋友">3.3 怎么找女朋友</span></h2><p>​       写到这写了七千字了，已经没啥力气了。说说你们最感兴趣的。</p><p>​       大学找女朋友，无非三种途径。当然女生找男朋友也一个道理，只不过东大尴尬的男女比例导致女生不太需要担心这个问题，尤其是南湖的女生。</p><ol><li>同班级or同专业or同学院</li></ol><p>几个班一起上课or学院的讲座开会or学院举办的活动等等，见的多了就心动了，之后就可以一块上课，一块学习，一块讨论问题，既是情侣又是同行，不会有太多隔阂，缺点就是分手了会每天尴尬好多次</p><ol><li>同学生组织or同社团or同参加了某个活动</li></ol><p>这个时候就知道我为什么建议加入学生组织了吧，大家一起举办活动，一起写策划书，一起讨论，之后就约跑步约自习，balabala。</p><p>当然傻傻的你不要加入院学生会这样的学生组织，尤其是机械学院的学弟们，不听劝等到后悔的时候不要哭。</p><p>参加志愿活动，尤其是浑南的去那个什么学校的志愿活动，满满的都是妹子，志协的凑不够志愿人数的工作者们，不用谢我，学弟们，也不用谢。</p><ol><li>老乡</li></ol><p>除了上述的途径也就老乡群了，在群里水群水着水着就自来熟了，然后借口借本书啥的面基一下，就见到真人了，爱情的小火花就怕擦怕擦辣。</p><p>这种情侣最让人羡慕了，可以一起回家，可以一起返校，作为前老乡群群主我们群里也有几对老乡情侣，哎，还有一对双双保研的，真的优秀。</p><p>良心学长只能帮你们到这了，加油吧！~</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 来自良心学长的东北大学新生攻略 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐喻识别综述</title>
      <link href="/2019/04/30/nlp-research/yin-yu-shi-bie-zong-shu/"/>
      <url>/2019/04/30/nlp-research/yin-yu-shi-bie-zong-shu/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-隐喻metaphor">1. 隐喻（Metaphor）</span></h2><p>隐喻不仅是人类语言中常用的一种修辞手法，更代表着人类对世界的认识和理解方式。</p><p>目前查到的最贴切的一个解释：隐喻就是人类利用在某一个领域的理解和经历，去解释另一个领域的认知行为。</p><p>隐喻识别属于隐喻计算的一个子任务。隐喻计算可以分为隐喻识别、隐喻理解和隐喻生成三个子任务。</p><p>隐喻理解的子任务举例：CCL2018 中文隐喻的情感分析(隐喻句子的情感分类)</p><h2><span id="2-隐喻识别的意义">2. 隐喻识别的意义</span></h2><p>自然语言处理研究领域中,隐喻是一个不可回避的问题.</p><p>一些研究表明,<strong>中文和英文的语料中存在着大量隐喻表达</strong>.      因此,机器翻译、文本处理、信息检索等若局限于获取字面意义,而无法理解隐喻等“言外之意”是远远不够的.</p><p>若能在这些领域中引入隐喻计算系统,则有希望进一步提高机器翻译质量,改善搜索引擎反馈,提高人工智能系统的水平等等.</p><h2><span id="3-隐喻的分类根据语言表达">3. 隐喻的分类（根据语言表达）</span></h2><h3><span id="31-名词性隐喻">3.1 名词性隐喻</span></h3><p>用一个名词去形容另一个名词</p><p>由指称词“是”“像”“如”等连接： 知识就是力量。    人生如梦<br>用标点符号“,” “—”等连接：  书 ,人类进步的阶梯</p><h3><span id="32-动词性隐喻">3.2 动词性隐喻</span></h3><p>动作（谓语）的实施者（主语）或者承受者（宾语）不符合人类对该动作的正常的搭配认知。</p><p>例如： &lt;狗，吃，良心&gt;</p><h3><span id="33-形容词性隐喻">3.3 形容词性隐喻</span></h3><p>表现为某个名词拥有了本身所不具备的（形容词）属性，即某个实体本身的属性与形容词所描述的没有关联。</p><p>如：愤怒的大海</p><h3><span id="34-副词性隐喻">3.4 副词性隐喻</span></h3><p>动作因为某种属性被描述成另一种具备类似属性的动作</p><p>如：他们的头点的跟小鸡啄米似的。（用小鸡啄米形容点头快）</p><h3><span id="35-常规隐喻-死喻">3.5 常规隐喻 - 死喻</span></h3><p>用太多成为常规搭配</p><p>如：尖锐的声音</p><h2><span id="4-隐喻识别的方法">4. 隐喻识别的方法</span></h2><h3><span id="对句子的分类-or-序列标注任务识别隐喻词汇">对句子的分类 or 序列标注任务识别隐喻词汇</span></h3><h3><span id="基于知识库-or-语义规则的方法">基于知识库 or 语义规则的方法</span></h3><p>常见的知识库有：Hownet（董振东、董强的知网）、Wordnet（英语词汇数据库）、同义词词林、vernet等</p><p>知网：以汉语和英语的词语所代表的概念为描述对象，以揭示概念与概念之间以及概念所具有的属性之间的关系为基本内容的常识知识库。 （<a href="http://www.keenage.com/zhiwang/c_zhiwang.html）" target="_blank" rel="noopener">http://www.keenage.com/zhiwang/c_zhiwang.html）</a><br>描述了上下位、同义、反义等各种关系。</p><p>偏语言学的角度，进行依存分析抽取文本语义关系，建立语言模型</p><ul><li>与人工定义的语义规则是否匹配 —&gt; 识别动词的常规表达、转喻、隐喻和异常表达;</li><li>名词间的上下位关系来识别名词性隐喻;       (语言学概念—概括性较强的单词叫做特定性较强的单词的上位词)</li><li>词语在语料库中的共现频率来识别动词和形容词性隐喻;</li><li>使用诗歌作为语料进行隐喻识别</li><li>…….</li></ul><p>最高精度为75%  —  2018 ACL Word Embedding and WordNet Based Metaphor Identification and</p><h3><span id="基于统计的方法">基于统计的方法</span></h3><p>主要思想是利用大规模的语料库进行统计和分析，再对具体的隐喻问题进行分类和识别。</p><p>隐喻识别被看作一个分类问题</p><p>使用概念的不同特征训练隐喻识别的分类器 (概念的属性特征和语义特征)</p><p>例如：</p><ul><li>使用属性特征为概念进行语义建模进而训练可解释的隐喻识别分类器</li><li>对数据库中的概念进行属性特征抽取,使用回归方法进行隐喻识别</li><li>……</li></ul><h3><span id="神经网络的方法">神经网络的方法</span></h3><p>2018 EMNLP Neural Metaphor Detection in Context  使用Bi-LSTM 识别隐喻词汇和句子分类</p><p>最高精度 2018 EMNLP </p><p>句子分类：MOH-X 79.1%<br>序列标注：MOH-X 75.6%</p><h2><span id="5-隐喻数据">5 隐喻数据</span></h2><ol><li>CCL2018  5000条人工标注隐喻数据用于评测(已过期，暂时未找到)</li><li>隐喻语料资源/知识库(英文)(<a href="http://www.jos.org.cn/html/2015/1/4669.htm#top" target="_blank" rel="noopener">http://www.jos.org.cn/html/2015/1/4669.htm#top</a>)</li></ol><p>Master Metaphor List    基于源域和目标域映射的在线知识库</p><p>Sense-Frame       词例化隐喻知识库</p><p>MetaBank、ATT-Meta、Metalude、Hamburg Metaphor Database、ItalWordNet等隐喻知识库</p><ol><li>目前最大的隐喻数据集（链接已过期）</li></ol><p>Introducing the LCC Metaphor Datasets<br><a href="https://pdfs.semanticscholar.org/edf9/b7367660d7f8255633717bf050f000a7c8fd.pdf" target="_blank" rel="noopener">https://pdfs.semanticscholar.org/edf9/b7367660d7f8255633717bf050f000a7c8fd.pdf</a></p><ol><li>Mohammad等人(2016)开发的数据集。(<a href="http://saifmohammad.com/WebPages/metaphor.html" target="_blank" rel="noopener">http://saifmohammad.com/WebPages/metaphor.html</a>)</li></ol><p>该数据集包含1230个普通文本和409个隐喻句子</p><p>2018 ACL Word Embedding and WordNet Based Metaphor Identification and Interpretation:  75%的精度</p><ol><li>TroFi and MOH\MOH-X（已获得）<a href="https://github.com/htfhxx/metaphor-in-context" target="_blank" rel="noopener">https://github.com/htfhxx/metaphor-in-context</a></li></ol><p>句子的分类</p><p>TroFi (Birke and Sarkar, 2006) </p><p>MOH (Mohammad et al., 2016)</p><p>MOH-X refers to a subset of MOH dataset used in previous work (Shutova et al., 2016)</p><ol><li><p>VUA<br>隐喻词的检测（序列标注）<br>VUA (Steen et al., 2010)<br>verb + sentence + verb_index + sentence_lable</p></li><li><p>动词的隐喻分类数据集<br><a href="https://github.com/EducationalTestingService/metaphor" target="_blank" rel="noopener">https://github.com/EducationalTestingService/metaphor</a></p></li></ol><p>2016 ACL Semantic classifications for detection of verb metaphors</p><ol><li>中文情感词汇本体库</li></ol><p>词语+情感信息（情感分类，强度，极性等）</p><p>2018 ACL Construction of a Chinese Corpus for the Analysis of the Emotionality of Metaphorical Expressions  大工-林鸿飞 </p><ol><li>其他</li></ol><p>隐喻的新颖分数</p><p>2018 AAAI Exploring the Terrain of Metaphor Novelty: A Regression-Based Approach for Automatically Scoring Metaphors</p><p>对隐喻的新颖性进行打分</p><p>对隐喻新颖性起作用的语言和概念特征</p>]]></content>
      
      
      <categories>
          
          <category> nlp_Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 隐喻识别综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《Neural Network Methods in NLP》笔记</title>
      <link href="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/"/>
      <url>/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p>在刚接触到机器学习的时候，看了周志华老师的《机器学习》，其中的概念和举例对入门机器学习很有帮助，李航老师的《统计学习方法》更是用严谨的定义和详细的公式对机器学习进行了补充。而在理论基础支撑的实践层面，使用scikit-learn库可以对<strong>结构化数据</strong>进行分析、预测等各种各样的操作。</p><p>结构化数据比较直观，可是<strong>文本数据</strong>该怎么表示和处理呢？ 《Neural Network Methods in Natural Language Processing》这本书给了答案，这本书是一本非常适合入门自然语言处理的书籍，足够薄，最关键的是有中文版。。。是哈工大车万翔老师团队翻译的，在一定程度上做到了权威。不过有的地方翻译的意思有出入，对照英文版就可以了 </p><p>本书可分为四部分。第一部分介绍神经网络的基础。第二部分介绍自然语言数据的处理。第三部分介绍特殊的深度学习结构。第四部分是一些非核心主题，我觉得相比之下，一些会议的Tutorials更值得阅读。</p><p>以下是我在阅读过程中的笔记，或者说是我摘录的重点，可以快速重复阅读的那种。</p><h2><span id="1-引言">1. 引言</span></h2><ul><li><strong>自然语言三个特性</strong>。离散性（ 语言是符号化和离散的）、组合性(字母形成单词，单词形成短语和句子) 和 稀疏性（以上性质的组合导致了数据稀疏性）</li><li><p>有两种主要的神经网络结构，即前馈网络（ feed-forward network）和循环／递归网络(recurrent/ recursive network），它们可以以各种方式组合。</p></li><li><p>循环神经网络 （RNN）是适于序列数据的特殊模型，<strong>循环网络很少被当作独立组件应用</strong>，其能力在于可被当作可训练的组件“喂”给其他网络组件，然后串联地训练它们。例如，循环网络的输出可以“喂”给前馈网络，用于预测一些值 。循环网络被用作一个输入转换器，其被训练用于产生富含信息的表示，前馈网络将在其上进行运算。</p></li><li>前馈网络，也叫 多层感知器（ Multi Layer Perceptron , MLP），其输入大小固定，对于变化的输入长度，我们可以忽略元素的顺序。</li><li><strong>循环网络打破自然语言处理中存在几十年的马尔可夫假设</strong>，设计能依赖整个句子的模型，并在需要的情况下考虑词的顺序，同时不太受由于数据稀疏造成的统计估计问题之苦。</li><li><strong>语言模型 (language modeling）指的是预测序列中下一个单词的概率</strong>（等价于预测一个序列的概率），是许多自然语言处理应用的核心 。</li><li><strong>大部分情况下，全连接前馈神经网络 CMLP 能被用来替代线性学习器</strong> 。 这包括二分类或多分类问题，以及更复杂的结构化预测问题 。网络的非线性以及易于整合预训练词嵌入的能力经常带来更高的分类精度</li></ul><h2><span id="2-学习基础与线性模型">2. 学习基础与线性模型</span></h2><h3><span id="21-有监督学习和参数化函数">2.1 有监督学习和参数化函数</span></h3><ul><li>假设类 ：搜索所有可能的程序〈或函数〉是非常困难（和极其不明确）的问题，通常把<strong>函数限制在特定的函数簇内</strong>，比如所有的具有d_in个输人、 d_out个输出的线性函数所组成的函数空间，或者所有包含 d个变量的决策树空间 。 <strong>这样的函数簇被称作假设类</strong>.</li><li>归纳偏置： 通过把搜索限制在假设类之中，我们向学习器中引入了归纳偏置（ inductive bias ）， 当学习器去预测其未遇到过的输入的结果时，会做一些假设。而学习算法中归纳偏置则是这些假设的集合，同时也使得搜索结果的过程更加高效。补充：归纳偏置可以<strong>看作学习算法自身在一个庞大的假设空间中对假设进行选择的启发式或者“价值观”</strong>。即天涯何处无芳草，却为什么偏偏选择你！！！</li><li>假设类也确定了学习器可以表示什么、不可以表示什么 。学习器的目标是确定参数的值，因此，搜索函数空间的问题被缩减成了搜索参数空间的问题 。</li><li><strong>线性函数的假设类相当有限，有很多它无法表示的函数</strong>（的确，它只限于线性关系〉。具有隐层的前馈神经网络同样是参数化函数，但构成了一个非常强大的假设类一一它们是通用近似器，可以表示任意的波莱尔可测量函数。</li><li>线性模型虽然表示能力有限，但它也有几个我们想要的性质 ： 训练简单高效，通常会得到凸优化目标，训练得到的模型也有点可解释性，它们在实践中往往非常有效 。它们也是更强大 的非线性前馈网络的基本模块。</li></ul><h3><span id="22-训练集-测试集和验证集">2.2 训练集、测试集和验证集</span></h3><ul><li>关于数据集划分：<strong>留一法——留一交叉验证</strong>：训练 k个函数  ，每次取出一个不同的样例 x作为测试集，剩余样本作为训练集，评价得到的函数 f预测的能力，最后选中一个表现最好的函数。非常浪费计算时间，只会在已标注样例数量 走特别小（小于 100 左右〉的时候使用 。在《统计学习方法》中，被叫做<strong>k折交叉验证</strong></li><li><strong>留存集</strong>：划分训练集为两个子集，可以按80% /  20%划分，在较大的子集（训练集）上训练模型，在较小的子集（留存集， held-out set ）上测试模型的准确率。《统计学习方法》中，被叫做<strong>简单交叉验证。</strong></li><li>进行划分时要注意一些事情一一通常来说在划分数据前打乱样例，保证一个训练集和留存集之间平衡的样例分布是更好的。有时随机的划分不是一个好的选择：与时间相关的训练集如果进行随机划分就会有影响。</li><li><strong>三路划分：</strong>留存集（测试集）进行误差估计不能代表新实例的效果，不知道最终的分类器的设置在总体上是好的，还是仅仅在留存集中的特定样例上是好的 。已被接受的方法是使用一种把数据划分为训练集、验证集（也叫作开发集）和测试集的三路划分方法 。 这会给你两个留存集： 一个是验证集，一个是测试集 。 所有的实验、调参 、误差分析和模型选择都应该在验证集上进行 。 然后，最终模型在测试集上的一次简单运行将会给出它在未见实例上的期望质量的一个好的估计 。</li></ul><h3><span id="23-线性模型">2.3 线性模型</span></h3><ul><li>非线性可分的情况下（不能使用一条直线或一个线性超平面将数据点分开), 解决方法<strong>要么是转换到更高维的空间</strong>（加入更多的特征），要么是<strong>转换到更丰富的假设类</strong>，或者<strong>允许一些误分类</strong>存在。</li><li><strong>特征表示</strong>：在大多数情况下这些数据点不会以特征列表的形式直接提供给我们，而是作为真实世界中的对象 。例如，我们被给予一系列公寓来分类。然后，我们需要做有意识的决定，手动选择我们认为对于分类任务有用的可测量属性 。就特征集合做出决定之后，我们创造一个特征抽取（ feature extraction）函数，它把真实世界的对象（公寓）映射成一个可测量量度（价格和大小）的向量，该向量可以作为我们模型的输入。</li><li><strong>线性模型设计的一个核心部分是特征函数的设计</strong>（所谓的<strong>特征工程</strong> ）。深度学习的开创性之一是，它通过让模型设计者指定一个小的核心、基本或者自然的特征集，让可训练的神经网络结构将它们组合成更有意义的、更高层次的特征或者表示，从而大大地简化了特征工程的过程 。</li><li><strong>sign和sigmoid</strong>：将原本值域为负无穷到正无穷的结果，使用sign函数将结果映射到[-1,+1]可以得到二分类的结果，使用sigmoid 函数将结果映射到 [0,1]范围之内，即可计算决策的置信度或者分类器分配这个类别的概率。</li></ul><h3><span id="26-对数线性多分类">2.6 对数线性多分类</span></h3><ul><li><p>多分类问题：不同于二分类的求得能将其分开的超平面的一个模型，n分类需要构建n个模型，每个模型预测一种结果的可能，最终的结果取决于最高分的模型所属于的结果。</p></li><li><p><strong>BOW( Bag-of-words）</strong>表示包含文档中所有单词的不考虑次序的信息 。一个独热表示可以被认为是一个单一单词的词袋 。<strong>CBOW(Continuous Bag Of Words) 连续单词词袋</strong>，表示可以通过求单词表示向量和或者通过将一个单词词袋向量乘以一个每一行对应于一个稠密单词表示的矩阵(嵌入矩阵 embedd ingmatricy)来得到。     1=”A B C”     2=”A D”     BOW=[A B C D]  CBOW=[A B C A D ]  </p></li><li><p><strong>Sigmoid和Softmax这两个概念有什么区别和联系</strong></p><ul><li>sigmoid把一个值映射到0-1之间，常常使用于二分类，把线性预测转变为一个概率估计，从而得到一个<strong>对数线性模型</strong>。</li></ul><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083751895.png" alt="img"></p><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083760411.png" alt="img"></p><ul><li>softmax。多分类情况中是把分数向量通过一个softmax，它将一个K维的任意实数向量映射成另一个K维的实数向量，其中向量中的每个元素取值都介于(0,1)之间并且和为1。 </li></ul></li></ul><h3><span id="27-训练和最优化">2.7 训练和最优化</span></h3><ul><li>关于损失函数：在大多数情况下，<strong>推荐使用常见的损失函数而不是自行定义</strong>。</li><li>hinge （二分类）、hinge （ 多分类 ）、对数 （log ） 损失、二元交叉熵—逻辑斯蒂 (logistic）损失、分类交叉熵 （ categorical cross-entropy） 损失（* ?）、等级损失</li><li><strong>正则化方法</strong>：L1、L2、dropout</li><li>随机梯度下降和基于minibatch的随机梯度下降：一个样本和一小批样本的区别</li></ul><h2><span id="3-从线性模型到多层感知器">3. 从线性模型到多层感知器</span></h2><ul><li>线性模型的局限性（只限于线性关系）：异或(XOR)函数,没有一条直线能够分割这两个类别。</li></ul><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083791339.png" alt="img" style="zoom: 33%;"><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083795864.png" alt="img"></p><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083791339.png" alt="img" style="zoom: 33%;"><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083795864.png" alt="img"></p><ul><li><strong>非线性输入转换</strong>：¢(x1,x2)＝［x1 * x2, x1 + x2］函数 ¢ 将数据映射为适合线性分类的表示。该解决方案有一个明显的问题，我们需要人工 。定义函数¢，此过程需要依赖特定的数据集 ，并且需要大量人类的直觉。</li><li><strong>核方法</strong>：通过定义一些通用的映射来解决上述问题。多项式映射， ¢(x)=(x)^d。d=2时，¢(x1,x2)=(x1x1,x1x2,x2x1,x2x2).对所有的变量进行两两组合.</li><li><strong>可训练的映射函数</strong>：映射函数可以采用参数化的线性模型形式，接一个作用于每一个输出维度上的非线性激活函数 g :    y= ¢(x)(W+b)， ¢(x)= g(xW’+b’)。自行定义g(x)和训练W‘、b’得到一组参数，即可获得一个映射。</li></ul><h2><span id="4-前馈神经网络">4. 前馈神经网络</span></h2><ul><li>完全连接层或仿射层：每个神经元连接到下一层中的所有神经元</li><li>最简单的神经网络称作感知器，是一个简单的线性模型 ：NN(x)=xW+b.</li><li>由线性变换产生的向量称为层 。 最外层的线性变换产生输出层，其他线性变换产生隐层 。 非线性激活操作接在每个隐层后面。由线性变换产生的层通常被称为完全连接的或仿射的。</li><li><strong>MLPl 是指带有单一隐层的多层感知机</strong></li><li>线性变换的维度计算：对一个输入维数 d_in ， 输出维数 d_out 的全连接层，有 l(x) =xW+ b ，其中 x 、 W、 b 的维数分别为 1 × d_in 、 d_in × d_out 、1× d_out。当输入的向量为行向量</li></ul><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083817021.png" alt="img"></p><ul><li><strong>常见的非线性函数</strong>:根据经验来看， ReLU 和 tanh 单元表现良好，显著地超过了 sigmoid<ul><li>sigmoid：σ(x)= l / (1 + e^(-x) )  逻辑斯蒂函数，近来被认为不适合用在神经网络的内层</li><li>tanh：tanh(x)=(e^(2x)-1) / (e^(2x)+1)    值域 [-1,1]</li><li>hard tanh=    -1(x&lt;-1)    1(x&gt;1)    x(其他)</li><li>修正线性单元（ ReLU)   ：max(0, x)  在很多任务中都表现优异，尤其是使用dropout正则化的时候</li></ul></li><li>称为权重衰减的 L2 正则化在许多情况下能有效地实现良好的泛化性能。</li><li>dropout也是一种防止神经网络过度拟合训 练数据的有效技术，旨在防止网络学习依赖于特定权重。在对每个训练样本进行随机梯度训练时，它随机丢弃网络(或特定层)中的一半神经元 。</li><li>dropout细节：考虑具有两个隐层 (MLP2 )的多层感知器：<ul><li>从x到y的过程：NN(x)=y;    h^1=g^1(xW^1+b^1);   h^2=g^2(h^1 <em> W^2+b^2);  y=h^2 </em>W^3;</li><li>当在 MLP2 中使用dropout时，我们在每轮训练中<strong>随机地设置 h^1 和 h^2 的部分值为0,</strong> 通过将h^1乘一个掩码向量(元素值为0或1)，将乘得的结果代替原隐层h^1，传入下一层。</li></ul></li><li>相似和距离层：希望基于两向量计算一个标量值，反映出两向量间的相似性、兼容性或距离 <ul><li>点积：两向量的同index数值相乘求和。</li><li>欧式距离：两向量的同index数值的差的平方和。这是一个距离度量而不是相似度, 在这里, 小值表示相似，较大值表示不相似</li><li><strong>可训练形式</strong>：有时希望使用一个参数化函数，通过关注向量的特定维度，来进行训练以产生所需的相似度。常见的可训练相似度函数是<strong>双线性形式（ bilinear form)</strong> sim(u,v)=uWv</li></ul></li></ul><p><strong>五、神经网络训练</strong></p><ul><li>计算图：一个有向无环图(DAG)，其中结点对应于数学运算或者变量，边对应于结点间计算值的流 。计算(a <em> b + 1) </em> (a * b +2): </li></ul><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083655556.png" alt="img"></p><ul><li>a * b 的计算是共享的 。有一个限制条件，就是计算图是连通的。</li></ul><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083827624.png" alt="img" style="zoom: 50%;"></p><ul><li><strong>实践经验：</strong><ul><li>优化算法：SGD收敛速度慢、Adam有效</li><li>初始化：减少陷入局部最小的可能</li><li>随机重启：多次训练，每次都随机初始化，选取验证集最好的一个</li><li>梯度消失和梯度爆炸、饱和神经元与死神经元</li><li>训练样本的随机打乱</li><li>学习率的调整</li></ul></li></ul><h2><span id="6-文本特征构造">6. 文本特征构造</span></h2><h3><span id="61-nlp-分类问题中的拓扑结构">6.1 NLP 分类问题中的拓扑结构</span></h3><p>词、文本、成对文本、上下文中的词、词之间的关系</p><h3><span id="62-nlp-问题中的特征">6.2 NLP 问题中的特征</span></h3><p>特征通常表现为标量indicator和可数count两种形式：条件是否出现，事件出现频率</p><h4><span id="621直接观测特征">6.2.1直接观测特征</span></h4><ul><li><strong>单独词特征</strong><ul><li>词元（booking, booked, books）和词干（book）</li><li>词典资源（和其他词语连接起来或者提供额外的信息）例如wordNet</li><li>分布信息（哪些词和当前词是类似的）</li></ul></li><li><strong>文本特征：字符和词再文本中的数量和次序</strong><ul><li>词袋（bag-of-words）：表示包含文档中所有单词的不考虑次序的信息 。词袋中各个词在文本中出现的数量，作为特征。</li><li>权重：TF-IDF（TF <em> IDF，词频</em>逆向文件频率）：某词对于一个语料库中的其中一份文本的重要程度。某词在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。</li><li>n元组</li></ul></li><li><strong>上下文词特征</strong><ul><li>词在句子和文本中，其在句子的位置、围绕它的词和字符都可以作为特征。越近信息量越大</li><li>窗口：{ word-2=brown, word-l=fox, word+l=over, word+2=the ｝</li></ul></li></ul><h4><span id="622-可推断的语言学特征">6.2.2 可推断的语言学特征</span></h4><ul><li>自然语言中的句子除了是词语的线性排序外还是有结构的。这种结构遵循复杂的不易于直接观察到的规律，这些规律被归类为语法。文本的语言学特性并不能够从词在句子中或其顺序的外在表现直接观察到，它们能够以不同程度的准确率从文本句子中推断出来。</li><li>语言学标注：<ul><li>成分树（ constituency tree），或短语结构树。</li><li>依存树（ dependency tree），除了句子的中心词（通常是动词），每个句子中的词都被另一个词所引领，每个词都是另一个词的修饰。</li><li>词性标签、句法角色、篇章关系、回指等概念是基于语言学理论的</li></ul></li></ul><p>6.2.3 核心特征和组合特征   多用组合特征</p><p>6.2.4 n 元组特征    在给定的长度下由连续的词序列组成，比单独的词富含更多信息</p><p>6.2.5 分布特征</p><h2><span id="7-nlp特征的案例分析">7. NLP特征的案例分析</span></h2><ul><li>7.1 文本分类：语言识别：字母级文法词袋（字母级别的特征）</li><li>7.2 文本分类：主题分类：字母级别特征不合适，词为基本单位，例如二元文法词袋等</li><li>7.3 文本分类：作者归属：避开内容词，侧重于文体属性（可以表现出来的特征list: 功能词与代词词袋，词性词袋，词性的n元文法词袋 ）</li><li>7.4 上下文中的单词：词性标注：内部线索（单词、单词的字母前缀与后缀，是否大写、是否包含连字符、是否数字）+外部线索（上下文的单词的内部线索）</li><li>7.5 上下文中的单词：命名实体识别：是一个序列分割任务，但通常被建模为序列标注任务，类似于词性标注。特征类似于词性标注，内部线索+外部线索</li><li>7.6 上下文中单词的语言特征：介词词义消歧。特征：词本身（单词、词元、词性、前后缀、词簇or分布式表示）+信息量较大的上下文（固定窗口上下文可能不富含太多信息，可用启发式规则，举例：左边第一个动词右边第一个名词形成三元组特征、依存分析器-好用），也可以用外部资源（知识库wordNet）</li><li>7.7 上下文中单词的关系：弧分解分析（依存分析任务）：给定长度为n的句子，有n*n个词关系，为每个词关系分配分数，得到一个最大化总体分数的有效的树。可构建特征：头词和修饰词的字面形式、词性、距离、方向、两者之间的词or词性等等。</li></ul><h2><span id="8-从文本特征到输入">8. 从文本特征到输入</span></h2><p>8.1 编码分类特征：独热编码(one-hot)和稠密嵌入向量</p><p>稠密表示的主要益处是具有很强的泛化能力 (Page95)</p><p><strong>8.2 组合稠密向量（个人认为这章的特征很实用）</strong></p><p>每个特征对应一个稠密向量，需要用某种方式将不同的向量组合起来，主要有拼接、相加（或者平均）和同时使用拼接与相加 。</p><ul><li><strong>基于窗口的特征</strong></li></ul><p>​       位置i为中心词时，需要编码位置在 i-2、i-1、i+1、i+2 上的词。不关心窗口内词的位置：<strong>词向量求和</strong>a+b+c+d；关心：<strong>词向量拼接</strong>[a:b:c:d]；不关心顺序但是关心距离：<strong>加权求和</strong>0.5a+b+c+0.5d（距离越远权重越小）；不关心距离但是关心前后：<strong>拼接和相加组合</strong>，[a+b:c+d] </p><p>​       显式拼接（[x: y: z]W＋b)，<strong>仿射变换</strong> （ xU+ yV+zW+ b ）,如果后者参数矩阵互不相同，则两种方式是等价的。</p><ul><li><p><strong>可变特征数目：连续词袋CBOW</strong></p><p>用于使用固定维度的向量表示任意数量的特征。</p></li></ul><p>CBOW 和传统的不考虑顺序信息的词袋表示非常相似，通过相加或者平均的方式组合特征的嵌入向量。or 加权CBOW，为不同向量赋予权重，权重可以是TF-IDF</p><p><strong>8.3 独热和稠密向量间的关系</strong></p><p>​       在训练神经网络时，使用稀疏独热向量作为输入，意味着使网络的第一层从训练数据中学习特征的稠密嵌入向量。当遇到多层前馈神经网络时，稠密和稀疏输入之间的区别会比起初看起来的更小。</p><p>独热向量，通过与一个嵌入矩阵E的乘法操作，得到稠密向量。这个矩阵E的维度是V <em> d，d是每个词的表示维度，V是单词数表示有V行，每一行代表一个独热向量的对应的稠密向量。这样的<em>*嵌入层/查找层</em></em>，即可将独热向量映射为稠密向量。</p><p><strong>8.4 杂项</strong></p><ul><li><strong>距离与位置特征</strong></li></ul><p>​        一个句子中两个词的线性距离可能作为一个提供信息的特征。在传统 NLP 情景下，通常将距离分配到若干组（如 1, 2, 3, 4, 5-10, 10+）中，并且为每一组赋予一个 one hot向量。在神经网络框架中，输入并没有直接分配一个距离数值的入口，而是将传统分组的one hot向量映射到一个类似词嵌入的d(6)维向量，这些距离嵌入向量作为网络参数进行训练（个人理解，欢迎指正，Page104有例子）</p><ul><li><p><strong>一些处理数据的tips</strong></p><ul><li><strong>补齐</strong><br>当使用拼接方法时，可以用零向量进行填充，这对于某些问题来说可能只是次优解，也许知道左侧没有修饰成分是有益的。推荐的做法是添加一个特殊符号（<strong>补齐符号</strong>〉到嵌入词表中，并且在上述情况中使用相应的补齐向量。根据要处理的问题不同，在不同情况下可能会使用不同的补齐向量 （比如左侧修饰缺失向量与右侧修饰缺失向量不同）。 这样的补齐对预测的准确性是很重要的，并且非常常用 。很多论文没提，只有源码才会看到。</li><li><strong>未登录词</strong><br>没有对应词嵌入向量时，保留一个特殊符号 UNK 表示未知记号来应对这种情况。同样对于不同的同表可能会使用不同的未知符号，但不管怎样，都不建议共享补齐和未登录向量，因为它们代表两种不同的情况 </li><li>词签名<br>处理未登录词的另一种技术是将词的形式回退到词签名 。使用 UNK 符号表示未登录词是将<strong>所有未登陆词回退到同样的签名</strong>，但是根据要解决的问题的不同，可能使用更加细粒度的策略，比如用 一ing  符号代替以 ing 结尾的未登录词，等等。</li><li>词丢弃<br>在训练集中抽取特征时，用未登录符号随机替换单词。基于词频</li><li>使用词丢弃进行正则化</li></ul></li><li><p>特征组合<br>特征设计者不仅需要人工指定感兴趣的核心特征，同时要指定特征间的交互。（如除引人“词为 X”，“词性为 Y”的特征外，还有组合特征表示“词为 X 并且词性为 Y”）</p></li><li><p>向量共享<br>构建输入时，是否需要对特征用两个不同向量表示。经验问题。</p></li><li><p>维度  ——每个特征分配的维度。速度和准确度取得一个良好平衡。</p></li><li><p>网络的输出：k分类问题输出一个k维向量，每一维表示类别强度。</p></li></ul><p>8.5  例子：词性标注     词序列提取特征的细节</p><p>8.6  例子：弧分解分析    包含两个词和中间文本的弧的特征提取细节</p><h2><span id="9-语言模型">9. 语言模型</span></h2><p>9.1 语言模型任务</p><ul><li><p>给某单词在一个词序列之后出现的可能性分配概率；给句子分配概率(一系列的词预测)</p></li><li><p><strong>马尔可夫假设</strong>：规定未来的状态和现在给定的状态是无关的。形式上， 一个走阶马尔可夫假设假设序列中下一个词<strong>只依赖</strong>于其前k个词。</p></li></ul><p>9.2 语言模型评估：困惑度</p><ul><li>以应用为中心的度量方法通过在<strong>更高级别的任务中的性能</strong>来进行评价 。 例如， 当将翻译系统中的语言模型组件从 A 替换为 B 后,测量翻译质量提高的程度 。</li><li>一个更直观的评估语言模型的方法是对于未见的句子使用<strong>困惑度</strong></li></ul><p>是一种信息论测度，用来测量一个概率模型预测样本的好坏，困惑度越低越好 。给定一个包含n个词的文本语料和一个基于词语历史的用于为词语分配概率的语言模型函数 LM ,LM 在这个语料的困惑度最低，语言模型越好。</p><p>9.3 语言模型的传统方法</p><ul><li>语言模型的传统方法假设 h 阶马尔可夫性质</li><li>预测从未在语料中观察到的序列，会出现0概率，造成极大困惑度，这是个很糟糕的情况。一种避免 0 概率事件的方法是使用平滑技术，确保为每个可能的情况都分配一个概率（可能非常小）。</li></ul><p>add-α： 它假设每个事件除了语料中观测的情况外，至少还发生 α 次</p><p>退避(back off)：如果没有观测到 h 元文法，那么就基于（k-1) 元文法计算一个估计值</p><ul><li>传统语言模型的限制：平滑技术错综复杂而且需要回退到低阶；将基于最大似然估计的语言模型应用于一个规模更大的 n 元文法是一个固有的问题；基于最大似然估计的语言模型缺乏对上下文的泛化。</li></ul><p>9.4 <strong>神经语言模型</strong></p><ul><li>非线性神经网络语言模型可以解决一些传统语言模型中的问题：它可以在增加上下文规模的同时参数仅呈线性增长，缓解了手工设计退避规则的需要，支持不同上下文的泛化性能 。</li><li>介绍：神经网络的输入是k元文法，输出是下一个词的概率分布。输入的词经过词嵌入后，被传给一个拥有一个或多个隐层的多层感知机（MLP）</li><li>训练、内存与计算效率、大规模输出空间（层次化softmax—构建huffman树、自归一化方法—没搞懂=_=）、期望特性、局限</li></ul><p>9.5 使用语言模型进行生成</p><p>9.6 副产品：词的表示</p><h2><span id="10-预训练的词表示">10. 预训练的词表示</span></h2><p>词嵌入一一将每个特征表示为低维空间中的向量。这些向量来自哪里？本章对常见方法进行了调研</p><h3><span id="101-随机初始化">10.1 随机初始化</span></h3><p>当有足够的有监督训练数据可用时－，可以将特征嵌入看作与其他模型参数相同 ： 将词嵌入向量初始化为随机值，使用网络训练过程将它们调整为“好”的 向量 。</p><h3><span id="102-有监督的特定任务的预训练">10.2 有监督的特定任务的预训练</span></h3><h3><span id="103-无监督的预训练">10.3 无监督的预训练</span></h3><p>训练词向量的技术本质上是有监督学习的技术，不是对我们关心的任务进行监督，而是<strong>从原始文本创建几乎无限数量的有监督训练实例</strong>，并希望我们创建的任务将匹配（或足够接近）我们关心的最终任务。</p><p>使用与训练的词嵌入，第一个选择是关于预处理，第二个选择是在这个任务上对预训练的向量进行微调 。</p><h3><span id="104-词嵌入算法这章建议看英文原版参考">10.4 词嵌入算法（这章建议看英文原版参考）</span></h3><p>神经网络社区倾向于从<strong>分布式表示 distributed representations</strong>的角度思考 。自然语言处理社区倾向于从<strong>分布语义 distributional semantics</strong>的角度思考</p><h4><span id="1041-分布假设和词表示-distributional-hypothesis-and-word-prepesentations">10.4.1 分布假设和词表示 Distributional hypothesis and word prepesentations</span></h4><p>explore the distributional approach to word representation <strong>词的分布表示</strong><br>关于语言和词义的分布式假设表明，在相同上下文中出现的词倾向于具有相似的含义。</p><ul><li><strong>词－上下文矩阵</strong> — i行j列，表示 i 个单词 <em> j 维度。每列表示词出现的语言学上下文。<br><em>*相似性度量</em></em>，词被表示为向量就可以通过计算向量相似度来计算词的相似度。 </li><li>余弦相似度(向量之间角度的余弦)、广义 Jaccard 相似度</li><li>词——上下文权重和<strong>点互信息(PMI)</strong><br>在求得词上下文矩阵时，其通常基于来自大规模语料库的计数。<br>PMI (w, c）通过计算词和上下文的联合概率（它们在一起的频率）与它们的边界概率（它们单独出现的频率）之间的比率的对数来测量词 w 和上下文 c 之间的关联 。减少过高的权重带来的不好的影响。（例如：the cat 和 a cat 可 以获得高于 cute cat 和 small cat 的分数，即使后者的信息量更多）</li></ul><ul><li><p>通过矩阵因式分解进行降维</p><p>将词表示为其出现的上下文的显式集合的潜在障碍是数据稀疏性。显式词向量具有非常高的维度。通过使用诸如奇异值分解(SVD)的降维技术来考虑数据的低阶表示，可以缓解这两个问题。</p></li></ul><h4><span id="1042-从神经语言模型到分布式表示-from-neural-language-models-to-distributed">10.4.2 从神经语言模型到分布式表示 From neural language models to distributed</span></h4><p>explore the distributed approaches.   <strong>词的分布式表示</strong></p><p>与词分布表示——基于计数的方法相比，神经网络社区主张使用分布式来表示词义。</p><ul><li>考虑语言建模网络：通过词嵌入矩阵和上下文矩阵确定给定上下文的不同词的概率，为上下文的k元组生成一个概率，上下文矩阵和词嵌入矩阵就是分布式的词表示：训练过程决定了词嵌入的理想值。</li></ul><ul><li>Collobert 和 Weston</li><li>word2vec：CBOW、Skip-gram(觉得这章讲的有点过于细了，有点乱)<br>word2Vec 模型训练带来了两个词嵌入矩阵 ，分别代表同和上下文矩阵 。 上下文嵌入会在训练后被丢弃，但保留了词嵌入。</li></ul><h4><span id="1043-词语联系">10.4.3 词语联系</span></h4><p>分布式“基于计数”的方法和分布式“神经”方法都是基于分布假设，尝试基于它们出现的上下文之间的相似性来捕获词之间的相似性。这两个方法之间的关系 比乍看之下更深。</p><h4><span id="1044-其他算法-nce-glove">10.4.4 其他算法    NCE   Glove</span></h4><p>10.5 上下文的选择</p><p>窗口方法(CBOW)、句子段落或文档(Skip-gram)、句法窗口（使用依存句法解析器自动解析文本，并将词的上下文视为解析树中邻近的词以及与之相关的句法关系 ）、多语种（使用多语种、基于翻译的上下文，例如与其对齐的外语单词）、基于字符级别和子词的表示（从一个词的组成字符中生成向量表示）</p><ol><li>6 处理多字单元和字变形</li></ol><ul><li>问题：1. 对多字分配单一变量、2. 形态学变形使有相同潜在概念的词不同。</li><li>可以通过对文本进行确定性预处理从而达到合理的程度，以至于更好地适应所想要的词语定义。可以生成多符号串词条列表，并用文本替换单个实体（即用 New_ York 替换 New York 的出现）</li><li>在变形的情况下，对语料预处理，包括对部分或全部词汇抽取词干，对词干嵌入而不是对其变形形式嵌入。</li></ul><ol><li>7 分布式方法的限制</li></ol><p>相似性的定义、害群之马（词的琐碎属性）、反义词（反而会出现在相似的情境）、语料库偏好、语境缺乏</p><h2><span id="11-使用词嵌入">11. 使用词嵌入</span></h2><p>本章讨论一部分词向量的用途。</p><ol><li>1 词向量的获取</li></ol><ul><li>word2Vec    可获取的单独的二进制文件、GenSim的python包、允许使用任意上下文信息的改进版二进制 Word2Vec、斯坦福发布的GloVe 模型的有效实现等等……</li></ul><ol><li>2 词的相似度（利用向量之间的相似度函数计算两个词的相似度：<strong>余弦相似度）</strong>11.3 词聚类、11 .4 寻找相似词、11. 5 同中选异、11. 6 短文档相似度、11. 7 词的类比（king - man - woman = queen）、11. 8 （对词向量的）改装和映射、11. 9 实用性和陷阱（当使用现成的词向量时，最好使用与源语料相同的切分词项的方法和文本规范化方法。仍然建议不要这样像黑盒方法一样盲目地下载和使用这些资源）</li></ol><h2><span id="12-案例分析一种用于句子意义推理的前馈结构">12. 案例分析：一种用于句子意义推理的前馈结构</span></h2><ol><li>1 自然语言推理与 SNLI 数据集</li></ol><ul><li>文本蕴含任务：两文本的关系有蕴含（前者推断出后者）、矛盾（不可同时为真）、中立</li><li>SNLI 是包括 57 万人类手写的句子对，每对都由人工标注为蕴含、矛盾和中立。</li></ul><ol><li>2 文本相似网络</li></ol><ul><li>计算句子对a、b的相似度<ul><li>依次计算a中某词与句子b中所有词的相似度，得到该词的对齐向量，softmax处理</li><li>每个对齐向量都有对应的权重</li><li>向量结果加和，传递到一个MLP分类器中，用于预测句子关系。</li><li>然后训练</li></ul></li><li>网络的目标是为了找到有助于蕴含的关键词。最终，决策网络从词对中聚合数据，并且据此提出决策 。 推断分为三个阶段：第一阶段根据相似度对齐找到较弱的局部证据，第二阶段查看带权重的多个词单元并加入方向性，第三阶段将所有局部证据整合成全局决策 。</li></ul><p>第三部分、特殊的结构（这个介绍不错，建议详细看）</p><h2><span id="13-n元语法探测器卷积神经网络">13. n元语法探测器：卷积神经网络</span></h2><ol><li>1 基础卷积＋池化</li></ol><ul><li>13.1.1 文本上的一维卷积<ul><li>卷积操作：滑动窗口向量与滤波器的内积，其后通常会使用一个非线性激活函数。</li><li><strong>宽卷积与窄卷积</strong>：窄卷积：窗口大小k，句子长度n，共有n-k+1个序列开始位置；             宽卷积：对句子每端填充k-1个填充词，n+k-1个序列开始位置(书写错了？)</li><li>两种卷积方式：实质上等价，不同的是卷积核矩阵维度为 k<em>d</em>l，和 l 个k*d维的卷积核</li><li>信道：多信道(图像数据)，通常对每个信道使用不同的滤波器集合，文本也可能多信道</li></ul></li><li>13.1.2 向量池化<ul><li>最大池化：获取整个窗口位置中最显著的信息</li><li>平均池化：一种理解是对句子中连续词袋 (CBOW) 而不是词进行卷积得到的表示。</li><li>k-max：每一维度保留前k个，同时保留文本中的顺序。保留了特征间的序关系，但对具            体位置不敏感</li><li>动态池化：卷积后得到的向量进行分组，每组分别池化，结果拼接。即分区域池化(例如关系抽取中两个词分割开的三块文本区域）</li></ul></li><li>13.1.3 变体<ul><li>平行的使用多个卷积层（窗口大小不等，捕获序列中不同长度的 n 元语法）</li></ul></li></ul><ol><li>2 其他选择：特征哈希</li></ol><p>用于文本的CNN计算开销大。直接使用 n元语法的词嵌入，然后池化得到连续词袋 n 元语法表示。使用特征哈希，n元语法通过哈希函数指派给N行词嵌入矩阵的一行，映射到[1，N]，使用对应行作为词嵌入。</p><ol><li>3 层次化卷积</li></ol><ul><li>一个窗口大小为k的卷积层学习识别输入中具有指示性的k元语法。这种方法可以扩展成层次化卷积层，卷积序列逐层相连。得到的向量捕获了句子中更有效的窗口(“感受野”)</li><li>步长、膨胀和池化</li></ul><p>膨胀的方法：步长为k-1；步长为1但是每层间使用局部池化缩短序列长度</p><ul><li>参数捆绑和跨层连接</li></ul><p>层次化卷积结构的一种变形是进行参数捆绑，对于所有参数层使用相同的参数集合 U, b 。</p><h2><span id="14-循环神经网络序列和栈建模">14. 循环神经网络：序列和栈建模</span></h2><p>处理序列：前馈网络(CBOW忽略序关系)、CNN(局限于局部)、RNN(<strong>序列输入翻译成定长向量</strong>)</p><ol><li>1 RNN 抽象描述：R、O决定网络类型</li></ol><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083912624.png" alt="img" style="zoom:67%;"></p><ol><li>2 RNN 的训练</li></ol><p>展开形式的RNN就是一个深度神经网络、不同部分在计算过程中  <strong>参数共享</strong></p><p>为了训练一个RNN，需要对输入序列构建一个展开的计算图，为展开的图添加损失节点，反向传播更新参数</p><ol><li>3 RNN 常见使用模式</li></ol><ul><li>接收器：监督信号仅置于最后的输出向量 y_n 上。观测最后一个状态，然后决策一个输出。</li><li>编码器：仅使用最后的输出向量y_n，把 y_n 与 输入相关的其他特征 进行后续任务。</li><li>传感器：对于每一个读取的输入产生一个输出（序列标注任务）</li></ul><ol><li>4 双向 RNN</li></ol><ul><li>动机：计算第i个单词，计算过程基于历史信息，然而后继的单词对于预测同样有效。</li><li>输出变动：第i个位置的输出基于<strong>前向后向两个输出向量的拼接</strong>，即同时考虑历史和未来的信息。</li><li>输出之后：拼接得到的输出向量，<strong>在接下来可以直接用来进行预测，或作为更复杂网络输入的一部分</strong></li><li>Bi-RNN 在一个输人向量对应一个输出向量的标注任务中非常有效 </li></ul><ol><li>5 堆叠 RNN</li></ol><ul><li><strong>Deep RNN:</strong>RNN逐层堆叠成网格，第n个RNN的输入是其下方RNN的输出。</li><li>在某些任务的观测经验中看， deep RNN 的确比浅层的表现更好 </li></ul><ol><li>6 用于表示梭的 RNN</li></ol><p>一些语言处理算法，需要对栈进行特征提取，RNN可以用来对整个栈的固定大小的向量编码。</p><ol><li>7 文献阅读的注意事项</li></ol><p>从学术论文的描述中推测出准确的模型形式往往是很有挑战性的 。</p><ul><li>RNN输入是独热向量还是词嵌入</li><li>输入序列有没有对开始字符和结束字符填充处理</li><li>一些论文假定RNN输出送入的softmax层是RNN自身的一部分</li><li>多层RNN中，状态向量可以是顶层输出，也可以是所有层状态向量的拼接。</li><li>在编码器－解码器框架下，作为解码器条件输入的编码器的输出可以有不同的诠释？</li></ul><h2><span id="15-实际的循环神经网络结构">15. 实际的循环神经网络结构</span></h2><ol><li>1 作为 RNN 的 CBOW</li></ol><p>选择加法函数：RNN内部是一个简单的加法：</p><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083954371.png" alt="img"></p><p>本质上是一个连续的词袋模型，输出是输入之和。</p><p><strong>15. 2 简单 RNN  ——</strong>  S-RNN</p><pre><code> 简单RNN是对前一个状态s_(i-1)和输入x_i 分别线性变换，结果相加，连同同一个偏置项，加一个激活函数tanh或ReLU。输出与隐藏状态相同。</code></pre><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083962745.png" alt="img"></p><p>S-RNN仅仅比 CBOW 稍微复杂了一点，主要不同之处在于非线性的激活函数 g 。然而，这个不同之处却至关重要，因为加入线性变换后跟随非线性变换的机制使得网络结构对于序列顺序敏感 。</p><p><strong>15. 3 门结构</strong></p><p><strong>LSTM要解决的问题：</strong>梯度消失，使得 S-RNN 很难有效地训练 。误差信号（梯度）在反向传播过程中到达序列的后面部分时迅速减少，以至于无法到达先前的输入信号的位置，这导致 S-RNN 难以捕捉到长距离依赖信息。</p><p><strong>LSTM的考虑：</strong>考虑S-RNN 其中的状态代表一个有限的记忆 。每一个单元都会读人一个输入x_i以及当前的记忆s_(i-1) ，对它们进行某种操作，并将结果写人记忆得到新的记忆状态 s_(i+1) 。从这种方式看来，S-RNN 的一个明显的问题在于记忆的获取是不受控制的。在每一步的计算过程中，整个记忆状态都被读入，并且整个记忆状态也被改写。</p><p><strong>改进：</strong>提出一种更加受控的记忆读写方式。由当前记忆状态和输入共同控制的门向量，来控制记忆状态的读写。</p><ul><li><strong>15. 3. 1 长短期记忆网络（下面有PPT做的图）</strong></li></ul><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083970476.png" alt="img"></p><ul><li><ul><li>时刻 j 的状态由两个向量组成：<strong>记忆组件c_ j 和 隐藏状态组件h_ j</strong></li><li><strong>输入门 i、遗忘门 f、输出门 o</strong>: 门的值由当前输入x_ j 和前一个状态 h_(j-1) 加一个sigmoid决定</li></ul></li></ul><ol><li><strong>遗忘门</strong>控制先前的记忆c_(j-1)，记忆c_ j 更新，</li><li><strong>产生更新候选项z：</strong> 由当前输入x_j 和前一个状态h_(j-1) 加一个tanh得到</li><li><strong>输入门</strong>控制更新候选项，记忆c_ j 更新</li><li><strong>记忆h_ j (y_j的输出) 生成</strong>，c_j 通过tanh非线性激活函数，并受输出门控制</li></ol><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083979477.png" alt="img"></p><ul><li><ul><li>实践的考虑：在训练 LSTM 网络的时候，强烈建议将遗忘门的偏置项设置为接近 1 的值 </li></ul></li><li><p><strong>15.3.2 门限循环单元</strong></p></li></ul><p>GRU是一种LSTM的替代方案。GRU 也基于门机制，但是总体上使用了更少的门并且网络不再额外给出记忆状态。</p><p><img src="/2019/04/08/nlp-research/neural-network-methods-in-nlp-bi-ji/clipboard-1579083866812.png" alt="img"></p><ul><li><ul><li>门 r 合并遗忘们和输入门，控制前一个状态s_(j-1)并计算一个新提出的 s_j~ </li><li>更新的状态s_j 由前一个状态和s_j~ 插值决定，比例通过门控z控制。</li></ul></li></ul><ol><li>4 其他变体</li></ol><ul><li>非门结构的改进</li><li>在可微门结构之外</li></ul><ol><li>5 应用到 RNN 的丢弃(dropout)机制</li></ol><p>在 RNN 中应用丢弃（ dropout）机制需要一定的技巧，因为在不同的时间点丢弃不同的维度会损害 RNN 有效携带信息的能力。</p><p>Gal 的变分 RNN 丢弃方法是目前应用于 RNN 中效果最好的丢弃机制。</p><h2><span id="16-通过循环网络建模">16. 通过循环网络建模</span></h2><ol><li>1 <strong>接收器</strong>（14. 3 有RNN 常见使用模式）</li></ol><p>接收器：监督信号仅置于最后的输出向量 y_n 上。观测最后一个状态，然后决策一个输出。读入一个序列，最后产生一个二值或者多分类的结果。</p><ul><li><ol><li><ol><li>1 情感分类器</li></ol></li></ol></li><li><ul><li>句子级情感分类任务中，给定一个句子，将这个句子分成两类中的一个 ： 积极的或消极的。这个任务通过使用 RNN 接收器能够很直接地进行建模： RNN 读人序列化后的词语 。RNN 的最终状态送人一个 MLP ，这个 MLP 的输出层是一个二输出的 softmax 层 。</li><li>文本级的情感分类。<strong>层次化结构的方法</strong>：每个句子通过门结构的RNN编码得到一个向量，然后将向量集合送入第二个门结构的RNN中，得到最终的向量，套上MLP和softmax用来预测。</li></ul></li><li><ol><li><ol><li>2 主谓一致语法检查</li></ol></li></ol></li><li><ul><li>英语句子必须要遵守“现在时态谓语动词的单复数必须与动词的主语一致”的规则</li><li>模型直接作为一种接收器被训练。这是一项困难的任务，只有非常间接的监督：监督信号不包括任何找到语法信息的线索 。</li><li>RNN 处理学习任务表现非常好，成功解决了测试集中的绝大多数句子</li></ul></li></ul><p>16.2 <strong>作为特征提取器的 RNN</strong></p><p>RNN 的一个最主要的应用场景就是作为灵活可训练的特征提取器，在处理序列问题时能够替代传统的特征提取通道 。 特别地， RNN 是基于窗口的特征提取器的良好替代者 。</p><ul><li><ol><li><ol><li>1 <strong>词性标注</strong></li></ol></li></ol></li><li><ul><li>n个词语的句子，使用特征提取函数转换为输入向量，输入到双层RNN中，产生的输出向量被送入MLP中，用于从可能的 h 个标签中预测这个词语的标签 。</li><li>通过训练程序，双向 RNN 将着重于学习序列中 预测词语标签 所需要的那些信息 ，并将其编码在输出向量中 </li><li>词语转换成输入向量：可以通过使用一个词嵌入矩阵，也可以通过字符级的 RNN 将词语转化为输入，使用字符级的卷积和池化神经网络来表示词语，</li></ul></li><li><ol><li><ol><li>2 <strong>RNN-CNN 文本分类</strong></li></ol></li></ol></li><li><ul><li>与标注任务相同的方式，将向量送入逐个读入单词的字符级RNN 或逐个读入单词的卷积一池化层中得到结果</li><li>或在字符上应用一个层次化卷积一池化网络，得到更短的向量序列，把结果的向量序列送入RNN 及分类层中</li></ul></li><li><ol><li><ol><li>3 <strong>弧分解依存句法分析</strong></li></ol></li></ol></li><li><ul><li>输入词语、词性标签和对应的特征向量，投入到RNN中，得到每个词语的编码。将这些编码进行两两拼接送入MLP中，得到一个核心词-修饰词 候选对的打分。</li><li>如果一个任务对词语顺序或者句子结构敏感，并且这个任务使用词语作为特征，那么这个任务中的<strong>词语</strong>就能够<strong>被使用词语自身训练的双向 LSTM输出的词向量代替</strong> 。</li></ul></li></ul><p><strong>十七、条件生成</strong></p><ol><li>1 RNN 生成器</li></ol><p>使用 RNN 转换器结构进行语言建模的一个特殊例子：<strong>序列生成</strong>。i 时刻的输出作为i+1时刻输入。</p><p>生成器的训练：将其当做<strong>转换器</strong>训练。长度为n的句子，生成具有n+1个输入和n+1个输出的RNN转换器，其中第一个输入是句首符，最后一个输出是句尾符。</p><p>局限性：不能更好的处理与黄金序列（观测序列）之间的偏差。在训练时，生成器使用实际词作为每一时刻的输入，而不是前一时刻的预测结果。</p><p><strong>17.2 条件生成（编码器－解码器）</strong></p><p>RNN 转换器的能力直到应用于条件生成的框架中才真正地显现出来 。</p><p>条件生成的框架中，下一个词项的生成依赖于己生成的词项以及一个额外的条件上下文 c，几乎任何我们能够获得并且认为有用的数据都可以被编码至上下文c</p><ul><li><ol><li><ol><li>1 序列到序列模型</li></ol></li></ol></li></ul><p>上下文 c 可以有很多种形式。一种方法是c本身就是一个序列，最常见的是一个文本片段。这种方式下产生了<strong>序列到序列（ sequence to sequence）条件生成框架</strong>，也被称为<strong>编码器一解码器框架</strong>。</p><p>给定一个长度n的源序列，生成一个长度m的输出序列。<strong>通过一个编码函数(通常是RNN)将源句子编码，再使用一个条件生成器（解码器）生成输出序列。</strong></p><p>编码器将源句子抽象表示为向量 c，作为解码器的 RNN 则根据当前已预测出的词以及编码后的源句子 c 来预测目标词序列。</p><p>用于编码器与解码器的 RNN 是<strong>联合训练</strong>的 。 监督信息只出现在解码器 RNN 端，但是梯度能够沿着网络连接反向传播至编码器 RNN 中</p><ul><li><p>17.2.2 应用</p></li><li><ul><li>机器翻译</li><li>邮件自动回复</li><li>形态屈折 — 输入基本词和期望的形态变化需求，输出该词的曲折形式</li><li>几乎所有任务都可以用编码-生成的方法建模，但也许存在更适合此任务的结构</li></ul></li><li><p>\17. 2.3 其他条件上下文</p></li></ul><p>条件上下文向量可以是基于单个词，或一种连续词袋 CCBOW）的编码，也可以由 一个卷积网络生成，或基于一些其他的复杂计算 。甚至不一定基于文本(对话任务用户的信息)</p><p>17.3 无监督的句子相似性</p><ul><li>为句子学习向量表示，将相似的句子编码为相近的向量。</li><li>大部分方法是基于序列到序列框架的：首先训练一个 RNN 编码器来产生上下文向量表示 c，该向量将用于一个 RNN 解码器来完成某个任务 。</li><li>作为解码器的 RNN 将被丢弃，而编码器则用来生成句子表示 c，最终得到的句子间相似度函数将严重依赖于训练解码器所完成的任务 。</li><li>这些任务可以是：自动编码、机器翻译、skip-thought、句法相似度</li></ul><p><strong>17.4 结合注意力机制的条件生成</strong></p><ul><li><p>编码器-解码器结构<strong>强制编码器中包含生成时所需要的全部信息</strong>，要求生成器中从该定长向量中提取出所有信息。  增加一个注意力机制，可以充分改进。</p></li><li><p>结合注意力机制的<strong>编码器-解码器结构预测过程</strong>：</p></li><li><ul><li>编码器对长度为n的输入序列编码，产生n个向量。</li><li>预测第j+1个词项时，注意力由前 j 个编码器输出的向量和前 j 个词项决定</li><li>解码器第j+1个状态，  由注意力、前 j 个词项、第 j 个状态决定</li><li>由解码器的最终状态，得到第j+1个词项</li></ul></li><li><p><strong>注意力函数</strong>是一个可训练、参数化的函数。</p></li><li><ul><li>注意力权重向量由解码器的第 j 个输出、编码器的每个向量，+MLP，决定</li><li>再使用 softmax 函数将权重归一化至一个概率分布</li><li>最终权重与编码器的输出，结合得到最终的注意力</li></ul></li><li><p>为何不省去编码器，把注意力机制直接作用于源序列？编码过程有重要的收益。</p></li><li><p>计算的复杂性：</p></li><li><ul><li>不含注意力的编码O(n)解码O(m)，整体时间复杂度是O(m+n)，</li><li>含注意力，编码O(n)，解码每一步需要计算注意力O(n)，整体O(m*n)</li></ul></li><li><p>可解释性：生成的注意力权重可以用来观察在产生当前输出时解码器认为源序列中哪些区域是相关的 。</p></li></ul><ol><li>5 自然语言处理中基于注意力机制的模型</li></ol><ul><li>机器翻译     子词单元  融合单语数据   语言学标注</li><li>形态屈折 — 输入基本词和期望的形态变化需求，输出该词的曲折形式</li><li>句法分析</li></ul>]]></content>
      
      
      <categories>
          
          <category> nlp_Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 《Neural Network Methods in NLP》笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>近五年(2014-2018)中文分词论文及趋势整理</title>
      <link href="/2018/12/28/nlp-research/jin-wu-nian-2014-2018-zhong-wen-fen-ci-lun-wen-ji-qu-shi-zheng-li/"/>
      <url>/2018/12/28/nlp-research/jin-wu-nian-2014-2018-zhong-wen-fen-ci-lun-wen-ji-qu-shi-zheng-li/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h2><span id="1-前言">1. 前言</span></h2><p>整理了近五年中文分词的文章，意图探索研究趋势。</p><p>文章来源来自于几大nlp顶会：ACL、EMNLP、COLING、NAACL.</p><p>在阅读16、17年文章的时候，发现了引用了这些文章的来自于AAAI、IJCAI的一些文章，也算是一个小Tip。</p><p>主要从以下几方面进行总结：</p><pre><code>论文名作者机构主要工作（解决了什么）主要方法（使用了什么）数据集取得的成果缺陷和不足下一步工作</code></pre><p>简要的统计与总结如下：</p><div class="table-container"><table><thead><tr><th>论文数量</th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th><th>总计</th></tr></thead><tbody><tr><td>ACL</td><td>5</td><td>3</td><td>4</td><td>3</td><td>0</td><td>15</td></tr><tr><td>EMNLP</td><td>3</td><td>2</td><td>0</td><td>2</td><td>1</td><td>8</td></tr><tr><td>COLING、NAACL</td><td>1-0</td><td>0-0</td><td>0-1</td><td>0-0</td><td>1-0</td><td>3</td></tr><tr><td>AAAI、IJCAI</td><td>/-/</td><td>/-/</td><td>/-/</td><td>0-2</td><td>1-1</td><td>4+</td></tr><tr><td>总计</td><td>9</td><td>5</td><td>5</td><td>7</td><td>4</td></tr></tbody></table></div><p>ps：“/”代表未进行统计</p><p>自己总结的一些趋势（不够专业，仅供参考）：</p><div class="table-container"><table><thead><tr><th></th><th>2014</th><th>2015</th><th>2016</th><th>2017</th><th>2018</th></tr></thead><tbody><tr><td>无监督</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>半监督</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr><tr><td>有监督</td><td>6</td><td>4</td><td>5</td><td>6</td><td>3</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>神经网络</td><td>1</td><td>3</td><td>5</td><td>6</td><td>4</td></tr><tr><td>传统方法</td><td>8</td><td>2</td><td>0</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>联合建模</td><td>0</td><td>0</td><td>1</td><td>1</td><td>0</td></tr><tr><td>特定领域</td><td>专利</td><td></td><td></td><td>微文本</td><td>医学</td></tr><tr><td>重点在于搭建or改进神经网络</td><td>1/9</td><td>3/5</td><td>3/5</td><td>3/7</td><td>2/4</td></tr><tr><td>解决领域适配问题</td><td>3/9</td><td>1/5</td><td>0/5</td><td>2/7</td><td>3/4</td></tr></tbody></table></div><h2><span id="2-具体整理内容">2. 具体整理内容</span></h2><h3><span id="21-2014年">2.1 2014年</span></h3><p><strong>论文名：A Joint Model for Unsupervised Chinese Word Segmentation</strong></p><p><strong>作者：</strong>Miaohong Chen  Baobao Chang  Wenzhe Pei</p><p><strong>机构：</strong>教育部计算语言学重点实验室， 北京大学电子工程与计算机科学学院</p><p><strong>主要工作（解决了什么）：</strong>一种无监督中文分词的联合模型，提升了分词的精读</p><p><strong>主要方法（使用了什么）：</strong>结合了基于字符的模型、非参数贝叶斯语言模型和基于良好性的模型的优点。</p><p><strong>数据集：</strong> PKU and MSRA </p><p><strong>取得的成果：</strong>提高了无监督分词模型的精度、较强的中文分词歧义解决能力</p><p><strong>论文名：Effective Document-Level Features for Chinese Patent Word Segmentation</strong></p><p><strong>作者：</strong>Si Li     Nianwen Xue</p><p><strong>机构：</strong>Brandeis大学中文处理组</p><p><strong>主要工作（解决了什么）：</strong>手工分割了大量的专利数据，设计了文档级的特性来捕捉专利中科学和技术术语的分布特征，</p><p><strong>主要方法（使用了什么）：</strong>CRF模型，提取了字符、词语、文档级别的特征</p><p><strong>数据集：</strong>142 Chinese patents（自己标注）</p><p><strong>取得的成果：</strong>解决了专利领域的分词问题</p><p><strong>论文名：Domain Adaptation for CRF-based Chinese Word Segmentation using</strong></p><p><strong>Free Annotations</strong></p><p><strong>作者：</strong>Yijia Liu , Yue Zhang , Wanxiang Che , Ting Liu , Fan Wu</p><p><strong>机构：</strong>新加坡科技与设计大学，麻省理工学院社会计算和信息检索研究中心，哈尔滨工业大学</p><p><strong>主要工作（解决了什么）：</strong>通过将各种免费标注源转换为部分标注数据的一致形式。并构造一种可以同时使用全部标注和部分标注数据进行训练的CRF变体，研究了分词的<strong>领域适配问题</strong>。</p><p><strong>数据集：</strong>自由数据+免费标注数据</p><p><strong>取得的成果：</strong>研究了分词的<strong>领域适配问题。</strong>自由数据的有效性，发现它们对于提高分割精度是有用的。</p><p><strong>论文名：</strong>Max-Margin Tensor Neural Network for Chinese Word Segmentation</p><p><strong>作者：</strong>Wenzhe Pei Tao Ge Baobao Chang∗</p><p><strong>机构：</strong>教育部计算语言学重点实验室，北京大学电子工程与计算机科学学院</p><p><strong>主要工作（解决了什么）：</strong>提出了一种新的中文分词神经网络模型——最大边缘张量神经网络(MMTNN)，显式地模拟了标签和上下文字符之间的交互。提出了一种有效地提高模型效率和避免过拟合风险的张量因子分解方法。</p><p><strong>主要方法（使用了什么）：</strong>模型明确地模拟了标签和上下文字符之间的交互，从而获取了更多的语义信息。在神经网络模型中引入张量分解用于序列标记任务，加快了模型的训练和推理，防止了过拟合。</p><p><strong>数据集：</strong>PKU and MSRA datasets</p><p><strong>取得的成果：</strong>我们的模型比以前的神经网络模型具有更好的性能，并且我们的模型可以在最小的特征工程条件下获得具有竞争力的性能。</p><p><strong>下一步工作：</strong>进一步扩展模型，并将其应用于其他结构预测问题。</p><p><strong>论文名：</strong>Automatic Corpus Expansion for Chinese Word Segmentation by Exploiting the Redundancy of Web Information</p><p><strong>作者：</strong>Xipeng Qiu, ChaoChao Huang and Xuanjing Huang</p><p><strong>机构：</strong>上海市智能信息处理重点实验室，复旦大学计算机科学学院</p><p><strong>主要工作（解决了什么）：</strong>处理一个新的领域而没有足够的标注语料库时，监督方法就不能很好地工作。</p><p>提出了一种自动扩充外域文本训练语料库的方法</p><p><strong>主要方法（使用了什么）：</strong>通过网络提供的大量相关的容易分割的句子，来分割一个复杂和不确定的分段。挑选出一些可靠的分段句，并将它们添加到语料库中，从而扩充语料库。</p><p><strong>数据集：</strong>both CTB6.0 and CTB7.0 datasets</p><p><strong>取得的成果：</strong>提出了一种自动扩充外域文本训练语料库的方法</p><p><strong>下一步工作：</strong>我们的方法的长期目标是建立一个在线的、不断学习的系统，能够识别困难的任务，并从众包中寻求帮助。</p><p><strong>论文名：</strong>Two Knives Cut Better Than One:Chinese Word Segmentation with Dual Decomposition</p><p><strong>作者：</strong>Mengqiu Wang、Rob Voigt、Christopher D. Manning</p><p><strong>机构：</strong>Stanford University  （Computer Science &amp; Linguistics） Department</p><p><strong>主要工作（解决了什么）：</strong>中文分词主要有两种方法:基于单词的分词和基于字符的分词，两种方法各具优势。先前的研究表明，将这两种模型结合起来可以提高分割性能。提出了一种有效地结合两种分割方案的强度的方法，使用一种有效的双分解算法进行联合推理。</p><p><strong>主要方法（使用了什么）：</strong></p><p><strong>数据集：</strong>SIGHAN 2005  第二届国际汉语分词</p><p><strong>取得的成果：</strong>本文提出了一种基于对偶分解的中文分词方法。可以对现有的CWS系统进行联合解码，比单独的任何一个系统都更加准确和一致，实现了迄今为止在该任务的标准数据集上所报告的最佳性能。</p><p><strong>论文名：</strong>Empirical Study of Unsupervised Chinese Word Segmentation Methods for SMT on Large-scale Corpora</p><p><strong>作者：</strong>Xiaolin Wang、 Masao Utiyama 、Andrew Finch、 Eiichiro Sumita</p><p><strong>机构：</strong>国家信息与通信技术研究所 （日本？）</p><p><strong>主要工作（解决了什么）：</strong>无监督分词(UWS)可以在没有标注数据的情况下为统计机器翻译(SMT)提供领域自适应分词，双语的UWS甚至可以优化分词的对齐。提出了一种有效的统一的基于pyp的单语和双语无监督方法。</p><p><strong>主要方法（使用了什么）：</strong>本研究旨在为统计机器翻译建立一个更好的中文分词模型。该算法利用基于双语特征的对齐自动学习的词界信息，提出了一种较好的分割模型。</p><p><strong>数据集：</strong>Chinese SIGHAN-MSR corpus</p><p><strong>取得的成果：</strong>提高无监督分词模型的精度</p><p><strong>论文名：</strong>Semi-Supervised Chinese Word Segmentation Using Partial-Label Learning With Conditional Random Fields 基于条件随机场的部分标签学习的半监督中文分词方法</p><p><strong>作者：</strong>Fan Yang 、Paul Vozila </p><p><strong>机构：</strong>Nuance Communications Inc. 从事语音识别软件、图像处理软件及输入法软件研发销售的公司</p><p><strong>主要工作（解决了什么）：</strong>Wikipedia数据中的标点符号和实体标记在句子中定义了一些单词边界。采用条件随机场的部分标签学习方法，将这些有价值的知识用于半监督汉语分词。</p><p><strong>数据集：</strong>CTB-6 corpus</p><p><strong>取得的成果：</strong>CTB-6 test set is 95.98% in F-measure</p><p><strong>论文名：</strong>Toward Better Chinese Word Segmentation for SMT via Bilingual Constraints</p><p><strong>作者：</strong>Xiaodong Zeng、 Lidia S. Chao、 Derek F. Wong、 Isabel Trancoso、 Liang Tian</p><p><strong>机构：</strong>澳门大学计算机与信息科学系</p><p><strong>主要工作（解决了什么）：</strong>针对SMT任务提出了一种新的CWS模型。该模型的目的是维护树库数据的语言分割监督，同时集成由bitexts产生的有用的双语分割。</p><p><strong>主要方法（使用了什么）：</strong>从基于字符的对齐中学习词的边界; 将学习到的单词边界编码为GP约束; 在GP约束下，利用PR框架对CRFs模型进行训练。</p><p><strong>数据集：</strong>BLEU、NIST、METEOR 机器翻译质量</p><p><strong>取得的成果：</strong>该模型能较好地实现用于机器翻译的分词</p><h3><span id="22-2015年">2.2 2015年</span></h3><p><strong>论文名：</strong>Semi-supervised Chinese Word Segmentation based on Bilingual</p><p>Information</p><p><strong>作者：</strong>Wei Chen、Bo Xu</p><p><strong>机构：</strong>自动化所</p><p><strong>主要工作（解决了什么）：</strong>半监督的分词模型，包含三个层次的双语语言特征学习的层叠对数线性模型</p><p><strong>数据集：</strong>1998人民日报语料库、Bakeoff-2  PKU、AS</p><p><strong>取得的成果：</strong>精度超过优于目前最先进的单语和双语半监督方法。</p><p><strong>论文名：</strong>Gated Recursive Neural Network for Chinese Word Segmentation</p><p><strong>作者：</strong>Xinchi Chen, Xipeng Qiu∗ , Chenxi Zhu, Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海市智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>提出了一种用于中文分词的门控递归神经网络(GRNN)。由于GRNN相对较深，我们还采用了一种有监督的分层训练方法来避免梯度扩散问题。</p><p><strong>数据集：</strong>PKU, MSRA and CTB6  NLPCC 2015 dataset微博文本</p><p><strong>取得的成果：</strong>模型优于以往的神经网络模型和最先进的方法。</p><p><strong>下一步工作：</strong>GRNN在其他序列标记任务上的应用。</p><p><strong>论文名：</strong>Long Short-Term Memory Neural Networks for Chinese Word Segmentation</p><p><strong>作者：</strong>Xinchi Chen, Xipeng Qiu∗ , Chenxi Zhu, Pengfei Liu, Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海市智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>提出了一种新的汉语分词神经网络模型，采用长短时记忆(LSTM)神经网络</p><p><strong>数据集：</strong>PKU, MSRA and CTB6 </p><p><strong>取得的成果：</strong>模型优于以往的神经网络模型和最先进的方法。</p><p><strong>下一步工作：</strong>改为双向</p><p><strong>论文名：</strong>Synthetic Word Parsing Improves Chinese Word Segmentation</p><p><strong>作者：</strong>Fei Cheng、 Kevin Duh、 Yuji Matsumoto</p><p><strong>机构：</strong>奈良信息科学研究所</p><p><strong>主要工作（解决了什么）：</strong>提出了一种利用人工分词器提高汉语分词性能的新方法，该解析器分析单词的内部结构，并试图将未登录词转换为词汇内的细粒度子单词。我们提出了一个管道CWS系统，该系统首先预测这种细粒度的分割，然后对输出进行分段，以重构原始的分词标准。</p><p><strong>数据集：</strong>PKU和MSR</p><p><strong>取得的成果：</strong>提升了精度。在OOV召回方面有了实质性的改进。</p><p><strong>论文名：</strong>Accurate Linear-Time Chinese Word Segmentation via Embedding Matching</p><p><strong>作者：</strong>Jianqiang Ma 、Erhard Hinrichs</p><p><strong>机构：</strong>德国图宾根大学语言学部</p><p><strong>主要工作（解决了什么）：</strong>本文提出了一种嵌入匹配的中文分词方法，该方法对传统的序列标记框架进行了推广，并利用了分布式表示的优点。在此基础上，提出了一种基于基准语料库的贪心分割算法。</p><p><strong>数据集：</strong>PKU  MSR</p><p><strong>取得的成果：</strong>提升了精度</p><h3><span id="23-2016年">2.3 2016年</span></h3><p><strong>论文名：</strong>Neural Word Segmentation Learning for Chinese</p><p><strong>作者：</strong>Deng Cai and Hai Zhao∗</p><p><strong>机构：</strong>上交计算机系</p><p><strong>主要工作（解决了什么）：</strong>在本文中，我们提出了一种新的神经网络框架，它彻底消除了上下文窗口，可以利用完整的分割历史。我们的模型采用基于字符的门控组合神经网络生成候选词的分布式表示，然后将其输入长期短期记忆(LSTM)语言评分模型。</p><p><strong>主要方法（使用了什么）：</strong>新的中文分词神经网络框架</p><p><strong>数据集：</strong>PKU、MSR</p><p><strong>取得的成果：</strong>提升精度。神经网络分词模型</p><p><strong>论文名：</strong>An Empirical Study of Automatic Chinese Word Segmentation for Spoken Language Understanding and Named Entity Recognition 汉语自动分词用于口语理解和命名实体识别的实证研究</p><p><strong>作者：</strong>Wencan Luo∗     Fan Yang</p><p><strong>机构：</strong>匹兹堡大学</p><p><strong>主要工作（解决了什么）：</strong>分词通常被认为是许多汉语自然语言处理任务的第一步，但它对这些后续任务的影响却相对缺乏研究。在对新数据应用现有的分词器时，如何解决不匹配问题?一个更好的分词器会产生更好的NLP后续任务性能吗?</p><p><strong>主要方法（使用了什么）：</strong>将分词输出作为附加特征，采用部分学习的自适应技术，利用n-best分词表。</p><p><strong>数据集：CTB6、PKU、NER、</strong>NER 3-fol</p><p><strong>取得的成果：</strong>解决分词应用到新数据不匹配的问题—3个方法。（提高了SLU、NER精度）</p><p><strong>论文名：</strong>A New Psychometric-inspired Evaluation Metric for Chinese Word Segmentation</p><p><strong>作者：</strong>Peng Qian Xipeng Qiu∗ Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>一种新的基于心理测量学的汉语分词评价指标</p><p><strong>主要方法（使用了什么）：</strong>从心理测量学的基本思想入手，在困难与容易、奖励与惩罚之间取得平衡，提高评价的准确性。</p><p><strong>数据集：PKU、MSR、NCC</strong></p><p><strong>取得的成果：</strong>一种新的基于心理测量学的汉语分词评价指标。实际评价结果表明，所提出的评价指标给出的评价结果更加合理、易于区分，与人的评价结果具有较好的相关性。传统的精度、召回率和F-score。</p><p><strong>论文名：</strong>Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation</p><p><strong>作者：</strong>Jingjing Xu and Xu Sun</p><p><strong>机构：</strong>北京大学计算机语言学教育部重点实验室</p><p><strong>主要工作（解决了什么）：</strong>一种基于依赖的门控递归神经网络</p><p><strong>主要方法（使用了什么）：</strong>为了将局部特征与长距离依赖相结合，提出了一种基于依赖的门控递归神经网络。局部特征首先由双向长短期记忆网络收集，然后通过门控递归神经网络将其组合并细化为长距离依赖。</p><p><strong>数据集：PKU、MSRA、CTB6</strong></p><p><strong>取得的成果：提高了精度</strong></p><p><strong>论文名：</strong>Transition-Based Neural Word Segmentation</p><p><strong>作者：</strong>Meishan Zhang1 and Yue Zhang2 and Guohong Fu1</p><p><strong>机构：</strong>黑龙江大学计算机科学与技术学院</p><p><strong>主要工作（解决了什么）：</strong>一种基于词的中文分词神经模型，将人工设计的离散特征替换为基于词的分词框架中的神经特征。</p><p><strong>数据集：</strong>PKU、MSR</p><p><strong>取得的成果：</strong>基于词的汉语分割神经网络模型。该模型可以方便地利用离散特征，从而得到了一个与以往工作相比性能最好的组合模型。提高精度。</p><h3><span id="24-2017年">2.4 2017年</span></h3><p><strong>论文名：</strong>Fast and Accurate Neural Word Segmentation for Chinese</p><p><strong>作者：</strong>Deng Cai1,2, Hai Zhao1,2,∗, Zhisong Zhang1,2, Yuan Xin3, Yongjian Wu3, Feiyue Huang3</p><p><strong>机构：</strong>上海交通大学计算机科学与工程系</p><p><strong>主要工作（解决了什么）：</strong>现有神经模型的训练和工作过程都存在计算效率低下的问题。提出了一种具有均衡的字和字符嵌入输入的贪婪神经词分割器，以克服现有的缺陷。</p><p><strong>主要方法（使用了什么）：</strong>一种新型的字符 - 字平衡机制，用于字的表示生成。通过去掉不必要的设计，建立更有效的字符组合模型。最大间隔训练期间的早期更新策略。</p><p><strong>数据集：PKU  95.8、MSR 97.1 （速度快）</strong></p><p><strong>取得的成果：</strong>更简单、更快、更准确。神经网络模型</p><p><strong>论文名：</strong>Adversarial Multi-Criteria Learning for Chinese Word Segmentation </p><p>汉语分词的对抗性多准则学习</p><p><strong>作者：</strong>Xinchi Chen, Zhan Shi, Xipeng Qiu∗ , Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>提出了针对CWS的对抗性多准则学习，将来自多个异构分割准则的共享知识集成在一起。</p><p><strong>数据集：</strong>MSRA AS PKU CTB CKIP CITYU NCC SXU</p><p><strong>取得的成果：</strong>在8个具有异构分割标准的语料库上的实验表明，与单标准学习相比，每个语料库的性能都有了显著的提高。</p><p><strong>论文名：</strong>Multi-Grained Chinese Word Segmentation     </p><p><strong>作者：</strong>Chen Gong, Zhenghua Li∗ , Min Zhang, Xinzhou Jiang</p><p><strong>机构：</strong>苏州大学</p><p><strong>主要工作（解决了什么）：</strong>利用三个SWS数据集的注释异构性，构建了用于模型训练和调优的大型伪MWS数据集。然后我们用真正的MWS注释手工注释1500个测试句子。提出了三种将MWS转换为构成解析和序列标记的基准测试方法。</p><p><strong>数据集：CTB、MSR、PPD</strong></p><p><strong>取得的成果：</strong>提出并解决了多粒度中文分词。</p><p><strong>论文名：</strong>Neural Word Segmentation with Rich Pretraining</p><p><strong>作者：</strong>Jie Yang∗ and Yue Zhang∗ and Fei Dong</p><p><strong>机构：</strong>新加坡科技与设计大学</p><p><strong>主要工作（解决了什么）：</strong>对大型中文文本进行预处理，嵌入字符和单词，可以提高分割准确率</p><p><strong>主要方法（使用了什么）：</strong>预训练是利用外部资源提高精度的一种方法</p><p><strong>数据集：</strong>CTB6  96.2  PKU 96.3  MSR 97.5  AS 95.7  CityU 96.9  Weibo 95.5</p><p><strong>取得的成果：</strong>研究了丰富的外部资源（预训练方法），以加强神经分词。。</p><p><strong>论文名：</strong>Segmenting Chinese Microtext: Joint Informal-Word Detection and Segmentation - IJCAI</p><p>with Neural Networks</p><p><strong>作者：</strong>Meishan Zhang, Guohong Fu∗, Nan Yu</p><p><strong>机构：</strong>黑龙江大学</p><p><strong>主要工作（解决了什么）：</strong>微文本中的非正式词汇问题</p><p><strong>主要方法（使用了什么）：</strong>提出了一种通过分词和非正式词检测同时进行的方法来增强中文微文本分割的联合模型。</p><p><strong>数据集：</strong>CTB6.0 and the released Weibo corpus</p><p><strong>取得的成果：</strong>该联合模型可以显著提高微博数据集的分割性能，提高幅度超过3%。</p><p><strong>论文名：</strong>Word-Context Character Embeddings for Chinese Word Segmentation</p><p><strong>作者：</strong>Hao Zhou∗ Zhenting Yu∗ Yue Zhang  Shujian Huang  Xinyu Dai Jiajun Chen</p><p><strong>机构：</strong>南京大学</p><p><strong>主要工作（解决了什么）：</strong>本文提出了一种基于分词标签信息的文本上下文字符嵌入方法。该方法将标签分布信息打包到嵌入中，可以看作是知识参数化的一种方式。</p><p><strong>数据集：</strong>CTB6 96.2    PKU 96.0   MSR    97.8</p><p><strong>取得的成果：</strong> 提出了半监督神经网络的词-上下文字符嵌入，使分割模型对域内数据更加准确，对域外数据更加稳健。</p><p><strong>论文名：</strong>A Feature-Enriched Neural Model for Joint Chinese Word Segmentation and Part-of-Speech Tagging</p><p><strong>作者：</strong>Xinchi Chen, Xipeng Qiu∗ , Xuanjing Huang</p><p><strong>机构：</strong>复旦大学上海智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>本文提出了一种用于汉语分词和词性标注联合作业的特征丰富的神经模型。具体来说，为了模拟传统离散特征模型的特征模板，我们使用不同的滤波器对复杂的混合特征进行卷积和池化建模，然后利用递归层的长距离依赖信息。</p><p><strong>数据集：CTB、PKU、NCC</strong></p><p><strong>取得的成果：</strong> joint S&amp;T task 词性标注精度提高</p><h3><span id="25-2018年">2.5 2018年</span></h3><p><strong>论文名：</strong>State-of-the-art Chinese Word Segmentation with Bi-LSTMs</p><p><strong>作者：</strong>Ji Ma、 Kuzman Ganchev、 David Weiss</p><p><strong>机构：</strong>Google AI Language</p><p><strong>主要工作（解决了什么）：改进模型—</strong>pretrained embeddings、dropout、超参数调整</p><p><strong>数据集：</strong>AS  CITYU CTB6 CTB7 MSR PKU UD</p><p> 98.03 98.22 97.06 97.07 98.48 97.95 97.00</p><p><strong>取得的成果：</strong>为汉语分词的进一步发展提供了两条重要的证据。首先，比较不同的模型体系结构需要仔细地调整和应用最佳实践，以便获得严格的比较。其次，如果不进一步努力收集数据，对神经体系结构的迭代可能不足以解决剩余的类分割错误。</p><p><strong>论文名：</strong>Adaptive Multi-Task Transfer Learning for Chinese Word Segmentation in Medical Text</p><p><strong>作者：</strong>Junjie Xing、 Kenny Q. Zhu、 Shaodian Zhang</p><p><strong>机构：上交</strong></p><p><strong>主要工作（解决了什么）：</strong>提出了一个自适应多任务转移学习框架和三个不同设置的模型实例。</p><p><strong>数据集：</strong>PKU  MSR   WEIBO</p><p><strong>取得的成果：</strong>提出了一种新的医学领域中文分词框架。提高了医学领域分词。三个医学数据集的标注工作。</p><p><strong>论文名：</strong>Neural Networks Incorporating Unlabeled and Partially-labeled Data for Cross-domain Chinese Word Segmentation</p><p><strong>作者：</strong>Lujun Zhao, Qi Zhang, Peng Wang, Xiaoyu Liu</p><p><strong>机构：</strong>复旦大学智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong></p><p><strong>主要方法（使用了什么）：</strong>提出了一种新的神经网络模型，利用未标记和部分标记的数据来完成跨领域的CWS任务。为了利用无标记数据，通过门机制将两种字符级语言模型与分割模型相结合。通过修改损失函数，利用部分标记数据对模型进行训练。</p><p><strong>数据集：</strong>the labeled People’s Daily (PD) dataset. SIGHANBakeoff 2010 Chinese Wikipedia</p><p><strong>取得的成果：</strong>解决缺乏注释数据的资源贫乏领域的CWS问题</p><p><strong>论文名：</strong>Neural Networks Incorporating Dictionaries for Chinese Word Segmentation</p><p><strong>作者：</strong>Qi Zhang, Xiaoyu Liu, Jinlan Fu</p><p><strong>机构：</strong>复旦大学智能信息处理重点实验室</p><p><strong>主要工作（解决了什么）：</strong>试图解决汉语分词任务中词典与神经网络相结合的问题。研究了基于神经网络的汉语分词任务字典集成问题。针对CWS任务，我们提出了两种将从字典中提取的信息集成到基于神经网络的方法中的方法。</p><p><strong>数据集：</strong>PKU MSR AS CITYU CTB6   2010bakeoff  </p><p>96.5 97.8 95.9 96.9 96.4</p><p><strong>取得的成果：</strong>合并字典可以显著提高神经分词能力。比目前最先进的神经网络模型和领域适应模型取得了更好的效果。</p>]]></content>
      
      
      <categories>
          
          <category> nlp_Research </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 近五年(2014-2018)中文分词论文及趋势整理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Cmake由浅入深</title>
      <link href="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/"/>
      <url>/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h3><span id="1-windows-和linux下执行单文件">1. Windows 和linux下执行单文件</span></h3><p>在windows环境下，大家都熟悉怎么编写并执行一份代码：</p><ol><li>先打开编译器例如codeblocks编写源代码，例如一个c++文件；</li><li>点击编译按钮，编译代码，生成.o的目标文件。</li><li>点击执行按钮，生成.exe的可执行文件，运行完毕。</li></ol><p>例如codeblocks下的build &amp; run：</p><p><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079026002.png" alt="1579079026002"></p><p>在linux环境下呢？</p><p>用vim编辑器编写代码，得到一个文本文件 main.cpp</p><p><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079040506.png" alt="1579079040506"></p><ol><li><p>使用g++编译main.cpp得到a.out文件<br><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079080005.png" alt="1579079080005"></p></li><li><p>执行a.out文件，执行完毕得到结果<br><img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079074703.png" alt="1579079074703"></p></li></ol><p>当然，更普遍的是使用-o来编译和执行的</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079089888.png" alt="1579079089888"></p><h3><span id="2-windows-和linux下执行多文件or项目">2. Windows 和linux下执行多文件or项目</span></h3><p>Windows自不必多说，在编译器下编译运行main函数即可</p><p>至于linux，有如下三个文件speak.h speak.cpp hellospeak.cpp</p><p>​    <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079098670.png" alt="1579079098670"></p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079114625.png" alt="1579079114625"></p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079124731.png" alt="1579079124731"></p><p>编译执行多个文件：</p><p> g++ hellospeak.cpp speak.cpp -o hellospeak</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079128351.png" alt="1579079128351"></p><p>这个时候会发现，如果源文件太多，一个一个编译时就会特别麻烦。于是人们想到了制作一种类似批处理的程序，来批处理编译源文件，于是就有了make工具。它是一个自动化的编译工具，你可以使用一条命令实现完全编译。但是你需要编写一个规则文件，make依据它来批处理编译，这个文件就是makefile。</p><h3><span id="3-解决多文件编译的困难makefile">3. 解决多文件编译的困难：makefile</span></h3><p>文件下包含一个头文件和两个cpp文件，以及一个写好的makefile：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079133459.png" alt="1579079133459"></p><p>Makefile大致内容就是要编译两个文件得到hellospeak.o和speak.o，再生成可执行文件hellospeak，最后删掉.o文件。</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079136368.png" alt="1579079136368"></p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079141053.png" alt="1579079141053"></p><p>对于一个大工程，编写makefile实在是件复杂的事，于是就出现了cmake工具，它能够输出各种各样的makefile或者project文件,从而帮助程序员减轻负担。但是随之而来也就是编写cmakelist文件，它是cmake所依据的规则。</p><h3><span id="4-cmake工具编译运行文件">4. Cmake工具:编译运行文件</span></h3><p>如果没有安装的话就通过sudo apt-get install cmake命令安装cmake工具：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079147001.png" alt="1579079147001"></p><p>准备好cmakelist.txt文件和要执行的main.cpp，以及一个build文件，用于放入cmake编译的繁多的中间文件：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079150587.png" alt="1579079150587"></p><p>要执行的main.cpp：</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079154023.png" alt="1579079154023"></p><p>cmakelist.txt文件（内容撰写待会再说）：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079156649.png" alt="1579079156649"></p><p>第一步，cmake + (cmakelists.txt所在文件夹)。</p><p>此处“..”指的是上一级文件夹，会从文件夹中找到cmakelists.txt：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079162873.png" alt="1579079162873"></p><p>系统自动生成了：CMakeFiles, CMakeCache.txt, cmake_install.cmake 等文件，并且生成了Makefile</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079166055.png" alt="1579079166055"></p><p>进行工程的实际构建，在这个目录输入<code>make</code> 命令，大概会得到如下的彩色输出：</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079171352.png" alt="1579079171352"></p><p>到这里就已经编译完成了，接下来执行这个项目得到hello world的输出：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079174053.png" alt="1579079174053"></p><p>即，整个流程为（网图，侵删）：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079177639.png" alt="1579079177639"></p><h3><span id="5-使用cmake方便的编译执行单文件demo">5. 使用cmake方便的编译执行单文件Demo</span></h3><p>先分析上一例子中的cmakelists.txt：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079183726.png" alt="1579079183726"></p><p>cmake_minimum_required(VERSION 2.8) </p><p>//指的是支持的cmake版本，可省略，但是为了方便后人，尽量加上自己所用的版本。</p><p>project(HelloWorld)</p><p>//指定项目名称，编译完成后生成的名字就是HelloWorld</p><p>add_executable(HelloWorld main.cpp) </p><p>//加入执行文件，此处是单文件，待会展开来讲</p><h3><span id="6-使用cmake方便的编译执行多文件项目">6. 使用cmake方便的编译执行多文件项目</span></h3><p> 两个cpp文件一个.h文件和一个build，这次我们试着用cmake编译执行多文件。</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079188704.png" alt="1579079188704"></p><p>这个example的cmakelists.txt里包含了之前讲的版本和项目名，加了一个include_directories，这个参数是把.h文件所在目录包含进去，可以是一堆的.h文件。其中cmake_source_dir是系统变量，可以通过set关键字来设置，默认来说是cmakelists.txt所在的文件。</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079192181.png" alt="1579079192181"></p><p>接着就是执行两个cpp文件，当然两个cpp文件也可以通过set来设置成变量Sources_code.</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079196573.png" alt="1579079196573"></p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079199576.png" alt="1579079199576"></p><h3><span id="7-一个复杂的例子关于cmakelists子目录-and-生成库">7. 一个复杂的例子：关于CMakelists子目录 and 生成库</span></h3><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079203657.png" alt="1579079203657"></p><p>这个文件夹下包含build文件、一个要执行的cpp文件和CMakelists.txt。除此之外，还有一个MathFunctions文件夹。</p><p>先来看子目录MathFunctions：</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079206479.png" alt="1579079206479"></p><p>这里的CMakeLists.txt只有几行，将speak.cpp中的函数生成一个MathFunctions库。</p><p>   <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079211710.png" alt="1579079211710"></p><p>在顶层目录下的CMakeLists.txt中，通过add_subdirectories加入子目录的CMakeLists.txt。</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079215728.png" alt="1579079215728"></p><p>前两行是版本和项目名，五六行加入.h文件的目录，10行设置一个变量默认为ON,12行判断是否OK，如果OK为ON的话就可以执行13-23行。</p><p>13-23行，是确定使用自己本地（即MathFunctions文件夹中的库），分别是加入.h文件、加入子目录（划重点，下面讲）、设置EXTRA_LIBS变量，如果未设置28行就不再链接这个库。</p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079221077.png" alt="1579079221077"></p><p> <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079224414.png" alt="1579079224414"></p><p>  <img src="/2018/12/12/yu-yan-gong-ju-ji-zhu-deng-wen-dang/cmake-you-qian-ru-shen/1579079227552.png" alt="1579079227552"> </p><h3><span id="8-总结cmakeliststxt的整理内容">8. 总结cmakelists.txt的整理内容</span></h3><p> cmakelists.txt内容不需区分大小写。</p><p>一般的cmakelists.txt的编写，包括以下几部分：</p><ol><li><p>指定cmake版本，就像上面所说的：为了方便后人，尽量加上自己所用的版本cmake_minimum_required(VERSION 2.8) </p></li><li><p>指定项目的名称，一般和项目的文件夹名称对应</p></li></ol><p>project(HelloWorld)</p><ol><li>设置环境变量 SET(变量名 变量值)<br>一般包括（但不仅仅包括）：</li></ol><pre><code>CMAKE_C_COMPILER：指定C编译器CMAKE_CXX_COMPILER：指定C++编译器CMAKE_C_FLAGS：编译C文件时的选项，如-g；也可以通过add_definitions添加编译选项EXECUTABLE_OUTPUT_PATH：可执行文件的存放路径LIBRARY_OUTPUT_PATH：库文件路径CMAKE_BUILD_TYPE:：build 的类型(Debug, Release, ...)</code></pre><p>变量很多很复杂，根据需要使用即可，可以从官方文档中查找：<a href="https://cmake.org/cmake/help/v3.0/manual/cmake-variables.7.html" target="_blank" rel="noopener">https://cmake.org/cmake/help/v3.0/manual/cmake-variables.7.html</a></p><p>当然还有一些自己定义的变量名，也用set设置</p><ol><li><p>LINK_DIRECTORIES  添加需要链接的库文件目录,即链接库搜索路径<br>link_directories(directory1 directory2 …)</p></li><li><p>添加可执行文件要链接的库文件的名称<br>TARGET_LINK_LIBRARIES(PROJECT_NAME libname.so)</p></li></ol><ol><li><p>头文件目录</p><pre><code>INCLUDE_DIRECTORIES(  Include )如果文件夹较多，则可以这样写：INCLUDE_DIRECTORIES( ${CMAKE_SOURCE_DIR}/include/ ${CMAKE_SOURCE_DIR}/include/a/ ${CMAKE_SOURCE_DIR}/include/b/)</code></pre></li></ol><ol><li><p>源文件目录<br>AUX_SOURCE_DIRECTORY(src DIR_SRCS)</p></li><li><p>添加要编译的可执行文件<br>ADD_EXECUTABLE(PROJECT_NAME TEST_CPP)</p></li><li><p>生成动态库or 静态库<br>这里多说两句，用cmake生成静态动态库，是将在cmakelists.txt文件中加入的源文件头文件等等，生成一个类似于.h/.a的文件<br>这与<8>中加入想要编译的可执行文件是二选一的关系。<br>add_library(person SHARED ${srcs})<br>add_library(person_static STATIC ${srcs})</8></p></li></ol><p>install（TARGETS）<br>创建规则以将列出的目标安装到给定目录中。</p>]]></content>
      
      
      <categories>
          
          <category> 语言工具技术等文档 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Cmake由浅入深 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>东北大学史上最全转专业攻略</title>
      <link href="/2017/07/20/jing-yan-fen-xiang/dong-bei-da-xue-shi-shang-zui-quan-zhuan-zhuan-ye-gong-lue/"/>
      <url>/2017/07/20/jing-yan-fen-xiang/dong-bei-da-xue-shi-shang-zui-quan-zhuan-zhuan-ye-gong-lue/</url>
      
        <content type="html"><![CDATA[<p>此文是大三时发在贴吧里的，博客建立后整理到这里，原帖链接：<a href="http://tieba.baidu.com/p/5233701747?fid=44908" target="_blank" rel="noopener">http://tieba.baidu.com/p/5233701747?fid=44908</a></p><h2><span id="1-背景介绍">1. 背景介绍</span></h2><p>本人是15级转专业学生，非大神，因为大一结束后成绩较差，又很想转专业，所以研究了近两年的转专业政策，打听了真实的转专业状况，期间联系到二十多位转入到不同专业的学长学姐，在此郑重感谢他们。</p><p>因为有不少的面临转专业机会的学弟学妹甚至刚入学的新生私下问我怎么转专业该不该转专业能不能转专业等问题，而转专业又涉及到你接下来三年的大学生活和以后甚至一辈子工作的方向，是一个应该慎之又慎的事情，我既不想三言两语说不清楚，又不想因为我的主观观点给你们带来影响，因此，我只是把我知道的了解的告诉你们，由你们自己来判断，选择自己接下来要走的路。</p><p>在这里难免要提到一些专业的名字，本人表示并无恶意，如果有观点不一的地方，欢迎讨论，有说的不对的地方，欢迎批评指正。</p><h2><span id="2-为什么要转专业-浅析各专业行业形势">2. 为什么要转专业-浅析各专业行业形势</span></h2><p>东大官网机构设置可得到东北大学所有的学院，对于院系划分，基本上类似或者同种的专业放在同一个学院中，也就是说，同学院即同行业（当然不同学院也可能是同行业，例如计算机学院与软件学院）。</p><p>为方便介绍转专业相关，我将这些专业划为三类：体育类，艺术类，普通类（包括文史类和理工类）。本人是工科生，对体育类和艺术类了解较少，就不瞎说了，而普通类大同小异，只需要注意，文史类不可转入理工类，理工类可转入文史类。</p><p>众所周知，东北大学听着像综合大学，但是他实际上是一个工科类大学，他强在理工类专业，在文史类有强专业但是整体偏弱。而在这些专业中，东北大学在冶金，采矿，机械等传统工业上极强，然而这些传统工业行业低迷，在信息时代，其相同努力程度带来的成果略不及自动化、计算机、软件等信息相关的行业。在这里我要普及一个常识，学校好的专业放在外面不一定好（例如采矿等，排名好然而待遇差），学校差的专业不一定出去以后就差（比如机械学院的车辆工程，分流后成绩靠前顺利保研岂不是美滋滋）。</p><p>因此，每年都有大量大量的大神从理学院、资土学院、冶金学院、材料学院、机械学院、建筑学院等转入信息学院、计算机学院、软件学院。然而，并不是说机械材料等专业不如自动化计算机软件电气，它就特别不好了，在机械材料这些专业中，仍然有一批一批的大神签到好工作，保研到好学校等等。只不过是行业的影响使得他们获得同种水平的回报前，需要更努力一点。网络上各种论坛上的各种信息，大家要有辨别的能力，不要听人说啥就是啥（毕竟哪一行都有混的不好的喷子），而一些学长学姐的切身经验就比较真实了，多途径获取信息，也是大学生最重要的能力之一。</p><p>因此，要不要转专业，你需要去通过各种途径去了解你的本专业和目标专业。如果你有转专业的资格，那么在高考后仓促报下的志愿，你又有了第二次机会去选择。如果没有资格还不喜欢本行业，那么自学其他专业知识，跨考研究生，甚至转销售、创业（虽然这些我不太赞成）也不是行不通的。关键在个人！</p><h2><span id="3-转专业需要做的事情">3. 转专业需要做的事情</span></h2><p>转专业，如果你是准大二，在大类专业分流后，你需要在下个学期提前去学校，填表格交给教务处等结果（也可能有面试），一般在开学前你就能得到转专业的结果了。</p><p>在这之前，你需要的是，本专业所在学院的转专业政策，目标专业所在学院的转专业政策。这些在学院的官网上或者教务处上都有，没有的话年级群也会发，如果找不到目标专业的政策，就去找一个那个专业的同学，问他要一份就ok了。仔细研读政策，这是你需要干的。</p><h2><span id="4-转专业政策解读以信息学院为例">4. 转专业政策解读（以信息学院为例）</span></h2><p>转专业，分两步，1.转出；2转入。转出需要满足本学院的要求，转入需要满足目标学院的要求。</p><p>百度：东北大学信息学院，进入官网，右边的学院通知有16级转专业政策，这就是传说中的转专业政策，接下来我一点点解读。</p><p>一、 工作原则：介绍了转专业的基本要求，以及部分本学院的特殊班级的转专业要求。</p><p>二、 组织机构：都是学院的院长教授副教授，不用管。、</p><p>三、 转出转入计划（重点）：这涉及到你们学院的转专业名额，一般转出的人会少于转出名额，不用担心，尤其是排名在名额内的，更不用担心。转入的名额就很重要了，一般转入名额越多，转入越容易。在后面我会简单介绍各专业转入难度。</p><p>四、 转出工作机制（重中之重）：</p><p>两种：</p><p>1.学科专长（很少，但是有成功的案例，我不太了解，就不多说了，接下来默认说的都是学业优秀类）。</p><p>2.学业优秀类，即靠成绩。这里比较重要的是，你的绩点排名占比（排名/总人数），必须在学院转出的比例之内，一般有5%，10%，20%，越高越容易转出，5%的那几个学院转出就很难。如果是大类招生，转专业是在分流后进行的，但是你的排名按照大类专业的比例，但是转入转出的名额是按照分流后的专业进行限制的。</p><p>五、 转入工作机制（重中之重）：</p><p>这里与转出的资格筛查很像，不一样的是除了必须满足的绩点排名外，多一个高数英语的排名。高数英语成绩，共四门，这里说的是四门加起来排名，择优录取（后面我会说大概能转入的分，这里只介绍政策，方便一些不了解政策的同学阅读）。</p><p>六、 工作进度安排（重点）：这个就不用我说了吧，别误了交表的点。</p><h2><span id="5-转专业潜规则">5. 转专业潜规则</span></h2><p>潜规则？不存在的，我只是想介绍一下转专业的真实难度。</p><p>可能你们在一些学长学姐的口中得知，转专业的都是大神，专业前几才能转专业的balabala什么的，这些学长都是好心，不是吓唬你们，只不过是他们可能是大四及以上的学长，他们转专业的那会，转专业确实很难。然而现在不一样了，转专业的不一定是大神了。</p><p>经过转专业政策解读后，你们应该知道了，转专业分转出本学院，转入目标学院两步。</p><p>关于转出：一般来说，只要绩点排名符合本学院的要求，就能转出，这是一个死规定，因为学院也不想丢失人才。</p><p>关于转入：只要你能转出，基本都能转入（有但是的，耐心看）。</p><p>转入，首先满足转入学院的绩点排名（基本条件），这个应该也是死规定。但是，之后的择优录取，各学院是不一样的，例如信息学院的高数英语排名，例如软件学院的70%高数英语、30%面试，例如机械学院的：1.品德优良，无违纪记录；2.热爱转入的专业，能够在转入专业安心学习；3.成绩无不及格记录（你懂的）。也就是说，在满足转入的绩点排名要求后，学院自行考核学生择优录取。</p><p>对于信息学院计算机学院这种抢手的学院，你需要高数英语分数很高，每年的高数英语平均分不一致，因此单凭分数估算太过片面。对于计算机科学与技术，因为名额少（去年十几个，但是听说录的远远多于这个数字），因此竞争很强。自动化虽然很抢手，但是每年招的人比计算机多，所以在10%的进入的可能性都很大。大概高数英语四门课的排名都在专业前10%应该稳进（可以互补），这个没办法估，因为转专业的人数基数还是少，统计的历年成绩变动也会很大，只能说，只要满足绩点排名要求就大胆去试。大概高数英语四门加起来320左右就可以一试，基本340以上就稳了。</p><p>重点来了：填表的时候可以填俩专业，这俩专业的录取和高考不一样！不一样！不一样！举个例子，学生ABC，分数分别为350，330，310。他们分别报的志愿：A（1计算机2机械）B（1计算机2机械）C（1机械2.材料）。录取顺序是这样的：A的1、B的1、C的1，A的2、B的2、C的2。因此：第一志愿相当重要，基本就决定了你能不能转成功（第二志愿开始录取前一般名额就满了）</p><p>听完是不是很担心，一志愿进不了就完蛋了。不存在的！首先，你需要评估一下你的高数英语成绩，要是只有300多，计算机物联网这样的就别想了。如果你高数英语成绩稍微高了点，但是别的专业名额还没满，老师会跟你打电话，问你调到其他专业行不行balabala，所以，老师录入你的时候，第二志愿也实际上有用，因此，在此我建议，一二志愿填同一学院！</p><p>转入难度：计算机（包括计算机科学与技术和物联网）&gt;自动化（包括自动化和电气）≈电子信息和通信&gt;软件（往年竞争不激烈，报的人数都不够，今年肯定竞争激烈，相信我）&gt;&gt;机械材料等等。其他的例如金融啥的，我不了解，就不瞎说了。</p><p>综上：只要你绩点排名满足转入，大胆报，一般都能进，最不济就是换了个同学院的专业，比如报的计科换成通信什么的（这对冶金的来说，能脱坑都是好的）。但是如果你的高数英语实在低的不像话，就别冲太好的专业，或许比你专业稍微好点的理学院的机械材料院的也是个选择（这些专业一般报了都要）。</p><h2><span id="6-转专业后学习生活和良心学长的寄语">6. 转专业后学习生活和良心学长的寄语</span></h2><p>转完专业，你就需要补相应的课程了，不用担心学不过来，完全学的过来，能转专业的学习能力不会太差。一般来说，有的学院强制要求补某些课（例如计算机学院），有的学院补够相关课群的学分就好了（根据培养计划）（例如信息学院和软件学院），一般来说，各个学院不一样，多和学院的教学办的老师交流，多多询问就好了，最好能抱上去年转进这个专业的学长的大腿。</p><p>还要考虑的是，你选择转入后 的下一步规划，例如想保研，又转计科自动化，那么保研的难度指数型增大，因为计科大牛太多了而且保研名额也没办，然后你会发现电子信息或者软件也不错，相对竞争也少一点。这些都是需要考虑的因素。转专业的同学都不会太差，但是你需要提高自己获取信息的能力，例如你会发现教务处能找到前几年转专业的名单就知道哪些专业抢手哪些不抢手了。相应的其他信息你也能收集到，然后综合起来，选择自己想要的。</p><p>洋洋洒洒敲了四千多字，其实也没多少内容，搜集信息，然后大胆尝试，才是最重要的。如果还有想问的，可以在楼下回复，也可以私信我要QQ。</p><p>嗯，希望能在九月听到你们成功的消息。</p>]]></content>
      
      
      <categories>
          
          <category> 经验分享 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 东北大学史上最全转专业攻略 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
