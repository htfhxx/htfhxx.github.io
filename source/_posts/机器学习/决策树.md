---
title: 决策树
date: 2019-12-02  
author: 长腿咚咚咚
toc: true
mathjax: true
categories: 机器学习
tags:
	- 决策树
---

#  1. 决策树

[TOC]

## 1.1 简介

决策树是一个分而治之的递归过程。 

1. 构建根节点，初始化特征集合和数据集合；
2. 选择最优特征，并划分子集，更新数据集合和特征集合；
3. 继续划分直到所有训练数据基本划分正确。



决策树学习的关键问题：**特征选择、决策树的生成和决策树的修剪。**

## 1.2 特征选择

**选择最优划分属性**是决策树的关键。

几种常见的划分属性的策略：ID3、C4.5、CART。

### 1.2.1 ID3、C4.5

* 数据集的**信息熵**：不确定性、用于度量样本的集合纯度，其中p是类别 $i$ 占总样本的比例:

$$
H(D)=-\sum_{i=1}^{n} p_{i} \log p_{i}, \\ p_i = D_i / D 
$$

* 针对某个特征 A，对于数据集 D 的**条件熵** $H(D|A)$:

$$
\begin{aligned}
H(D \mid A) &=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right) 
\end{aligned}
$$

* 信息增益：**数据集D的信息熵$D(D)$**与**特征A下的数据集D的条件熵**之差：

$$
g(D, A)=H(D)-H(D | A)
$$



* 信息增益的算法
  ![image-20210710164845451](%E5%86%B3%E7%AD%96%E6%A0%91/image-20210710164845451.png)

  * 输入：数据集D和特征A
    输出：特征A对数据集D的信息增益$g(D, A)$

    1. 计算数据集D的经验熵H(D)： 

    $$
    H(D)=-\sum_{k=1}^{K} \frac{\left|C_{k}\right|}{|D|} \log _{2} \frac{\left|C_{k}\right|}{|D|}
    $$

    2. 计算特征A对数据集D的经验条件熵H(D|A)：

    $$
    H(D | A)=\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{|D|} H\left(D_{i}\right)=-\sum_{i=1}^{n} \frac{\left|D_{i}\right|}{D |} \sum_{k=1}^{K} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|} \log _{2} \frac{\left|D_{i k}\right|}{\left|D_{i}\right|}
    $$

    3. 计算信息增益： $g(D, A)=H(D)-H(D | A)$

* 信息增益比：

$$
g_{R}(D, A)=\frac{g(D, A)}{H(D)}
$$

* 优缺点：
  * 信息增益对**取值数目较多的属性**有偏好(分支多，划分后的信息增益大)
  * 增益比对取值数目较少的属性有所偏好
  * 于是：c4.5先从侯选属性中找出信息增益高于平均水平的，再从中选择增益率最高的

* 为什么信息增益偏向属性取值多的分支？
  * 当特征取值较多时， 根据此特征划分得到的子集纯度提升的更多（对比取值较少的特征）

  

* 其他问题：
  * 生成树的过程中：如果特征$A_g$的信息增益（比）小于阈值，则置为单节点的树
  * 终止条件：数据集为空或者特征集为空的情况返回树



### 1.2.2 CART（Classification And Regression Tree)

将信息熵换为基尼指数

* 基尼指数：表示样本集合的不纯度，数值越小纯度越高
  $$
  \operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
  $$

* 给定样本D的基尼指数
  $$
  \operatorname{Gini}(D)=1-\sum_{k=1}^{K}\left(\frac{\left|C_{k}\right|}{|D|}\right)^{2}
  $$

* 如果根据特征 $A$ 被分割成 $D_1$ 和 $D_2$ 两部分， 那么在特征 A 的条件下， 集合 D 的基尼系数定义为：
  $$
  \operatorname{Gini}(D, A)=\frac{\left|D_{1}\right|}{|D|} \operatorname{Gini}\left(D_{1}\right)+\frac{\left|D_{2}\right|}{|D|} \operatorname{Gini}\left(D_{2}\right)
  $$

* 选择最小的$\operatorname{Gini}(D, A_i)$来确定特征的选择

* 优缺点：减少了大量的对数运算



## 1.3 决策树的剪枝处理

剪枝的动机：防止决策树过拟合，泛化能力差

剪枝的分类：预剪枝和后剪枝

预剪枝：在节点划分前确定是否停止剪枝：

1. 节点内数据样本数量低
2. 划分后精度变低

后剪枝：尽可能多的划分，然后在已经生成的决策树上进行剪枝

1. 从叶子节点自底向上递归，回缩到对应的父节点上，计算精度的变化，如果剪枝后精度上升，就可以将子树替代为节点



## 1.4 面试常问

* 递归的终止条件是什么呢？
  * 树的最大深度
  * 直到每个叶子节点都**只有一种类型**时停止，（这种方式很容易过拟合） 
  * 当叶子节点的**样本个数**小于一定的阈值
  * 节点的**信息增益**（或基尼系数）小于预定阈值    **推荐**
  * 没有更多特征

* 回归树？
  * 使用**标准差**来衡量子集中元素的纯度。

* 为什么信息增益偏向属性取值多的分支？
  * 当特征取值较多时， 根据此特征划分得到的子集纯度提升的更多（对比取值较少的特征）



 



## Reference：

```
《统计学习方法》
《机器学习》——西瓜书
https://github.com/NLP-LOVE/ML-NLP
https://github.com/htfhxx/NLPer-Interview
https://www.bilibili.com/video/BV1Ca4y1t7DS?p=2&spm_id_from=pageDriver
```

