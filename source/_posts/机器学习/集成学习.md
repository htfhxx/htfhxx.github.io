---
title: 集成学习
date: 2021-07-01
author: 长腿咚咚咚
toc: true
mathjax: true
categories: 机器学习
tags:
	- 集成学习
---



[TOC]

## 1 集成学习

集成学习，通过**构建**并**结合**多个机器学习器来完成学习任务

常见的集成学习框架有2种：

1. Bagging： 基学习器之间无强依赖关系，可并行生成
2. Boosting：有强依赖关系，必须串行生成

### Bagging

- 有放回的随机采样
- 思想：
  1. 基于自助采样法（bootstrap sample）构造T个样本集
  2. 基于每个采样集训练T个基学习器，然后将这些基学习器结合
  3. 对分类任务采用投票法，对回归任务采用平均法
- Bagging可以降低模型方差，但不改变模型偏差

<img src="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/1042406-20161204200000787-1988863729.png" alt="img" style="zoom: 33%;" />

### Boosting

* 每个基学习器都会在前一个的基础上进行学习，最终结合所有基学习器来预测
* Boosting的工作机制：将弱学习器提升为强学习器的算法
  1. 从初始训练集中学习一个基学习器
  2. 基于前一个基学习器，学习下一个基学习器
  3. 重复进行，最终得到所有的弱学习器，并组合成强学习器

* 组合方法涉及两种：
  1. 加法模型
  2. 前向分布算法



### 方差&偏差

偏差（Bias）描述的是预测值和真实值之差；

方差（Variance）描述的是预测值作为随机变量的离散程度。



基分类器的错误，是偏差和方差之和

- boosting通过逐步聚焦分类器分错的样本，减少集成分类器的**偏差**
- Bagging采用分而治之的策略，通过对样本多次采样，分别训练多个模型，减少**方差**



为什么决策树是常用的基分类器？

* 决策树受样本和特征集合扰动的影响比较大，随机性大，不稳定的模型适合当基学习器

  

## 2 随机森林

随机森林以决策树为基学习器。是 Bagging 的优化版本。

* 构建过程：
  1. 随机选择一部分样本：基于自助采样法（bootstrap sample）构造多个样本集
  2. 随机选择一部分特征：随机选取特征集合，特征数远小于原特征集合；
  3. 每棵树都尽最大程度的生长，并且没有剪枝过程；
  4. 进行投票决策

* 优点：
  1. 防止单一决策树过拟合，消除了决策树容易过拟合的缺点
  2. 减少预测的方差（偏差不一定减小



## 3 AdaBoost

<img src="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/1042406-20161204194331365-2142863547.png" alt="img" style="zoom: 67%;" />

* AdaBoost （Adaptive Boosting）的思想
  
  * 在前一个学习器分错的样本基础上学习第二个弱学习器
  * 学习第二个弱学习器的方式是：**样本重加权**
    – 分对的样本，其权重减小
    – 分错的样本，其权重增大
  * 最终的集成，根据错误率对基分类器的**权重系数**做加权
  
* **AdaBoost的学习过程**
  有训练数据集   $\mathrm{T}=\left\{\left(\mathrm{x}_{1},\mathrm{y}_{1}\right),\left(\mathrm{x}_{2},  \mathrm{y}_{2}\right), \ldots,\left(\mathrm{x}_{\mathrm{N}}, \mathrm{y}_{\mathrm{N}}\right)\right\}$

  1. 初始化训练数据的权值分布
     $$
     D_{1}=\left(w_{11}, \cdots, w_{1 i}, \cdots, w_{1 N}\right), \quad w_{1 i}=\frac{1}{N}, \quad i=1,2, \cdots, N
     $$

  2. 训练一个新的基学习器、计算学习器系数、更新样本权重

     1. 根据当前训练数据的权值分布**学习到一个新的基学习器$G_{m}$**
        $$
        G_{m}(x): \mathcal{X} \rightarrow\{-1,+1\}
        $$

     2. 计算基学习器在训练集上的分类误差率$e_{m}$
        $$
        e_{m}=P\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)=\sum_{i=1}^{N} w_{m i} I\left(G_{m}\left(x_{i}\right) \neq y_{i}\right)
        $$

     3. 根据分类误差率得到系数$\alpha_{m}$，这个权重系数也是最终分类器组合时的权重：
        $$
        \alpha_{m}=\frac{1}{2} \log \frac{1-e_{m}}{e_{m}}
        $$

     4. 更新训练数据集的权值分布
        $$
        \begin{array}{c}
        {D_{m+1}=\left(w_{m+1,1}, \cdots, w_{m+1, i}, \cdots, w_{m+1, N}\right)} \\
        {w_{m+1, i}=\frac{w_{m i}}{Z_{m}} \exp \left(-\alpha_{m} y_{i} G_{m}\left(x_{i}\right)\right), \quad i=1,2, \cdots, N}
        \end{array}
        $$

  3. 重复步骤2，得到多个基学习器，构建基本分类器的线性组合，并得到最终分类器

  $$
  G(x)=\operatorname{sign}(f(x))=\operatorname{sign}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
  $$


* 一些问题
  * 为什么分类误差率使用这个公式：对loss重写使loss公式包含$\alpha_{m}$，对其求偏导后，极值点对应的偏导为0时loss为em，得到这个系数权重



## 4 GBDT

### BDT

以CART树为基学习器的集成学习模型。

<img src="%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20210710190409892.png" alt="image-20210710190409892" style="zoom:50%;" />







### GBDT

GBDT（梯度提升决策树 Gradient Boosting Decision Tree）GBDT 的核心在于累加所有树的结果作为最终结果。

核心思想是**梯度迭代（Gradient Boosting）**：根据前一个基学习器预测结果与训练样本真实值的残差，来训练下一个基学习器（训练f使得Fk逼近真实值）

![image-20210706212436417](%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/image-20210706212436417.png)

举个例子：比如说 A 用户年龄 20 岁，第一棵树预测 12 岁，那么残差就是 8，第二棵树用 8 来学习，假设其预测为 5，那么其残差即为 3，如此继续学习即可。

![img](%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/v2-fedb38d98fdc20eeaea35d966f085836_720w.jpg)



缺点：依赖强、并行难、效率低



## 5. XGBoost

基于GBDT，但是进行了若干优化





## 6. LightGBM





## reference：

```
《统计学习方法》
《机器学习》
https://github.com/NLP-LOVE/ML-NLP
https://github.com/htfhxx/NLPer-Interview
https://zhuanlan.zhihu.com/p/87885678
https://zhuanlan.zhihu.com/p/162001079
https://zhuanlan.zhihu.com/p/148050748
https://zhuanlan.zhihu.com/p/86263786
https://www.cnblogs.com/pinard/p/6131423.html
```

