---
title: 激活函数
date: 2019-12-24
author: 长腿咚咚咚
toc: true
mathjax: true
categories: deep-learning
tags:
	- 激活函数
---

[TOC]



### 1 为什么需要激活函数

如果不使用非线性激活函数，那么每一层输出都是上层输入的**线性组合**

此时无论网络有多少层，其整体也将是线性的



### 2 常用的激活函数

#### 2.1 sigmoid

<img src="%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/00630Defly1g2x34jlnrrj306g0590st.jpg" alt="image" style="zoom: 67%;" />
$$
\begin{aligned} a &=g(x)=\frac{1}{1+e^{-x}} \\ g(x)^{\prime} &=\frac{d}{d x} g(z)=\alpha(1-\alpha) \end{aligned}
$$

#### 2.2 tanh(双曲正切)

<img src="%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/00630Defly1g2x355gdkij306k04q0sr.jpg" alt="image" style="zoom:67%;" />

相当于sigmoid向下平移和伸缩变形
$$
\begin{array}{l}{a=g(x)=\tanh (x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}} \\ {g(x)^{\prime}=\frac{d}{d z} g(x)=1-(\tanh (x))^{2}}\end{array}
$$

#### 2.3 Relu

![00630Defly1g2x3f01a0gj306d04xmx7](%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/00630Defly1g2x3f01a0gj306d04xmx7.jpg)

- 一定程度上缓解了梯度问题：其导数始终为一个常数
- **计算速度非常快：** 求导不涉及浮点运算，所以速度更快
- **减缓过拟合：** `ReLU` 在负半区的输出为 0，不会产生梯度/不会被训练，造成了网络的稀疏性——**稀疏激活**， 这有助于减少参数的相互依赖，缓解过拟合问题的发生

### 2.4 softmax

softmax将所有的输入归一化，多用于多分类问题
$$
P(i) = \frac{e^{a_i}}{\sum_{k=1}^T e^{a_k}} \in [0,1]
$$
softmax的损失：
$$
L = - \sum_{j=1}^T y_j \, log \, s_j
$$


### 3 如何选择激活函数

- 经验：如果是二分类0-1的问题：sigmoid，其余选Relu
- 一般隐层采用Relu， 有时也要试试 tanh



### 4 常见问题

为何 tanh 比 sigmoid 收敛快？导数值域大
$$
tanh^{'}(x)=1-tanh(x)^{2}\in (0,1) \\
sigmoid^{'}(x)=sigmoid(x)*(1-sigmoid(x))\in (0,\frac{1}{4}]
$$














